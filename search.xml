<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[(ECCV 2018) Exploring the Limits of Weakly Supervised Pretraining]]></title>
    <url>%2F2018%2F05%2F11%2FExploring%20the%20Limits%20of%20Weakly%20Supervised%20Pretraining%2F</url>
    <content type="text"><![CDATA[Mahajan D, Girshick R, Ramanathan V, et al. Exploring the limits of weakly supervised pretraining[C]//Proceedings of the European Conference on Computer Vision (ECCV). 2018: 181-196. Overview Motivation the large pre-training datasets are difficult to collect and annotate In this paper present a study transfer training with large network trained to predict hashtags on billions of social media images (Weak Supervised) training for large-scale hashtag prediction leads to excellent result Hashtags Dataset billions of images “labeled” in the wild with social media hashtags (no manual labeling) hashtags are noisy and image distribution might be biased multi-label samples. if has k hashtags, then each hashtag is 1/k probability pre-processing utilize WordNet synsets to merge some hashtags into a single canonical form deduplication between training and test sets. compute R-MAC features use kNN (21) Model ResNeXt-101 32xCd (classification). 32 groups, C group width Mask R-CNN (detection). Cross-Entropy. sigmoid and binary logistics get worse results Training Methods Full Network Finetuning. view pre-training as sophisticated weight initialization Feature Transfer. pre-trained network as feature extractor, without updated, only trained classifierEnsure the experiments of paper on the standard validation sets are clean. Dataset ImageNet CUB2011 Places365 Related Work JFT-300M. 300 million weakly supervised images, proprietary and not publicly visible Word or n-gram supervision. weaker than hashtag supervision Observation maybe important to select a label space for the source task to match the target task current network are underfitting when trained on billions of images. lead to very high robustness to noise in hashtags training for large-scale hashtag prediction improves classification while at the same time possibly harming localization performance Experiments Hashtags Vocabulary Size pre-trained with 17k hashtags strongly outperforms 1.5k hashtags 17k span more objects scenes and fine-grained categories Training Set Size when training network on billions of training images, current network architecture are prone to underfitting on IN-1k, pre-trained with 1.5k hashtags outperform other larger hashtags as the matching between hashtags and target classes disappear, larger hashtags outperform the effectiveness of the feature representation learned from hashtag prediction.train linear classifier on fixed feature are nearly as good as full network finetuning. For CUB2011. when training data is limited, the 1.5k hashtag dataset is better The Noise of HashtagsRandomly replaced p% of the hashtags by hashtags obtained by sampling from the marginal distribution over hashtags. The Sampling Strategy of Hashtags using uniform or square-root sampling lead to an accuracy improvement Model Capacity with large-scale Instagram hashtag training, transfer-learning performance appears bottlenecked by model capacity Most Helpful Class of Pre-training more concrete, more helpful Detection when using larger pre-training data, detection is model capacity bound gains from Instagram pre-training mainly due to improved object classification rather than spatial localization]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Learning</category>
        <category>Weakly Supervised</category>
      </categories>
      <tags>
        <tag>Weakly Supervised Learning</tag>
        <tag>Hashtags</tag>
        <tag>Social Media Images</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(CVPR 2018) Two-stream convolutional networks for dynamic texture synthesis]]></title>
    <url>%2F2018%2F05%2F11%2FTwo-stream%20convolutional%20networks%20for%20dynamic%20texture%20synthesis%2F</url>
    <content type="text"><![CDATA[Tesfaldet M, Brubaker M A, Derpanis K G. Two-stream convolutional networks for dynamic texture synthesis[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018: 6703-6712. Overview MotivationTwo-stream hypothesis model the human visual cortex in terms of two pathways ventral stream. involved with object recognition dorsal stream. involved with motion processing In this paper, it proposed two-stream model for dynamic texture synthesis object recognition (appearance). encapsulate the per-frame appearance, pre-trained for object recognition optical flow prediction (dynamic). model dynamics, pre-trained for optical flow prediction combine the texture appearance from one texture with the dynamics of another to generate entirely novel dynamic textures first work to demonstrate this form for style transfer Related Work Two General Approaches non-parametric sampling statistical parametric model Gram Matrix capture the style information, ignore the spatial location [b, c, h, w]→ [b, c, hw] &amp; [b, hw, c]→ [b, c, c] Future Work extent the idea of a factorized representation into feed-forward generative networks Method Synthesizing a dynamic texture is formulated as an optimization problem with the objective of matching the activation statistics. Appearance Stream N_l. the number of filter M_l. the number of spatial location t. time t Average over the target frames (as groud-truth). T. the number of target frames k. spatial location i, j. the index of filter each single frame to be synthesised (as prediction). The Loss Function L_{app}. the number of layers used to compute Gram Matrices T_{out}. the number of frames being generated in the output Dynamic Stream input. a pair of consecutive greyscale images T-1. T frames group into (T-1) pairs The Loss Function Overall memory increases as the frames grows separate the sequence into sub-sequenceinitialize the first frame of a sub-sequence as the last frame from the previous sub-sequence and keep it fixed. Experiments w/o Dynamic Stream Loss of Flow Decode Layer vs Concat Layer concatenation layer activation is far more effective than the flow decode layer Failure Example fail to capture spatially-inconsistent dynamics fail to capture textures with spatially-variant appearance]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Image Processing</category>
      </categories>
      <tags>
        <tag>Image Generation</tag>
        <tag>Image Processing</tag>
        <tag>Dynamic Texture Synthesis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(CVPR 2018) Who Let The Dogs Out Modeling Dog Behavior From Visual Data]]></title>
    <url>%2F2018%2F05%2F11%2FWho%20Let%20The%20Dogs%20Out%20Modeling%20Dog%20Behavior%20From%20Visual%20Data%2F</url>
    <content type="text"><![CDATA[Ehsani K, Bagherinezhad H, Redmon J, et al. Who Let The Dogs Out? Modeling Dog Behavior From Visual Data[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018: 4051-4060. Overview Motivation most cv task related to visual intelligence In this paper, it directly model a visullay intelligent agent input visual information, predict actions of the agent DECADE dataset. ego-centric videos from a dog’s perspective how the dog acts how the dog plans learn from a dogthe task of walkable surface estimation and scen classification by using this dog modeling task as representation learning. Definition of the Problems understanding visual data to the extent that an agent can take actions and perform tasks in the visual world Dataset mount Intertial Measurement Units (IMU) on the joints and body of the dog. record the absolute position and calculate the relative angle of the dog’s main limbs and body (angular displacement represented as a 4 dimensional quaternion vector) mount a camera on dog’s head. (380 video clips; 24500 frames, 21000 for training, 1500 for validation and 2000 for testing; various indoor and out door scenes, more than 50 different location) the differences of the angular displacements between two consecutive frames represents the action of the dog in that timestep connect all IMU to the same embedded system (Raspberry pi 3.0) the rate of the joint movement readings and video frames are different. perform interpolation and averaging to compute the absolute angular orientation for each frame use K-means clustering to quantize the action space. formulate the problems as classification rather than regression Related Work Visual Prediction. (activity forecast, people intent) Sequence to Sequence Models Ego-centric Vision Ego-motion estimation Action Inference &amp; Planning Inverse Reinforcement Learning Self-supervision Act like a Dong input. a series of frame (1~t) output. a series action (t+1~N) ResNet’s weights are shared. Plan like a Dog Input. two frames (1, N) Output. a series action (2, N-1) Learn from a DogCompare the pre-trained ResNet-18 (input two frames [t, t+1], predict the action between [t, t+1]) on DECADE and ImageNet. Future Work variety input. touch, smell collect data from multiple dogs. evaluate generation across dogs ExperimentsMetric class accuracy perplexity Learning to Act Learning to Plan Learning from Dog]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Prediction</category>
      </categories>
      <tags>
        <tag>Prediction</tag>
        <tag>Behavior</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(ICLR 2015) Explaining and harnessing adversarial exampless]]></title>
    <url>%2F2018%2F05%2F11%2FExplaining%20and%20harnessing%20adversarial%20examples%2F</url>
    <content type="text"><![CDATA[Goodfellow I J, Shlens J, Szegedy C. Explaining and harnessing adversarial examples[J]. arXiv preprint arXiv:1412.6572, 2014. Overview Motivation ML and DL model misclassify adversarial examples. Early explaining focused on nonlinearity and overfitting generic regularization strategies (dropout, pretraining, model averaging) do not confer a significant reduction of vulnerability to adversarial examples In this paper explain it by their linear nature fast gradient sign method to generate adversarial examples adversarial training can provide an additional regularization benefit beyond that provided by dropout Related Work the same adversarial examples is often misclassified by a variety of classifiers with different architecture or trained on different subsets of the training data trained on adversarial examples can regularize the model Summary adversarial examples can be explained as a property of high-dimensional dot products the generation of adversarial examples across different models. different models learn similar function when trained to perform the same task adversarial training can result in regularization, further than dropout models optimized easily are easy to perturb linear models lack the capacity to resist adversarial perturbation. only structures with hidden layer should bt trained to resist it ensembles are not resistant to adversarial examples Linear Explanation of Adversarial Examples precision of an individual input feature is limiteddiscard all information (η) below 1/255 classify x and x‘ to the same class when the elements of η is less than precision ε consider the dot product between a weight vector w and an adversarial example x’ the adversarial perturbation (η) causes the activation to grow by maximize the growth, when assume w has n dimension, and the average magnitude of an element in w is m, the growth will be grow linearly with n. when dimension n is large, it will make greatly effects Linear Perturbation of Non-linear Models Fast Gradient Sign Method Θ. parameters of model x. Input y. output J. cost function Adversarial Training of Deep Network deep network at least can represent function to resist adversarial perturbation, but shallow linear model can not The adversarial training on maxout network does not work well. After make the model larger it works well.]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Adversarial Attack</category>
      </categories>
      <tags>
        <tag>Adversarial Attack</tag>
        <tag>FGSM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(CVPR 2015) Deep neural networks are easily fooled:High confidence predictions for unrecognizable images]]></title>
    <url>%2F2018%2F05%2F11%2FDeep%20neural%20networks%20are%20easily%20fooled%3A%20High%20confidence%20predictions%20for%20unrecognizable%20images%2F</url>
    <content type="text"><![CDATA[Nguyen A, Yosinski J, Clune J. Deep neural networks are easily fooled: High confidence predictions for unrecognizable images[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2015: 427-436. Overview MotivationA recent study revealed that changing an image in a way imperceptible to humans can cause a DNN to label the image as something else entirely. In this paper show that easy to produce images that are urecognizable to humans, but DNN 99.99% believe it is a recognizable obj Evolution Algorithmdirectly (row1 ) and indirectly (row 2) Gradient Ascent Procedure Dataset MNIST ImageNet Network AlexNet LeNet CaffeNet Discussion the are a discriminative model allocates to a class may be much larger than the area occupied by training examples for that class Application. security camera (face, voice), search engine rankings (image’s background), driverless car (generate fooling images) Evolution Directly Encoding unrecognizable to human. uniform random initialize each pixel within [0, 255] each pixel has 10% chosen to be mutated, rate decay every 1000 generation polynomial mutation operator with a fixed mutation strength of 15 to mutate chosen pixel Indirectly Encoding recognizable to human. producing image contain compressible patterns (symmetry and repetition) based on Compositional Pattern-Producing Network (CPPN). take pixel as input, and output a new pixel Gradient Ascent maximize the softmax output for classes via gradient ascent to find image employ L2-regularization to produce images with some recognizable features of classes (dog face, fox ears) Experiments Directly Encoding on ImageNetLess successful at producing high-confidence images on large dataset compare with MNIST. (larger dataset→ less overfit→ more difficult to fool) Indirectly Encoding on ImageNet More successful. Similar images for closely related categories. GeneralizationSame Architecture &amp; Different Initialization many images fooling A also fool B still some images different Different Architecture many images generalize across DNN architecture Train Network to Recognize Fooling Images (MNIST) similar performance to without fooling images (ImageNet) on the contrary]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Adversarial Attack</category>
      </categories>
      <tags>
        <tag>Adversarial Attack</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(ECCV 2018) Group normalization]]></title>
    <url>%2F2018%2F05%2F11%2FGroup%20normalization%2F</url>
    <content type="text"><![CDATA[Wu Y, He K. Group normalization[C]//Proceedings of the European Conference on Computer Vision (ECCV). 2018: 3-19. Overview Motivation Normalization along batch dimension introduces problems: when batch size smaller, BN’s error increase rapidly BN helps to converge (stochastic uncertainty of batch statistics acts as a regularizer, benifit generalization), but worse for small batch SIFT, HOG. group-wise feature and group-wise normalization BN’s statistics are computed for each GPU, not broadcast across all GPU In this paper, it proposed Group Normalization (GN) independent of batch divide channels into groups and compute μ, σ of each group Related WorksNormalization Local Response Normalization (LRN). compute the statistics in a small neighbourhood for each pixel BN Layer Normalization (LN) Instance Normalization (IN) Weight Normalization (WN)LN, IN, WN. independent with batch.LN, IN. successful in RNN and GAN model. Addressing Small Batch Batch Renormalization (BR). two parameters constraint the μ,σ of BN Synchronized BN. μ,σ computed across multiple GPUs Group-wise Computation group convolution. AlexNet ResXNet depth-wise. MobileNet, Xception ShuffleNet Dataset ImageNet. Classification COCO. obj detection, Segmentation Kinectics. Video Classification Group NormalizationRelation in Group horizontal orientation frequency shape illumination texture general formulation BN (along NHW) LN (along HW) GN (along HWC_group) G. group number; C/G. channel per group (G=1)→ LN (assume all channels make similar contribution, more stricted than GN) (G=C)→ IN (not exploit channel dependence) Future Works investigate GN in reinforcement learning (RL) Experiments Ablation StudyBatch Size (Classification) Linear Learning Rate Scaling Rule. LR 0.1 for size 32, LR 0.1N/32 for size N. Batch Size (Video Classification) Group &amp; Channel Number Distribution ComparisonClassification Detection &amp; Segmentation replace BN* with GN, when fine-tuneing weight decay of 0 for γ and β is important for good detection results the distribution of RoIs batches sampled from the same image is not i.i.d. degrades BN’s estimation Video Classification]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Architecture</category>
        <category>Component</category>
      </categories>
      <tags>
        <tag>Component</tag>
        <tag>Group Normalization</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(2017) Paying more attention to attention:Improving the performance of convolutional neural networks via attention transfer]]></title>
    <url>%2F2018%2F05%2F11%2FPaying%20more%20attention%20to%20attention%3A%20Improving%20the%20performance%20of%20convolutional%20neural%20networks%20via%20attention%20transfer%2F</url>
    <content type="text"><![CDATA[Zagoruyko S, Komodakis N. Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer[J]. arXiv preprint arXiv:1612.03928, 2016. Overview Motivation different observers with different knowledge, goals lead to different attentional strategies can a teacher network improve the performance of another student network by providing to it information about where it looksIn this paper, it improves the student network by forcing it to mimic the attention maps of a powerful teacher network. activation-based and gradient-based attention map Contribution attention mechanism to transfer knowledge activation-based (better and can combine with knowledge distillation) and gradient-based spatial attention maps Related Work Attention Mechanism image caption VQA weakly-supervised object localization classification Gradient-Based Knowledge Distillation shallow networks has been shown to be able to approximate deeper ones without loss in accuracy Network after a certain depth, the improvements came mostly from increased capacity of the networks (parameter number) 16 layer wider ResNet can learn as good or better as very thin 1000 layers one Dataset ImageNet. classification, localization COCO. obj detection, face recognition amd fine-grained recognition Attention Transfer Activation-Based Attention Transfer get the attention map from the feature maps first layer. activate for low-level gradient points middle level. high for discriminative regions (eyes, wheels) top level. reflects full obj three methods stronger networks hace peaks in attention where weak networks don’t (F_sum)^p put more weight (than F_sum) to spatial locations the correspond to the neurons with the highest activation (F_max)^p only consider one of the max rather than sum of all Cases of Student and Teacher Networks same depth different depth Loss Function L(W, x). task loss I. pairs of student-teacher attention maps The normalization of attention maps is important for student training Attention transfer can also be combined with knowledge distillation. Gradient-Based Attention TransferIf small changes at a pixel can have a large effect on the network output then it is logical to assume that the network is “paying attention” to that pix flip invariant version Experiments Attention-Based trained with all transfer loss better than only one transfer loss F_sum better than F_max Compared with Knowledge Distillation]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Learning</category>
        <category>Transfer Learning</category>
      </categories>
      <tags>
        <tag>Transfer Learning</tag>
        <tag>Attention Map</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(ECCV 2014) Visualizing and understanding convolutional networks Network for Point Cloud Analysis]]></title>
    <url>%2F2018%2F05%2F11%2FVisualizing%20and%20understanding%20convolutional%20networks%2F</url>
    <content type="text"><![CDATA[Zeiler M D, Fergus R. Visualizing and understanding convolutional networks[C]//European conference on computer vision. Springer, Cham, 2014: 818-833 Overview Factors of DL big data GPU model regularization strategy (dropout, …)In this paper, it proposed a visualization technique to explore the network unpooling the horizontal and vertical of Conv kernel Dataset ImageNet 2012 Visualization Method Unpooling→Rectification→Filtering (horizontal and vertical corresponding Conv kernels) Network Change based on AlexNet. Feature Visualization Layer2. corners, edge/color conjunctions Layer3. texture (more complex invariances) Layer4. significant variation (more class-specific) Layer5. entire obj with pose variation Feature Evolution during Training lower layers. converge within a few epochs upper layers. converge after a considerable epoch Feature Invariance small transformation dramatic effect in the first layer, less impact at the top layer not invariant to rotation, except for the obj with rotational symmetry Architecture Selection (b). mix of extremely high and low frequency information, with little converage of the mid frequency, and some dead featuresreduce 11x11 kernel size to 7x7 (d). aliasing artifacts caused by large stride 4change stride 4 to stride 2 Occlusion Sensitivity w/o Pre-trained]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Visualization</category>
      </categories>
      <tags>
        <tag>Visualization</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(ICCV 2017) SO-Net:Self-Organizing Network for Point Cloud Analysis]]></title>
    <url>%2F2018%2F04%2F29%2FSO-Net%3A%20Self-Organizing%20Network%20for%20Point%20Cloud%20Analysis%2F</url>
    <content type="text"><![CDATA[Li J, Chen B M, Hee Lee G. So-net: Self-organizing network for point cloud analysis[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2018: 9397-9406. Overview Motivation The computation of rasterizing 3D data into voxel Point cloud can be easily acquired with popular sensor (RGB-D, LiDAR, camera with the help with Structure-from-Motion) PointNet. can not handle local feature extraction PointNet++. not reveal the spatial distribution of the input cloud Kd-Net. lack of overlapped receptive fields In this paper, it proposde So-Net model the spatial distribution of point cloud by Self-Organizing Map (SOM) overlapped receptive field. conducting by point-to-node KNN search hierarchical feature extraction permutation invariant Experiments. reconstruction, classification, part segmentation, shape retrieval Contribution explicity utilize the spatial distribution of point cloud overlapped receptive field pre-trained point cloud autoencoder faster speed Related Work voxel grid orientation pooling project 3D into 2D VAE sparse method sepctral ConvNet render 3D into multi-view 3D and view-pooling Kd-Net PointNet (++) Dataset MNIST ModelNet10, ModelNet40 ShapeNetPart So-Net Permutation Invariant SOM produce 2D representation of input point cloud. m x m nodes, m∈[5, 11] unsupervised trained The reason of not permutation invariant training result highly related to the initial nodes per-sample update rule depends on the order of the input points Solution fixed initial nodes. disperse the node uniformly inside a unit ball batch update (matrix operation highly efficient on GPU). based on all points, instead of once per point Encoder given the ouput of SOM, for each point p, seach KNN SOM nodes s N input points M nodes kN points (k control the overlapped) normalize each point based on it’s node kN normalized points processed by shared FCs maxpool each M mini cloudsM points Feature Aggregation point feature→ node feature node feature→ global feature Isolated Node outside the Point Cloud set node features to zero Segmentation combine the point, node and global features found middle fusion with avg pooling is effective AutoEncoder stacking series of FC will heavy memory and computation, if point is large contains FC branch (details) and Conv branch (main body) upconv. 3x3 Conv + NN upsample (more effective than DeConv) conv2pc. 1x1 Conv + 1x1 Conv coarse-to-fine Chamfer loss. point number between input and output need not equal P_s, P_t: input and recovered cloud Experiments Details 8x8 SOM; k=3 batch size 8 each layer followed by (BN + ReLu) Dataset Augmentation Gaussian Noise (0, 0.01) to point coordinate and surface normal vectors Gaussian Noise (0, 0.04) to SOM node scaling point clouds, surface normal vectors and SOM node by a factor from an uniform distribution (0.8, 1.2) Reconstruction Classification pre-trained can improve accuracy overfitting when too many layers (SOM grouping + PointNet) Segmentation]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Point Cloud</category>
      </categories>
      <tags>
        <tag>Classification</tag>
        <tag>Point Cloud</tag>
        <tag>Segmentation</tag>
        <tag>SO-Net</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(ICCV 2017) Escape from cells:Deep kd-networks for the recognition of 3d point cloud models]]></title>
    <url>%2F2018%2F04%2F29%2FEscape%20from%20cells%3A%20Deep%20kd-networks%20for%20the%20recognition%20of%203d%20point%20cloud%20models%2F</url>
    <content type="text"><![CDATA[Klokov R, Lempitsky V. Escape from cells: Deep kd-networks for the recognition of 3d point cloud models[C]//2017 IEEE International Conference on Computer Vision (ICCV). IEEE, 2017: 863-872. Overview Motivation rasterize 3D models onto uniform voxel grids lead to large memory footprint and slow process time there exist a large number of indexing strucutres (kd-tree, oc-trees, binary spatial partition tree, R-trees and constructive solid geometry) In this paper, it proposed Kd-Networks divide the point cloud to construct the kd-tree perform multiplicative transformation and share parameters of these transformation (mimic ConvNet) not rely on grids and avoid poor scaling behavior Related Works 3D Conv (+ GAN) 2D Conv (2D projection of 3D obj) spectral Conv PointNet RNN OctNet (Oct-Trees) Graph-based ConvNet Dataset classification. ModelNet10, ModelNet40 shape retrieval. SHREC’16 shape part segmentation. ShapeNet part dataset Network InputRecur to divide the point clouds into two equally-sized subsets. get N - 1 nodes and each divide direction d_i (along x, y or z). N. the fixed size of point cloud (sub-sample or over-sample) d_i. divide direction of each level l_i. the level of tree c_1(i) = 2i, c_2(i) = 2i + 1. children of ith node Processing Data with Kd-NetGiven a kd-tree, compute the representation v_i of each node. In the ith level, apply the sharing layer to the same divide direction node. v_i. the representation of ith node φ. Relu []. concate W, b. parameters of the layer in ith level, d_i direction (dimension: 2m_{l+1} x m_l, m_l) Classification Shape Retrieval output a descriptor vector (remove trained classifier of Classification) histogram loss. also can use Siamese loss or triplet loss Segmentation mimic encoder-decoder (Hourglass) skip connection Properties Layerwise Parameter Sharing CNN. share kernels for each localized multiplication Kd-Net. share kernel (1x1) for points with same split direction in same level Hierarchical Representation Partial Invariance to Jitter split direction Non-invariance to Rotation Role of kd-tree Structure Kd-tree determine the the combination order of leaf representation Kd-tree can be regarded as a shape descriptor Details normalize 3D coordinates. [-1, 1]^3 and put the origin at centroid data augmentation. perturbing geometric transformation, inject randomness into kd-tree construction (direction probability) γ=10. Experiments Details MNIST→2D Point Cloud. point of the pixel center 3D Point Cloud. sample faces→ sample point from face Self-ensemble in test time Augmentation TR. translation long axis ±0.1 AS. anisotropic rescaling DT. deterministic tree RT. randomized tree Classification Ablation Shape Retrieval 20 rotations→pooling→FC Part Segmentation duplicated random sample with an addition of a small noise. help with rare calss during test, predict on upsampled cloud and then obtain the mapping of original points low memory footprint &lt; 120 MB]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Point Cloud</category>
      </categories>
      <tags>
        <tag>Detection</tag>
        <tag>Point Cloud</tag>
        <tag>Kd-Net</tag>
        <tag>Segmentation</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(CVPR 2018) Frustum pointnets for 3d object detection from rgb-d data]]></title>
    <url>%2F2018%2F04%2F29%2FFrustum%20PointNets%20for%203D%20Object%20Detection%20from%20RGB-D%20Data%2F</url>
    <content type="text"><![CDATA[Qi C R, Liu W, Wu C, et al. Frustum pointnets for 3d object detection from rgb-d data[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018: 918-927. Overview Previous method focus on images or 3D voxels Treat RGB-D data as 2D maps for CNN Learning in 3D space can better exploit the geometric and topological structure of 3D space and apply transformation In this paper, it proposed Frustum PointNet operate on raw point clouds by RGB-D scans leverage both 2D detection and 3D object localization key challenge. efficientlypropose possible localtions of 3D obj in 3D space 2D proposal→frustum proposal→segmentation→3D box estimation coordinate normalization Related Works Front View Image Based Methodsrepresent depth data as 3D maps Bird’s Eye View Based MethodsMV3D. project LiDAR point cloud to bird’s eye view and train RPN for 3D bounding box proposal 3D Based Methods Deep Learning on Point Clouds Problem Definitions depth data. obtained from LiDAR or indoor depth sensors and represented as a point cloud the projection matrix is known. can get a 3D frustum from a 2D image region 3D box is parameterized by size (h, w, l), center (c_x, c_y, c_z) and orientation (Θ, φ, ψ).only consider heading angle Θ in this paper. Dataset KITTI (outdoor). RGB + LiDAR point cloud (sparse due to distence) SUN-RGBD (indoor). RGB-D (dense)general framework to sparse cloud and dense cloud. Frustum PointNets Frustum ProposalThe resolution of data produced by moist 3D sensors (especially real-time depth sensors) is still lower than RGB image from commodity camera. 2D RGB detector. Fast R-CNN, FPN, focal loss with known camera projection matrix, 2D box can be lifted to frustum rotate. center axis of frustum if orthogonal to the image plane 3D Instance Segmentation V1 PointNet V2 PointNet++ directly regress 3D object location from a depth map using 2D CNN is not easy, as occluding objects and background clutter segmentation (binary classification of pixel level) in 3D point cloud is much more natural leverage the semantics from 2D detector (one-hot class vector)segmentation network can use this prior to find geometries of that category. coordinate normalization. transform the point cloud by subtracting XYZ values of centroid mask the input frustum Amodal 3D Box EstimationT-Net the origin of the mask coordinate frame may be far from the amodal box center STN (no direct supervision) vs T-Net (explicitly supervise) Box Estimation PointNetV1 PointNet V2 PointNet++ box center residual prediction. combined with the previous center residual from the T-Net and the masked points’ centroid to recover an absolute center pre-defined NS size templates (3:height, width, length) and NH equally split angle (Θ) bins (NS scores for size, NH socres for heading) output dimension. 3(center point) + 4xNS + 2xNH Multi-task Loss L_{c1-reg}. center of T-Net L_{c2-reg}. center of box estimation net L_{h-cls}, L_{h-reg}. heading angle prediction L_{s-cls}, L_{s-reg}. size prediction L_{corner}. corner loss for joint optimization pf box parameters Optimized for final 3D box accuracy (center, size and heading) have separate loss terms. And they should be jointly optimized→corner positions are jointly determined by center, size and hearding. for each of NS x NH box only foucs on the gt size/heading class sum of the distance between the eight corners of prediction and gt box To avoid large penalty from flipped heading estimation, further compute p from the flipped gt box and use minimum** of them. Experiments Comparison Ablation Study2D vs 3D contains clutter and background contains clutter and background Frustum rotation and mask centroid subtraction are critical Loss Function PointNet Version Failure Case inaccurate pose and sparse cloud (less than 5 points) multiple instances from the same category in a frustum 2D detector misses objects due to dark light or strong occlusion]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Point Cloud</category>
      </categories>
      <tags>
        <tag>Detection</tag>
        <tag>Point Cloud</tag>
        <tag>Segmentation</tag>
        <tag>Localization</tag>
        <tag>Frustum PointNets</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(NIPS 2017) Pointnet++:Deep hierarchical feature learning on point sets in a metric space]]></title>
    <url>%2F2018%2F04%2F29%2FPointnet%2B%2B%3A%20Deep%20hierarchical%20feature%20learning%20on%20point%20sets%20in%20a%20metric%20space%2F</url>
    <content type="text"><![CDATA[Qi C R, Yi L, Su H, et al. Pointnet++: Deep hierarchical feature learning on point sets in a metric space[C]//Advances in Neural Information Processing Systems. 2017: 5105-5114. Overview PointNet aggregates all individual point features to a global point cloud and does not capture local structures Point sets are usually sampled with varying densities In this paper, it proposed PointNet++ Hierarchical. learn local features with increasing contextual scales Adaptively combine features from multiple scales Two Issues how to generate the partitioning of the point set (neighbourhood ball with centroid, farthest point sampling,FPS)small neighbourhood may consist of too few points due to sampling deficiency, which might be insufficient to allow PointNet to capture patterns robustly. how to abstract sets of points or local features through the local feature learner (recursively PointNet) Related Work Hierarchical Feature Learning Deep Learning of Unordered Sets Point Sampling 3D representation. (volumetric grids and geometric graphs) Datasets MNIST (2D objects) ModelNet40 (3D) SHREC15 (3D) ScanNet (real 3D scenes) Network Sampling LayerUsing farthest point sampling (FPS) to get the the coordinate of N’ centroids’. Input: [N x d], the coordinates of point set Output: [N’ x d], the coordinates of centroid points Grouping LayerUsing ball query (or KNN) to group the point set based on centroid point set. Input: [N’ x d], centroid point set; [N x (d+C)], point set Output: [N’ x K x (d+C)], grouped point set Ball query guarantees a fixed region scale, but KNN can’t The coordinates of points in a local region are firstly translated into a local frame relative to the centroid point (to capture point-to-point relations in the local region) PointNet Layer Input: [N’ x K x (d+C)] Output: [N’ x (d+C’)] Robust under Non-Uniform Sampling DensityFeatures learned in dense data may not generalize to sparsely sampled region. In contrast, the same. Sample Method (random input dropout)[during training] for each training point set, draw a dropout rate Θ uniformly sampled from [0, p], p≥1 for each point, randomly drop a point with probability Θ In practise, set p=0.95 to avoid empty point set. various sparsity (induced by Θ) varying uniformity (induced by randomly dropout) [during testing] keep all available points. Solutionseach abstraction level extracts multiple scales of local patterns and combine them intelligently according to local point densitie Multi-scale Grouping (MSG) Features at different scales (processed by PointNet) are concat to form a multi-scale feature Drawbacks. computationally expensive (the number of centroid points is quite large at the lowest level) Multi-resolution Grouping (MRG) Left vector (a). processed by the set abstraction (sampling-grouping-PointNet, L-1 level-group-PointNet) Right vector (b). directly processed by the PointNet (L-1 level-PointNet) low densities. (a) may be less reliable than (b) the subregion in computing (a) contains even sparser points and suffers more from sampling deficiency. high densities. (a) better (a) provides information of finer details sincepossesses the ability to inspect at higher resolutions recursively in lower levels. Point Feature Propagation for SegmentationSet abstraction layer will subsampled the original point set, but the segmentation task need to obtain point features for all the original points. Using hierarchical propagation strategy with distance based interpolation (inverse distance weighted average based on KNN) Experiments Classification Comparison Robust to Sampling Density Variation Segmentation Comparison]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Point Cloud</category>
      </categories>
      <tags>
        <tag>Detection</tag>
        <tag>Point Cloud</tag>
        <tag>Segmentation</tag>
        <tag>Pointnet++</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(CVPR 2017) Pointnet:Deep learning on point sets for 3d classification and segmentation]]></title>
    <url>%2F2018%2F04%2F29%2FPointnet%3A%20Deep%20learning%20on%20point%20sets%20for%203d%20classification%20and%20segmentation%2F</url>
    <content type="text"><![CDATA[Qi C R, Su H, Mo K, et al. Pointnet: Deep learning on point sets for 3d classification and segmentation[J]. Proc. Computer Vision and Pattern Recognition (CVPR), IEEE, 2017, 1(2): 4. Overview Most method based on 3D voxel grids or collections of images, unnecessary and computation cost Point clounds are simple and unified structures, invariant to permutations. In this paper, it proposed PointNet directly consumes unordered point clouds (xyz coordinate plus color etc) using symmetric function (maxpooling) using STN to aligned points learn critical points set (contribute to the results of maxpooling) and upper-bounded shapes (all point has nothing to do with maxpooling) Contribution design PointNet for 3D point set exploit to classification, segmentation empirical and theoretical analysis on stability and efficiency illustrate 3D features computed by the selected neurons Related WorkPoint Cloud Feature handcrafted Deep Learning on 3D Data Volumetric CNN FPNN Vote3D Multiview CNN Spectral CNN Feature-based DNN DL on Unordered SetProperties of Point Sets Unorderd. Invariant to permutation Interaction among points. neighbouring points form a meaningful subset Invariant under transformation. not modify the global point cloud category and segmentation of the points. Network Three key modules maxpooling –&gt; unordered a local and global information combination structureSegmentation requires a combination of local and global knowledge. two joint alignment networksTransformation matrix in the feature space has much higher dimension (64*64) which greatly increase the difficulty of optimization. So constrain it to be close to orthogonal matrix Formulation using g (maxpooling + single variable function) and h (MLP) to approximate f, so For two sets S and S’. the their distance is small, the mapping f of them is also similar And f can be approximated by PointNet If T (input corruption) contains the critical point set of S, it is unchanged. Based on this, if T contains some noise (not beyond upper-bounded shape), it is also unchanged critical point set only contains a bounded number of points (at most each K points contribute to one dimension of K dimensions global feature) Dataset Classification. ModelNet40 Part Segmentation. ShapeNet part dataset Semantic Segmentation. Stanford 3D semantic parsing dataset Experiments Classification uniformly sample 1024 points on mesh faces according to face area and normalize them into a unit sphere Part Segmentation Semantic Segmentation &amp; Detection baseline. handcrafted point features Order-Invariant Methods Alignment Robust Time and Space]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Point Cloud</category>
      </categories>
      <tags>
        <tag>Classification</tag>
        <tag>Point Cloud</tag>
        <tag>Segmentation</tag>
        <tag>Pointnet</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(CVPR 2018) Cascaded Pyramid Network for Multi-Person Pose Estimation]]></title>
    <url>%2F2018%2F04%2F29%2FCascaded%20Pyramid%20Network%20for%20Multi-Person%20Pose%20Estimation%2F</url>
    <content type="text"><![CDATA[Chen Y, Wang Z, Peng Y, et al. Cascaded pyramid network for multi-person pose estimation[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018: 7103-7112. Overview Challenge cases of multi-person pose estimation, such as occluded keypoints invisible ketpoints complex background In this paper, it proposed Cascaded Pyramid Network (CPN) GlobalNet. localize simple keypoint RefineNet. explicitly handle hard keypoint (online hard keypoints mining) Top-down pipeline. generate human box based on detector first Contribution CPN Explore the effects of various factors in top-down pipeline Related Works Classical. pictorial structure, graphical model, tree structure and hand-crafted feature Multi-Person. top-down and bottom-up Single-Person. regressors, heatmap and score map Human Detection. one stage and two stages Dataset&amp;Metrics MS COCO. trainval (57k images and 150k person instances), minival (5k images), test-dev (20k) and test-challenge (20k). OKS-based mAP. (object keypoints similarity) Architecture GlobalNet Top-Down: C2, C3, C4, C5. C2,C3. High spatial resolution for localization but low semantic information for recognition C4,C5. More semantic information but low spatial resolution Drawbacks: the hard keypoint requires more context rather than the appearance feature nearby RefineNet Stack more bottleneck blocks in deeper layers (small spatial) explicitly select the hard keypoint online (top M) based on training loss and BP the loss from the them. Experiments Data Process box 256:192 and resize to 256x192 flip, rotation (-40~+40), scale (0.7~1.3) Test ensemble mechanism Ablation StudyNMS strategysoft-NMS surpasses hard-NMS. Detector PerformanceAP less important for pose estimation. Hard Keypoints NumberM = 8 works well. With\Without Concatenation DilationDilation increase AP and FLOPs. Image Size Comparison]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Body</category>
        <category>Pose</category>
      </categories>
      <tags>
        <tag>Pose</tag>
        <tag>Human Body</tag>
        <tag>CPN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(CVPR 2017) Annotating object instances with a polygon-rnn]]></title>
    <url>%2F2018%2F04%2F29%2FAnnotating%20object%20instances%20with%20a%20polygon-rnn%2F</url>
    <content type="text"><![CDATA[Castrejon L, Kundu K, Urtasun R, et al. Annotating object instances with a polygon-rnn[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2017: 5230-5238. Overview Motivation Most current method treat object segmentation as a pixel-labeling problem In this paper, it cast this segmentation task as polygon prediction proposed Polygon-RNN architecture for semi-automatic annotation speed up the annotation process by 4.7 times in Cityscapes dataset Polygon-RNN Input image crop vertices sequence Feature Extractor modified VGG (boundary) low-level about the edge and corner (see object) high-level about the semantic information exploit bilinear interpolation or max-pooling before concat RNN ConvLSTM. preserve spatial information; reduce parameters compared to FC-RNN Related WorkSemi-automatic Annotation GrabCut. exploit annotation GrabCut + CNNMost define a graphical model at the pixel-level which are hard to incorporate shape prior.Annotation ToolInstance Segmentation pixel-level explicit box or patch produce polygon Training Detail cross-entry at each time step of RNN feed t-1, t-2 gt to prediction t step for the first vertex prediction. train another CNN using multi-task loss 250 ms/img about inference time set chessboard distance threshhold T. If distance large than T, simulated human correction Dataset Cityscapes KITTI Data Process perform polygon simplification with zero error in the quantized grid. eliminate vertices which are in a line or fall into same grid Data Augmentation random flip enlarged box 10%~20% random select starting vertex Experiments Metrics IoU number of clicks Step Limitation set max step to 70 instance-wise. treat the entire instance as an example component-wise. treat each component as a single example Result]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Annotation</category>
      </categories>
      <tags>
        <tag>Annotation Algorithm</tag>
        <tag>Semi-Automatic</tag>
        <tag>Polygon-RNN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(CVPR 2018) Shufflenet:An extremely efficient convolutional neural network for mobile devices]]></title>
    <url>%2F2018%2F04%2F29%2FShufflenet%3A%20An%20extremely%20efficient%20convolutional%20neural%20network%20for%20mobile%20devices%2F</url>
    <content type="text"><![CDATA[Zhang X, Zhou X, Lin M, et al. Shufflenet: An extremely efficient convolutional neural network for mobile devices[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018: 6848-6856. Overview Motivation the limitation of computing power on mobiledevices costy dense 1x1 convolutions In this paper, it proposed ShuffleNet pointwise group convolution Depthwise convolution channel shuffle. (a) output from a certain channel are only derived from a small fraction of input channel. block information flow between channel group and weakens representation Channel Shufflefor a gn (group, number of each group) feature map reshape to gxn transpose nxg reshape to ng ShuffleNet Unit Architecture Unit Comparisonfor a point of c channels feature map, m channels of bottleneck ResNet. 2cm + 9mm ResNeXt. 2cm + 9mm/g ShuffleNet. 2cm/g + 9mShuffleNet apply group convolution to two 1x1 pointwise convolution. Related WorkModel GoogleNet SqueezeNet SENet NASNet Group Convolution AlexNet. 50% kernel on first GPU, 50% on second GPU ResNeXt Xception. depthwise MobileNet. depthwise Channel Shuffle Operation cuda-convnet. random sparse convolution layer, equivalent to random channel shuffle + group Conv Model Acceleration Pruning connection Channel reduction Quantization Factorization Implement convolution by FFT Distillation PVANET Experiments Hyperparameter Shuffle Channel sx. means the scale of channel, sxs times complexity of 1x. Comparison 18 times faster than AlexNet Inference Time on Mobile Devices Empirically g=3 has a proper trade-off between accuracy and actual inference time]]></content>
      <categories>
        <category>Paper Note</category>
        <category>LightWeight</category>
      </categories>
      <tags>
        <tag>Network</tag>
        <tag>LightWeight</tag>
        <tag>Shufflenet</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(PAMI 2017) Segnet:A deep convolutional encoder-decoder architecture for image segmentation]]></title>
    <url>%2F2018%2F02%2F25%2FSegnet%3A%20A%20deep%20convolutional%20encoder-decoder%20architecture%20for%20image%20segmentation%2F</url>
    <content type="text"><![CDATA[Badrinarayanan V, Kendall A, Cipolla R. Segnet: A deep convolutional encoder-decoder architecture for image segmentation[J]. IEEE transactions on pattern analysis and machine intelligence, 2017, 39(12): 2481-2495. Overview Motivation Due to the lack the good encoding techniques (poor ability to delineate boundaries), existing method use CRF to increase accuracy In this paper. it proposed SegNet In decoder, using upsampling according to the pooling indices of the encoder’s corresponding max-pooling layer. Upsample. get sparse feature map Conv. get dense feature map Eliminate the need for learning to upsample Less memory in the inference time Comparison Traditional method DeepLab-LargeFOV DeconvNet Dataset CamVid road scenes SUN RGB-D indoor scenes Related Work Hand Engineered Feature DNN Random Forest Boosting Smooth classifier by CRF Region Proposal. do not exploit co-occurrence of object or other spatial-context RGBD segmentation CRF-RNN. using RNN mimic the charp boundary delineation capabilities of CRF; can be appended to any deep segmentation architecture Multi-scale. image or feature DeconvNet DecoupledNet SegNet is inspired by the unsupervised feature learning architecture proposed by Ranzato. Experiments Best performance is achieved when encoder feature maps are stored in full When memory during inference is constrained, the compressed form (dimensionality reduction, max-pooling indices) can improve performance Larger decoder increase performance for a given encoder]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Architecture</category>
        <category>Network</category>
      </categories>
      <tags>
        <tag>Network</tag>
        <tag>Segmentation</tag>
        <tag>Pooling Indices</tag>
        <tag>SegNet</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(2017) Mobilenets:Efficient convolutional neural networks for mobile vision applications]]></title>
    <url>%2F2018%2F02%2F25%2FMobilenets%3A%20Efficient%20convolutional%20neural%20networks%20for%20mobile%20vision%20applications%2F</url>
    <content type="text"><![CDATA[Howard A G, Zhu M, Chen B, et al. Mobilenets: Efficient convolutional neural networks for mobile vision applications[J]. arXiv preprint arXiv:1704.04861, 2017. Overview Motivation DNN become deeper and more complicated to achieve higher accuracy computational limited platform in reality In this paper, it proposed an efficient model called MobileNet split standard Conv into depthwise Conv and pointwise Conv introduce two hyperparameter to trade off between latency and accuracy experiment on object detection, finegrain classification, face attributes and large scale geo-localization Dataset Stanford Dogs YFCC100M (Yahoo Flickr Creative Commons 100 Million) Related Workmost work compress pretrained networks train small networks directly depthwise separable Conv Flattened Networks Factorized Network Xception Network SqueezeNet Structured Transform Networks Deep Fried Convnets hasing pruning, vector quantization and Huffman coding distillation. using large model train small model instead of gt low bit networks Convolution operations are Implemented by GEMM (general matrix multiply) which makes the kernel and input feature into two matrix and do matrix multiply. Link PlaNet. divide earth into a grid of geography cells, then do geolocation classification on images Architecture Depthwise Separable ConvStandard Conv There exists the interaction between N and K. K. kernel size F. feature size M. input channel N. output channel Depthwise Separable Depthwise Conv (KxKxM -&gt; FxFxM). one kernel for one feature map. Pointwise Conv (1x1xMxN * FxFxM). the whole pointwise kernel for the whole output feature map of depthwise conv. It breaks the interaction between the number of output channel and the size of the kernel. Comparison for 3x3 kernel, 8~9 time less. Structure Mose computation time are in 1x1 Conv Small model has less trouble with overfitting. It is important to put very little or no weight decay on depthwise filter. Hyperparameter Width Multiplier (α). Thinner Model (channel number) Resolution Multiplier (ρ). Reduced Representation (input size) Experiments Thinner or ShallowThinner is better than Shallow. Comparison Stanford Dogs Geolocalization Face AttributeDetection]]></content>
      <categories>
        <category>Paper Note</category>
        <category>LightWeight</category>
      </categories>
      <tags>
        <tag>Network</tag>
        <tag>Mobilenets</tag>
        <tag>LightWeight</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(2017) MentorNet:Regularizing Very Deep Neural Networks on Corrupted Labels]]></title>
    <url>%2F2018%2F02%2F25%2FMentorNet%3A%20Regularizing%20Very%20Deep%20Neural%20Networks%20on%20Corrupted%20Labels%2F</url>
    <content type="text"><![CDATA[Jiang L, Zhou Z, Leung T, et al. MentorNet: Regularizing Very Deep Neural Networks on Corrupted Labels[J]. arXiv preprint arXiv:1712.05055, 2017. Overview Motivation DNN can remember entire data which are labeled randomly DNN has more parameters than the number of training example Poor performance when overfit noise Curriculum Learning. gradually learn samples in a meaningful sequence In this paper, it proposed MentorNet and SPADE (SG-partial-D) algorithm first study to learn a curriculum (weighting schema) from data by neural network supervise the training of StudentNet, improve the generalization on corrupted training data learn time-varying weights for each example to train StudentNet, result in a curriculum that decide the timing and attention to learning each example Step pretrain MentorNet to approximate predefined weighting specified in labeled data finetune MentorNet on the third clean label dataset train StudenNet using fixed MentorNet StudenNet make prediction without MentorNet Related WorkModel Regularizer less effective on corrupted label. weight decay data augmentation dropout Data Regularizertackle in the data dimension. MentorNet. foucs on weighting example in corrupted labels. can understand and further analyzed existing weighting schemes (self-paced weighting, hard negative mining, focal loss). Weight Schemes Curriculum Learning Mine hard-negative example Training a network using clean data, coupled with a knowledge graph, to distill soft logits to the noisy data ModelGoal overcome overfitting by introducing a regularizer to weight example alternately minimize w and v (fix one, update another) Weighted Loss (WL) v (n samples x m classes). weight of examples L. loss g_{s}. StudentNet Explicit Data Regularizer (G). which contains two forms, both lead to same solution. explicit. analytic form of G(v) implicit. closed-form solution. v^{*} = argmin_{v} F(w, v) (F can be MentorNet) Weight Decay Algorithm Problems fix v update w. wasteful when v is far away from optimal point fix w update v. matrix v is too large for memory SPADEminimize w and v stochastically overmini-batches. (5) moving average on the p-th percentile (8) weight decay (9) SGD or other optim MentorNet Goal learn optimal Θ to compute the weight of example step. pretrain - finetune - fix and plug in Algorithm Architecture Input z label epoch absolute loss moving average Sampling Layer sample the weights v, without replacement, according to the normalized weight distribution only perform on trained MentorNet sampling rate. hyperparameter PretrainingDataset enumerate the input space of z, and annotate a weight for each data point weight can derived from any weight schemes Objective Function (5). explicit (6). converge fast converge to the same solution FinetuningThe weighting schemes may change along with the learning process of StudenNet. Dataset sample from dataset D binary label for whether learn this example Experiments Other network label weight according to different weighting schemes Other regularizer (b). Weighted loss converge to zero Representation of MentorNet similar images have less distance.]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Architecture</category>
        <category>Network</category>
      </categories>
      <tags>
        <tag>Network</tag>
        <tag>MentorNet</tag>
        <tag>StudentNet</tag>
        <tag>SPADE</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(CVPR 2018) Data Distillation:Towards Omni-Supervised Learning]]></title>
    <url>%2F2018%2F02%2F25%2FData%20Distillation%3A%20Towards%20Omni-Supervised%20Learning%2F</url>
    <content type="text"><![CDATA[Radosavovic I, Dollár P, Girshick R, et al. Data distillation: Towards omni-supervised learning[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018: 4119-4128. Overview Motivation Semi-supervised simulates labeled/unlabeled data (upper-bounded on full annotated data) Omni-supervised exploits extra unlabeled data (lower-bounded on full annotated data) Model distillation distills knowledge from the prediction of multiple model In this paper it proposed data distillation which ensembles prediction from multiple transformations of unlabeled data, using single model Do experiments on keypoint detection and object detection Knowledge Distillation Train teacher model on large amount of labeled data (A) Generate annotation on unlabeled data (B) based on teacher model Retrain student model on data (A+B) Problem. training model on its own prediction can not provide meaningful information Solution. ensembling the prediction of the different transformation of unlabeled data (flipping, scaling) Related Work Ensemble multiple model, Model Compression FitNet Cross modal distillation Multi-view geometry Auto-encoder (multiple capsule) Self-training can be used for training object detection Multiple views or perturbations of the data can provide useful signal for semi-supervised learningThe method of this paper are also based on multiple geometric ransformations. Data Distillation Step Train teacher model on labeled data (A) Apply teacher model to multiple transformation of unlabeled data (B) Ensemble the multiple prediction to get annotation Retrain student model on data (A+B) Multi-transform multi-crop multi-scaleIn the experiment of this paper, it used multi-scale and flipping. Ensemble Aggregated prediction generate new knowledge Aggregated prediction outperform any single prediction Ensemble way###Soft Label average class probability generate probability vector, not category label not suitablefor structure output space (pose, detection) Hard Label need task specific logic (NMS for merging multiple box) Detail of Pose Estimation Selecting PredictionGenerate annotation only from the prediction that are above a certain score threshold. And found that the average number of annotated instances per unlabeled image equal to labeled image’s (similar distribution) still work robust and well when not equal Retraining retraining is better than fine-tuning (which is in a poor optimum). Experiments Data split co-80. labeled co-35. labeled co-115. co-80 + co-35 un-120. unlabeled s1m-180 (sports-1M static frame). dissimilar distribution Amount of Annotated Data 1:ρ in minibach. Accuracy of Teacher Model Result]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Data Distillation</category>
      </categories>
      <tags>
        <tag>Pose</tag>
        <tag>Detection</tag>
        <tag>Data Distillation</tag>
        <tag>Omni-supervised Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(CVPR 2017) Feature pyramid networks for object detection]]></title>
    <url>%2F2018%2F02%2F25%2FFeature%20pyramid%20networks%20for%20object%20detection%2F</url>
    <content type="text"><![CDATA[Lin T Y, Dollár P, Girshick R, et al. Feature pyramid networks for object detection[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2017: 2117-2125. Overview MotivationFeature pyramid are basic in detection, but expensive. In this paper, it proposed Feature Pyramid Network (FPN). fater-rcnn based on FPN run at 6 fps on GPU Forms Pyramidal Feature Hierarchy High-resolution maps have low-level feature. And it will harm representational capacity for detection. SDD build pyramid from high up in the network Feature Pyramid Network combine low-resolution (strong semantic) high-resolution (weak semantic) Related WorkHand-Engineered SIFT HOG Deep ConvNet OverFeat R-CNN SPPnet Using Multiple Layers FCN. sum partial scores over multi-scale Hypercolumns, HyperNet, ParseNet, ION. concat feature of multi-layers SSD, MS-CNN. predict at multi-layers without combining U-Net, Sharp-Mask. Recombinator, Hourglass, Laplacian pyramid. lateral/skip connection FPN Two Pathway bottom-up pathway top-down pathway and lateral connectionThe bottom-up map is lower-level semantics, more accuracy for localizing. Application RPN Head binary classification bounding box regression Attach head (shared) to each (P2, P3, P4, P5, P6) level of FPN. Each level single scale anchor 3 aspect ratios {1:2, 1:1, 2:1}Head shared mechanism is analogous to image pyramid mechanism with common head classifier. Fast R-CNNNo RPN, only RoI pooling. view feature pyramid as produced from image pyramid assign RoI (w,h) to level Pk of FPN 224 is the canonical ImageNet pre-training size k0 is the level of 224x224 RoI Shared headAnalogous to ResNet-based Faster R-CNN, set k0 = 4. The k of 112x112 RoI is 3 (k0 - log[112/224]). Experiments Ablation StudyUsing P2 have more proposal. Comparison Extension]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Architecture</category>
        <category>Network</category>
      </categories>
      <tags>
        <tag>Network</tag>
        <tag>Detection</tag>
        <tag>FPN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(ICCV 2017) Focal Loss for Dense Object Detection]]></title>
    <url>%2F2018%2F02%2F25%2FFocal%20Loss%20for%20Dense%20Object%20Detection%2F</url>
    <content type="text"><![CDATA[Lin T Y, Goyal P, Girshick R, et al. Focal loss for dense object detection[C]//Proceedings of the IEEE international conference on computer vision. 2017: 2980-2988. Overview Motivation two-stage approach contains sparse set of candidate object location one-stage approach suffered from foreground-background class imbalance In this paper, it proposed focal loss function to deal with class imbalance of one-stage approach (foucs on hard example [large error]). Comparison Two-stage Proposal mechanism (1-2k proposals) Biased minibatchsampling (foreground:background=1:3) One-stage 100k proposals (densely) Most are easy negative which contribute no useful learning signal Easy negative overwhelm Related Work Two-Stage R-CNN series One-Stage OverFeat SSD YOLO Class Imbalance Hard Negative Mining. completely discard easy example Sampling Schemes Robust Estimation Huber Loss. reduce the contribution of outliers by down-weighting Focal Loss. Down-weighting inliers Focal Loss α. balance factor (inverse class frequency or hyperparameter setted by cross validation) improved accuracy over non-α form γ. focusing parameter, equal to 2 better for experiments When summed over a large number of easy examples, small loss values can overwhelm the rare class. InitializationIntroduce prior concept for the model estimation probability in the beginning. set final layer’s means that p = π when get through sigmoid function. (π = 0.01 in experiments) RainNet Detector Experiments Effect Ablation Study Experiments]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Architecture</category>
        <category>Loss Function</category>
      </categories>
      <tags>
        <tag>Detection</tag>
        <tag>Focal Loss</tag>
        <tag>Loss Function</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(NIPS 2016) Coupled generative adversarial networks]]></title>
    <url>%2F2018%2F02%2F25%2FCoupled%20generative%20adversarial%20networks%2F</url>
    <content type="text"><![CDATA[Liu M Y, Tuzel O. Coupled generative adversarial networks[C]//Advances in neural information processing systems. 2016: 469-477. Overview In this paper, it proposed coupled GAN based on existence of shared high-level representation in the domains learn a joint distribution of multi-domain images unsupervised used in image transformation, domain adaption… Model Generator share the same high-level conceptDiscriminator sharing constraint can reduce parameters Loss Function Related Work VAE Attention Model Moment Matching Diffusion Process Cross-domian Image Generation GAN Laplacian Pyramid Conditional GAN Experiments Metric ratios of agreed pixels Digit [digit-edge], [digit-negative] without weight-sharing constraint, GAN generate unrelated image correlated to the weight sharing of G uncorrelated to D FaceColor and Depth Image Application Unsupervised Domain Adaption (UDA) [MNIST(labeled)-UDA(unlabeled)] attached a softmax layer c to last hidden layer of D, train on MNIST, predict on UDA Cross-Domain Image TransformationGiven x1 in domain 1, find corresponding image x2 in domain 2. As for CoGAN get the most suitable z* for x1 use z_* generate x2]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Image Processing</category>
      </categories>
      <tags>
        <tag>Image Generation</tag>
        <tag>coupleGAN</tag>
        <tag>GAN</tag>
        <tag>Image Enhancement</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(2017) Multi-scale dense convolutional networks for efficient prediction]]></title>
    <url>%2F2018%2F02%2F25%2FMulti-scale%20dense%20convolutional%20networks%20for%20efficient%20prediction%2F</url>
    <content type="text"><![CDATA[Huang G, Chen D, Li T, et al. Multi-scale dense convolutional networks for efficient prediction[J]. arXiv preprint arXiv:1703.09844, 2017, 2. Overview Motivation Small model can deal with easy example, but make mistake for hard Hard example need to be deal with by large model, but waste for easy last layer’s features for classification, early layer’s are not first layer for fine-scale, later layer for coarse-scale In this paper, Multi-Scale DenseNet (MSDNet) is proposed which automatically small for easy large for hardcontains two setting anytime classification budgeted batch classification (probability threshhold) has two feature multi-scale feature map + multi-classifier dense connectivity Contribution First deep learning architecture of its kind that allows dynamic resource adaptation with a single model First discover that dense connectivity is crucial to early-exit classifier SettingAnytime prediction stop at any time point (budget exhausted) return most recent predication nondeterministic budget, varies per test instance L. suitable loss function B. budget f. model x. input image Budget batch classification stop when sufficient confidence Less than B/M computation for easy example More than B/M computation for hard example Related Work Computation-efficient prune weights quantize weights compact model knowledge-distillation Resource-efficient FractalNet Adaptive computation time approach Visualization Future Work Extend to other task. segmentation Combine MSDNet with model compression, spatially adaptive computation, more efficient convolution operation Multi-Scale DenseNet Problem Lack of coarse-level feature Accuracy of the classifier is correlated with its position within the network Solution. multi-scale feature map Early classifier interfere with later classifier Solution. Dense connectivity Model Lazy EvaluationFiner feature map do not influence the prediction of classifier. Loss Function Empirically, $w_k$ = 1.]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Architecture</category>
        <category>Network</category>
      </categories>
      <tags>
        <tag>Network</tag>
        <tag>Multi-Scale DenseNet</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(AAAI 2017) Volumetric ConvNets with Mixed Residual Connections for Automated Prostate Segmentation from 3D MR Images]]></title>
    <url>%2F2018%2F02%2F02%2FVolumetric%20ConvNets%20with%20Mixed%20Residual%20Connections%20for%20Automated%20Prostate%20Segmentation%20from%203D%20MR%20Images%2F</url>
    <content type="text"><![CDATA[Yu L, Yang X, Chen H, et al. Volumetric ConvNets with Mixed Residual Connections for Automated Prostate Segmentation from 3D MR Images[C]//AAAI. 2017: 66-72 Overview 目前prostate MRI segmentation存在一些挑战 不同MR扫描协议导致MRI存在差别 prostate MRI缺少清晰边界（prostate与周围组织很相似） 不同病变或分辨率导致prostate形状大小变化很大 3D MRI segmentation 耗时 因此，论文提出一种end-to-end train, volume-to-volume predict, 适用于limited training data的模型，并在MICCAI PROMISE12数据集上达到state-of-art结果。 模型特点 3D CNN 存在down sampling和up sampling两个阶段 residual block和connection. block内部存在short connection, block之间存在long connection（恢复down sampling造成的空间信息丢失）. 能够增强局部信息和全局信息在整个网格中的传递，以及加快收敛，避免梯度问题 加权(0.3, 0.6, 1) auxiliary cross-entropy loss. 加快收敛; Function as a strong regularization which is important for limited training data Dig the Potential of limited data Data augmentation. 但增加的信息量有限 使用skip connection. 提升网络内信息的propagation Related WorkMulti-altas based method Deformable method Graph cut based mathod Feature based machine learning method（Deep Learning） 使用2D CNN aggregate 3D contextual feature. adjacent slices, orthogonal planes, multi-view planes. 但没有充分利用3D空间信息 3D CNN. with residual block 数据集MICCAI PROMISE12 训练集. 50 transversal T2-weighted MRI and segmentation gt 测试集. 30 MRI, 未提供gt图像来自不同医院、设备、协议以及存在maximum variation in clinical setting (voxel size, dynamic range,position, field of view and anatomic appearance.). Experiments 数据预处理 resize MR volumes into 0.625x0.625x1.5 mm 归一化 rotation (90°, 180°, 270°) , x-flip 训练细节 batch size 8 lr 0.001 divided by 10 every 3000 iteration weight decay 0.0005 SGD, 0.9 momentum weighted loss (0.3, 0.6, 1) 训练（4小时）. random crop 64x64x16 sub-volume 测试（12 s/MRI, 320x320x60）. overlap sliding window (64x64x16), stride (50x50x12), average probability map 评价指标 Dice coefficient (DSC). 链接 aRVD. absolute difference between volumes ABD. average over shortest distance between the boundary points of volumes 95HD (95% Hausdorff distance). 链接 Ablation 实验结果]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Medical</category>
        <category>Prostate</category>
      </categories>
      <tags>
        <tag>Medical</tag>
        <tag>Segmentation</tag>
        <tag>Prostate</tag>
        <tag>3D</tag>
        <tag>MRI</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(2017) A survey on deep learning in medical image analysis]]></title>
    <url>%2F2018%2F02%2F02%2FA%20Survey%20On%20Deep%20Learning%20in%20Medical%20Image%20Analysis%2F</url>
    <content type="text"><![CDATA[Litjens G, Kooi T, Bejnordi B E, et al. A survey on deep learning in medical image analysis[J]. arXiv preprint arXiv:1702.05747, 2017. Overview 论文检索并分析了308篇与深度学习和医学图像有关的论文。 检索方式 对PubMed数据库（标题摘要）进行关键词（convolutional or deep learning）检索 ArXiv检索medical imaging MICCAI, SPIE, ISBI, EMBC conference proceeding 相关任务 image classification object detection segmentation registration retrieval generation enhancement 应用领域 neuro retinal（视网膜） pulmonary（肺） digital pathology（病理学） breast（乳腺） cardiac（心脏） abdominal（腹部） musculoskeletal（肌肉骨骼） CNN Architecture Classification Architecture Multi-stream Architecture (multi-scale; 2.5D, multiple angled patched from 3D space) Segmentation Architecture (U-Net) RNN pixelRNN for segmentation Unsupervised Model AE, SAE (AE+AE+…+AE). for denoising RBM, DBN (RBM+RBM+…+RBM). 无监督训练，再加上分类器fine tuing VAE, GAN 数据集 OASIS. brain MRI registration BRATS, LSLES, and MRBrains challenges. brain lesion segmentation Kaggle Diabetic Retinopathy challenge 2015. disbetic retinopathy classification PROMISE12 challenge. prostate segmentation LUNA16 challenge. nodule classification CAMELYON16, TUPAC, DREAM. breast cancer metastases detection in lymph nodes ICPR 2012, AMIDA 2013, GLAS. gland segmentation SLIVER07. liver segmentation LIDC-IDRI. detect nodules in lung CT 医学图像中的深度学习应用 ClassificationImage/Exam Classification （图像-&gt;是否病变）使用预训练权重(迁移学习) 预训练网络提取特征，作为后续模型输入 使用medical数据fine tuning预训练网络2015~2017年，47篇exam classification paper， 36篇使用CNN， 5篇使用AE， 6篇使用RBM. 且涉及不同的医学领域：retinal, digital pathology, lung computed tomography. Object or Lesion Classification对图像中已经识别出来的某个小部分进行多分类（病变类型, nodule classification in chest CT）。通常需要考虑（Multi-stream） 病变外观的局部信息 病变位置的全局上下文信息到达高分类准确度。 DetectionOrgan, Region and Landmark Localization通常需要处理3D图像，一些方法将3D空间看做由一些2D正交平面构成（2D MRI slices）。图像localization方法分为 RoI 直接regress此外，还涉及到temporal数据的scan plane or key frame localization (standardized scan planes in mid-pregnancy fetal US). Object or Lesion Detection定位并识别lesion (multi-stream for CT and PET data; micro-bleed in brain MRI) SegmentationOrgan and Substructure SegmentationContour or interior of objects (pectoral muscle in breast MRI, coronary arteries in cardiac CT angiography, vertebral body in MRI). 相关技术 2D U-Net 3D V-Net, Dice coefficient 基于RNN，融合双向信息(left/top, right/bottom) 结合双向LSTM和2D U-Net 结合FCN和graphical model (MRF, CRF) Lesion Segmentation包含 Object detection Organ and substructure segmentation需要考虑global and loval context [Multi-stream] (white matter lesions in brain MRI). Registrationspatial alignment. 包含两种方法 估计两张图像的相似度 预测transformation parameters Other TaskRetrieval使用Hashing forest进行压缩 Generation and Enhancement超分辨率 Combine Image with ReportCaption, LDAMammography中的3个描述 shape margin density 应用领域 BrainAlzheimer classification, brain tissue segmentation, anatomical structure (hippocampus). 相关技术 multi-scale for contextual 2D slice-by-slice处理3D数据 Eyecolor fundus imaging (CFI). Chest基于CT扫描估计lung cancer概率 Digital Pathology and Microscopywhole-slide images （WSI） 检测、分割、分类nuclei 分割organ 检测分类 the disease of interest at the lesion or WSI-level Breast考虑三个子任务 检测分类mass-like lesion 检测分类micro-calcfication breast cancer评估 Cardiac使用2D CNN slice by slice处理3D或4D数据。 ##Abdomen ##Musculoskeletal ##Other 总结 Overview prefer end-to-end CNN Key Aspect data preprocessing or augmentation（相同模型结构，不同结果） 针对特定任务的模型结构（multi-view, multi-scale） 增加patch size观察更多context 超参数（LR, Dropout rate） Challenge 医学图像数据已有很多，挑战在于图像的标注（需要领域专家进行标注、3D图像标注成本大）。因此，从有限数据进行高效学习是重要的研究方向。 一些研究利用sparse 2D segmentation训练3D segmentation、一些考虑 non-expert labels via crowd-sourcing Label noise. 多专家分别进行标注，对比结果是否一致 Fail in rare category Class imbalance. 研究方向（通常对较少样本进行scale, rotate操作） 结合patient history, age, demographics进行诊断 Balance image feature (thousands) and clinical feature (handful) patch不具有全局位置信息，而entire image占用较大内存 Outlook Unsupervised, VAE, GAN are attractive Combine Bayesian statistic with deep learning Image reconstruction (unexplored area)]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Medical</category>
        <category>Overview</category>
      </categories>
      <tags>
        <tag>Overview</tag>
        <tag>Medical</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(2017) Deep Learning in Medical Imaging:General Overview]]></title>
    <url>%2F2018%2F02%2F02%2FDeep%20Learning%20in%20Medical%20Imaging%3A%20General%20Overview%2F</url>
    <content type="text"><![CDATA[Lee J G, Jun S, Cho Y W, et al. Deep Learning in Medical Imaging: General Overview[J]. Korean Journal of Radiology, 2017, 18(4): 570-584. Overview 论文综述了深度学习技术的历史、发展和应用（涉及到医学图像）。 AI三种方法 Symbolism Connectionism Bayesian 算法 分类. Bayesian模型、SVM、集成学习(结合不同分类算法) 回归. SVR ANN. DNN Unsupervised restricted Boltzmann machine. Hinton, 解决DNN的局部最优、过拟合问题。以无监督方式生成数据特征 CNN. RNN Trick Data Augmentation 预训练权重能够在100 cases per class得到较好地结果。 突破 ReLu Dropout Data augmentation 放射学应用 Segmentation lungs（肺） tumor（肿瘤） structure in brain biological cells（生物细胞） membranes（薄膜） tibial cartilage（胫骨软骨） bone tissue（骨组织） cell mitosis（细胞有丝分裂） Registration医学图像配准. 同一患者几幅图像放在一起分析时，首先要做图像严格对齐。医学图像配准是指对于一幅医学图像寻求一种 (或一系列 )空间变换，使它与另一幅医学图像上的对应点达到空间上的一致。 方法 Patch Based Entire Image Automatic Labeling and Captioning 数据集 Caption. Flickr8k, Flickr30k, MS COCO Publicly available radiology dataset of chest radiographs Reading Assistant and Automatic Dictation Automatic radiological dictation system. 深度学习应用到放射学的局限性 要求高质量数据，数据量太小容易过拟合 无法得到通用方法. 世界各地成像设备、协议以及疾病流行率的差异 法律伦理问题 事故责任承担问题]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Medical</category>
        <category>Overview</category>
      </categories>
      <tags>
        <tag>Overview</tag>
        <tag>Medical</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(CVPR 2016) Deep region and multi-label learning for facial action unit detection]]></title>
    <url>%2F2018%2F01%2F17%2FDeep%20region%20and%20multi-label%20learning%20for%20facial%20action%20unit%20detection%2F</url>
    <content type="text"><![CDATA[Zhao K, Chu W S, Zhang H. Deep region and multi-label learning for facial action unit detection[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2016: 3391-3399. Overview 在人脸AU检测中，存在两种常见的任务 Region Learning (RL) Multi-label Learning (ML)论文将两者进行结合，提出能够同时解决上述两个任务的框架Deep Region and Multi-label Learning (DRML)。 框架中包含一个region laryer结构。 region layer可通过两种方式设计 locally connected layers (LCN). 每个像素点对应一个卷积核（参数量较大） conventional convolution layers. 首先将输入特征图分成n*m个region, 每个region内的像素点共享卷积核权重（即每个region通过一个卷积层） DRML框架特点 end to end训练 non-linear模型论文在BP4D和DISFA数据集上进行实验，对比F1-score和AUC评价指标。 Region Learning 通过传统方法识别特定的区域来提高检测性能（类似于Attention）。例如patch-based方法，首先将图像划分为patch，然后将patch分类为普通patch和特定patch来描述不同的表情 patch-based 缺点. easily fail on faces with modest or large pose （某些patch中部分相关，但被排除在外，没有用于识别，从而导致性能降低） Multi-label Learning 传统的AU检测方法（AdaBoost、SVM等）都是对某个AU进行检测 ML对于每个表情，同时预测多个AU。 能够在一定程度上解决正负样本不平衡的问题 评价指标 F1-score. precision和recall的调和均值，常用于AU检测 AUC. 量化true positive和false positive之间的关系 DRML结构 大多数表情分析都使用很小的人脸图像（如4848）作为输入，为了避免人脸微小细节的丢失，论文使用**170170**大小的图像作为输入。 Loss Function Region Layer 人脸图像相对于自然图像更加结构化，不同的人脸区域具有不同的局部统计。因此，可使用LCN（参数太多）或者区域卷积进行处理 可看做是对每个区域进行Attention操作 结构包含三部分 patch clipping （论文采用8*8 grid） local convolution identity addition (避免梯度消失；如果patch与AU检测无关，it would be easier to directly forward the patch than learning a filter bank to reduce the patch’s effect) Region可视化使用saliency map（通过计算每个像素点对于某个特定AU的梯度级数）进行可视化。 与相关工作的比较 DRML受JPML启发，但存在不同之处 JPML通过对数据集统计，定义AU关系 JPML使用manually-crafted feature (SIFT) JPML轮流学习PL和ML JPML线性 Experiments 200200图像random crop为170170，并进行horizontally mirrored。 DRML收敛更快，训练loss更低，更接近gt统计 DRML比LCN速度快；ConvNet（去掉Region Layer的DRML）比AlexNet速度慢（由于卷积核为11*11） BP4D&amp;DISFA实验结果 learned features are of lower dimension, more than 40% of learned features for AlexNet, LCN, and DRML, are zeros Multi-label训练提高了效果 当训练数据很少时，multi-label学习能够减少imbalance的影响]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Face</category>
      </categories>
      <tags>
        <tag>Face</tag>
        <tag>Facial Action Units</tag>
        <tag>Multi-label Classification</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(CVPR 2017) Fatauva-net:An integrated deep learning framework for facial attribute recognition, action unit (au) detection, and valence-arousal estimation]]></title>
    <url>%2F2018%2F01%2F17%2FFatauva-net%3A%20An%20integrated%20deep%20learning%20framework%20for%20facial%20attribute%20recognition%2C%20action%20unit%20(au)%20detection%2C%20and%20valence-arousal%20estimation%2F</url>
    <content type="text"><![CDATA[Chang W Y, Hsu S H, Chien J H. Fatauva-net: An integrated deep learning framework for facial attribute recognition, action unit (au) detection, and valence-arousal estimation[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshop. 2017. Overview 目前人脸表情识别的两种主流方式为 Action Units (AUs) Valence-Arousal space (V-A space) 结合上述两种方式，论文提出一种能够同时用于 人脸属性识别 AU检测 V-A估计3种任务的集成深度学习框架FATAUVA (Facial Attribute Recognition, Action Unit Detection, Valence-Arousal Estimation)。 在FATAUVA框架中 将Attribute层的输出作为中间特征，用于后续AU检测 将AU层的输出作为中间特征，用于后续V-A估计 训练过程 利用CelebA数据集训练Core Layer和Attribute Layer 固定Core Layer和Attribute Layer权重, 利用FERA2015数据集训练AU Layer 固定Core Layer, Attribute Layer和AU Layer权重，利用AFF-Wild Challenge训练V-A Layer V-A space分为两个维度 相关数据集 cross-age celebrity dataset (CADA) [Attribute] CelebA [AU] FERA2015, [AU] BP4D (Video) [AU] SEMAINE (实验环境Image) [V-A] AFF-Wild Challenge 训练集共253个视频，每帧都有标注；测试集47个视频 网络结构 Attribute Layer分为四个子层：Face、Eye、Eyebrow、Mouth 论文从CelebA数据集中选出10种人脸属性，并将这10种属性归属到最相关子层代表的区域中（通过在子层后连接相应的2-way FC层进行预测，每种属性对应一个FC层）。 AU Layer将AUs归属到最相关的Attribute子层代表的区域中（通过在子层后连接相应的AU Conv层，并连接2-way FC层进行预测）。 V-A Layer将AU分为两组（Valence和Arousal），每组AU concat在一起，输入后续Conv层以及FC层。 Convolutional Block使用PolyNet中的块结构 Core Layer 8 rPoly-2 blocks Attribute Layer 2 rPoly-2 blocks AU Layer 2 rPoly-3 blocks V-A Layer 2 rPoly-3 blocks Experiments 数据预处理 Attribute和AU数据集 使用MTCNN截取人脸区域 V-A数据集 使用数据集给定的bounding box截取人脸区域 对每个AU的预测是一个二分类问题。由于正负样本比例不平衡，实验对较少的AU进行over sampling，对负样本进行down sampling. 将V-A得分量化到[-5,5]范围，进行可视化 由于样本分布不平衡，实验同样进行over sampling和down sampling. Loss Layer在Attribute Layer和AU Layer后连接3层FC，最后对2维输出做softmax操作。 在V-A Layer后连接3层FC，并使用了两种loss class-based 将[-5, 5]范围的得分离散化为11种类别。选择top 3得分：（1）如果得分连续（1,2,3或1,3,2），进行加权求和得到最终得分。（2）如果得分不连续，取top 1得分作为最终得分。 regression-based 结合center loss和smooth L1 loss x 倒数第二层FC输出的特征 c 类别y的中心（倒数第二层FC输出对应类别y的特征的均值） y 预测值 y^{~} ground truth t L1与L2之间的转折点 Attribute Recognition实验结果 AU Detection实验结果 V-A Estimation实验结果 使用AU能够提高V-Aestimation结果 使用regression-based loss优于class-based loss CCC: Concordance Correlation Coefficient]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Face</category>
      </categories>
      <tags>
        <tag>Face</tag>
        <tag>Facial Action Units</tag>
        <tag>Multi-label Classification</tag>
        <tag>Valence-Arousal space</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(ICCV 2017) DeepCoder:Semi-Parametric Variational Autoencoders for Automatic Facial Action Coding]]></title>
    <url>%2F2018%2F01%2F17%2FDeepCoder%3A%20Semi-Parametric%20Variational%20Autoencoders%20for%20Automatic%20Facial%20Action%20Coding%2F</url>
    <content type="text"><![CDATA[Linh Tran D, Walecki R, Eleftheriadis S, et al. DeepCoder: Semi-Parametric Variational Autoencoders for Automatic Facial Action Coding[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2017: 3190-3199. Overview 人脸表情可以编码成一系列的面部活动单元(facial action units, AUs)及其对应的活动强度(intensity). 而变分自编码器(VAE)能够通过无监督学习（重构loss+KL loss）提取数据的隐含表达（latent representation）。因此，对于人脸AU强度估计的任务可分为两个步骤 利用VAE提取人脸特征 使用分类器对特征进行AU活动强度估计 另一方面，non-parametric方法（如Gaussian Process）的效果优于parametric，但该方法只适用于小样本数据，无法很好地处理大样本数据。因此，论文将两者进行结合，提出semi-parametric的DeepCoder框架 parametric VC-AE (Variational Convolutional AEs) non-parametric VO-GPAE (Variational Ordinal GP AEs)并在DISFA和FERA2015数据集上进行实验验证。 FACSFacial Action Coding System 定义30多个面部肌肉活动单元，及其活动强度评分标准。 框架结构 VC-AE包含两部分loss KL loss (Z0) reconstruction loss (x-&gt;Z0-&gt;x’) 实验中使用warming strategy, 额外加入了AU强度估计loss VO-GPAE包含三部分loss KL loss (Z0) reconstruction loss (Z0-&gt;Z1-&gt;Z0) 强度估计loss (Z1-&gt;Y) Joint LearningLoss function VO-GPAE中的covariance function计算量会随着数据量的增多而增加，因此论文提出leave-subset-out策略，将训练集X分为不相交的两个子集X_R和X_L. X_R用于训练VC-AE, X_L用于训练VO-GPAE, 且X_R&gt;&gt;X_L. Experiments NLPD negative log-predictive density for reconstruction error ICC intra-class correlation, agreement between annotators 在Z1空间中模型将每个点都fit到一个独立的cluster中，从而使得对Z1空间上的特征进行AU强度估计效果更好。]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Face</category>
      </categories>
      <tags>
        <tag>VAE</tag>
        <tag>Face</tag>
        <tag>Facial Action Units</tag>
        <tag>Multi-label Classification</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(ECCV 2016) Stacked hourglass networks for human pose estimation]]></title>
    <url>%2F2018%2F01%2F03%2FStacked%20hourglass%20networks%20for%20human%20pose%20estimation%2F</url>
    <content type="text"><![CDATA[Newell A, Yang K, Deng J. Stacked hourglass networks for human pose estimation[C]//European Conference on Computer Vision. Springer International Publishing, 2016: 483-499. Overview 论文提出一种用于single person pose estimation的 repeated bottom-up, top-down (Hourglass) intermediate supervision模型结构（Stacked Hourglass Networks），能够captures and consolidates information across all scales of the image. 论文在 FLIC MPII数据集上是进行实验 模型结构 Hourglass图中每个box都是一个residual结构. 在top-down过程中只使用upsampling，不适用deconv. Bottom-up. Conv + ReLU + BN + Max pooling Top-Down. Upsamping + Add Residual Block &amp; Full Network 网络输入256x256，输出64x64 整个网络最开始使用一个Conv(7x7, 2s) 所有residual block输出通道数为256 网络中的Hourglass结构不共享参数 intermediate supervision使用相同gt Experiments 数据处理 对于多人情况. MPII训练集和测试集都提供了target person的中心点、scale (相对于200pixel的倍数)，可根据这些信息crop person, resize到256x256，再训练. 另外，可移动target person中心点到图像中心 Data augmentation. 旋转(±30°), 缩放(.75-1.25), 不使用平移 训练&amp;测试 使用MSE计算loss 测试时，使用origin image和flip image的平均结果作为最终预测 评价标准. FLIC：normalized by torso size, MPII： normalized by head size 实验结果 Ablation Multiple People Occlusion]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Body</category>
        <category>Pose</category>
      </categories>
      <tags>
        <tag>Pose</tag>
        <tag>Human Body</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(CVPR 2016) Convolutional pose machines]]></title>
    <url>%2F2018%2F01%2F03%2FConvolutional%20pose%20machines%2F</url>
    <content type="text"><![CDATA[Wei S E, Ramakrishna V, Kanade T, et al. Convolutional pose machines[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2016: 4724-4732. Overview 论文将CNN与Pose Machine相结合，提出Convolutional Pose Machine (CPM) Multi-stage结构. 将stage_n的预测输入到stage_{n+1}中进行refine Intermediate supervised &amp; end-to-end. 每个阶段的预测都计算loss。Forward完成后，各阶段loss一起回传. 解决gradient vanish问题 级联CNN. 实现long-range spatial modeling 相关工作 使用CNN直接预测人体关键点. 没有保留空间不确定性，导致accuracy很低 论文中CPM使用heap map. 基于数据集中人体关键点gt构建grid，并在关键点的位置加上Gaussian分布，形成heap map。最后通过MSE计算loss 数据集 MPII. 16 key points, 全身（单人/多人） Extended Leeds Sports Dataset (LSP). 14 key points, 全身（单人） FLIC. 9 key points, 全身（单人） 评价指标 PCKh. 记头部框对角线长度距离为d，预测点与gt之间的距离小于d表示预测正确 PCKh-0.5. 0.5d距离内表示预测正确 CPM stage_1的输入图像可以与后续stage的输入图像不同 每阶段预测结果(batch_size, body_part + 1, h’, w’) Stage 1 predicts part beliefs from only local image evidence. 虽然stage_1的预测结果较差，但能为后续stage的预测提供信息 LossMSE Pytorch版本代码 Center Map (中心点的高斯分布). 训练阶段，数据集label直接给出中心点(或根据关键点坐标计算得出). 中心点计算方法（关键点最大最小坐标和，求平均） 根据中心点生成center map方法 Experiments 模型比较 实验结果]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Body</category>
        <category>Pose</category>
      </categories>
      <tags>
        <tag>Pose</tag>
        <tag>Human Body</tag>
        <tag>CPN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(2017) Going deeper into action recognition:A survey]]></title>
    <url>%2F2018%2F01%2F03%2FGoing%20deeper%20into%20action%20recognition%3A%20A%20survey%2F</url>
    <content type="text"><![CDATA[Herath S, Harandi M, Porikli F. Going deeper into action recognition: A survey[J]. Image and Vision Computing, 2017, 60: 4-21. Overview 论文介绍了Action Recognition方面的四种深度学习结构 Spatiotemporal Network Multiple Steam Network Deep Generative Network Temporal Coherency Network以及Action Recognition数据集（KTH、Weizmann、Hollywood2、HMDB-51、UCF-101、Sports 1-M）。 Spatiotemporal Network 3D CNN 在实际中，加入一些补充信息（optical flow）训练网络能得到更好的性能。将时间信息输入网络的过程称为fusion，有以下3种机制 slow fusion. 同时输入视频中的几个片段。在foveated结构中，输入context stream的同时，还输入了fovea stream（图像中心；假设在拍摄时，会将重要的内容移动到视野中心） early fusion. 输入相邻帧集合 late fusion. 逐帧输入进行处理，最后将所有帧的特征融合在3D CNN结构中，使用更长的帧能够提高效果；将3D卷积核分解为2D卷积核和1D卷积核能够减少参数。 RNN 先用3D CNN提取特征，再输入到LSTM中 LRCN（Long-term Recurrent Convolutional Network） Multiple Stream Network 两个并行输入 基准帧 连续光流特点 使用ImageNet预训练权重 光流 early fusion multi-task训练 （由于数据集较小，因此使用多数据集进行训练，每个数据集对应一个分类层）在中间层进行fusion，既能提高效果，也能减少参数。 Deep Generative Models 时间序列维度的生成预测是一个无监督问题。 Dynencoder分为三层 输入帧x_t得到h_t 根据h_t预测h_{t+1} 根据h_{t+1}生成帧x_{t+1}先分别预训练每层，再end-to-end fine tuning. LSTM Autoencoder分为两层 encoder LSTM decoder LSTM （合成和预测） Adversarial方法训练 Temporal Coherency Network 一种弱监督形式。模型判断一段视频帧是否时序正确。 Siamese Network判断给定序列是否Coherency。相比ImageNet预训练权重，give more attention to human poses，并且能够提高准确度。 缺点. 视频段之间可能会出现场景变化（如Sports 1M数据集中的广告插播）。## 基于Siamese网络的并行结构将视频帧分为两个集合- prediction set X_p- effect set X_e将X_p特征进行变换，与X_e特征进行对比，从而识别行为。 数据数据 ##KTH&amp;Weizmann dataset controlled condition (limited camera motion, almost zero background clutter) limited to basic action (walking, running and jumping) HMDB-51&amp;UCF-101 非专业拍摄的Youtube视频(contain camera motion (and shakes), view-point variations and resolution inconsistencies) Actions are well cropped in the temporal domain, not well-suited for measuring the performance of action localization 包含subtle classes (chewing and talking or playing violin and playing cello)，要求网络深层次理解时空线索 Hollywood2&amp;Sports-1M 视角变换（view-point/editing complexities） 行为只发生在视频中某个很小的clipsSports-1M中还包含观众和广告条 未来发展 knowledge transfer &amp; domain adaptation 算法混合（3D CNN、temporal pooling、 optical flow frames、LSTM） 提升性能（data augmentation、foveated architecture、distinct frame sampling strategis） 实际应用 通常会涉及到joint dection（人体关键点检测） fine-grained行为识别，而非识别所有类别的行为]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Body</category>
        <category>Action Recognition</category>
      </categories>
      <tags>
        <tag>Overview</tag>
        <tag>Action Recognition</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(CVPR 2016) Compact Bilinear Pooling]]></title>
    <url>%2F2017%2F12%2F30%2FCompact%20Bilinear%20Pooling%2F</url>
    <content type="text"><![CDATA[Gao Y, Beijbom O, Zhang N, et al. Compact bilinear pooling[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2016: 317-326. Overview 此前提出的full bilinear模型存在一些问题(channel=512, k=1000时) 输出512512(260000)维度，导致后续的分类器参数很大(1000512*512≈250 million) storage expensive (2TB) 无法扩展到spatial pyramid matching、domain adaptation等 在few-shot learning上存在困难（如参数太多很容易过拟合） 基于上述问题，论文提出两种compact bilinear pooling方法 Random Maclaurin Projection (RM) Tensor Sketch Projection (TS) 具有以下特点 减少两个数量级维度的同时，几乎不降低性能 back-propagation efficient且end-to-end train 实际上，full bilinear方法在只使用2%特征维度情况下，与论文提出方法的性能相似。因此，98%的特征都是冗余的。 与其他方法比较 数据集 CUB（鸟类） MIT（室内场景） DTD（结构） Pooling方法 circular convolution（循环卷积）a=[1,2,3,4], b=[5,6,7,8], a*b计算过程 a逆时针固定在圆上，b顺时针固定在圆上。对应相乘求和结果y(0)=66. a顺时针旋转一位，计算y(1)=68 最终y的维度与a,b相同 Bilinear Sum Pooling(h, w, c)-&gt;(h,w, cc)-&gt;(cc) Compact Bilinear Models Full Bilinear Pooling S. set of location, 数量为height*width X. local descriptors, 输入特征图每个点的值(长度为channel数c)维度变化(h, w, c)-&gt;(h, w, cc)-&gt;(cc) Compact Bilinear Pooling找到Φ(x)∈R^d使得 $Φ(x) ≈ xx^T$ Random Maclaurin Tensor Sketch pytorch版本代码过程 stream_1, stream_2;(b, 512, h, w), d=8000 初始化h_1, s_1, h_2, s_2;(512)，构造矩阵sparse_sketch_matrix1,sparse_sketch_matrix2;(512, 8000) reshape stream_1,stream_2;(bhw, 512) sketch=stream*sparse_sketch_matrix; (bhw, 8000) FFT(sketch_1)·FFT(sketch_2); (bhw, 8000) IFFT; (bhw, 8000) reshape (b, h, w, 8000) sum pooling; (b, 8000) 计算时间 在Caffe K40c GPU下 VGG16 forward backward时间为312ms Bilinear pooling 0.77ms TS(d=4096) 5.03ms，FFT计算量比矩阵运算大 Experiments Pooling Methods Full Bilinear Pooling Compact Bilinear Pooling. 相对于512*512（≈260000）特征维度，8000维度足够表示特征。fine tuning能够提高效果，但提升很小。 Fully Connected Pooling Improved Fisher Encoding. 对输出进行64 GMM编码 维数&amp;方式比较 在维度很小时，fine tune能够显著提升效果 维度在2000～8000合适 与PCA比较PCA Bilinear通过在bilinear layer前插入1*1Conv实现，初始化为PCA权重。 实验结果 Few Shot Learning 训练样本数量与分类器假设空间（hypothesis space）大小相关 实验证明低维度表示更适合few-shot learning]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Architecture</category>
        <category>Component</category>
      </categories>
      <tags>
        <tag>Fine-Grained Classification</tag>
        <tag>Component</tag>
        <tag>Compact Bilinear Pooling</tag>
        <tag>Few Shot Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(ICCV 2015) Bilinear CNN Models for Fine-grained]]></title>
    <url>%2F2017%2F12%2F30%2FBilinear%20CNN%20Models%20for%20Fine-grained%2F</url>
    <content type="text"><![CDATA[Lin T Y, RoyChowdhury A, Maji S. Bilinear cnn models for fine-grained visual recognition[C]//Proceedings of the IEEE International Conference on Computer Vision. 2015: 1449-1457. Overview 人类大脑双流假说 (two-streams hypothesis). 假说认为大脑中有两种视觉系统 腹流(ventral stream; what pathway). 参与物体识别 背流(dorsal stream; where pathway). 处理物体相对于viewer的空间位置 基于上述假说，论文提出bilinear模型，该模型可end-to-end训练，有助于fine-grained分类问题。 模型分为两条stream，分别负责 localization (where) [part detector] appearance modeling (what) [feature extractor]但最终实验表明两条stream并没有明显的界限，它们都趋向于激活特定的semantic part. 计算过程 得到两条stream输出的特征图后(h, w, c1), (h, w, c2) 首先，对应空间点进行外积操作，从而实现part-feature interaction. (h, w, c1*c2) 其次，进行sum-pooling操作. (c1*c2) 接着，进行signed square-root和L2归一化 最后，分类 数据集 (bird) CUB-200-2011. 11788张图片，200种鸟类 (aircraft) FGVC-aircraft. 10000张图片，100中飞机类型 (car) Cars. 16185张图片，196种车类型 实验结果]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Architecture</category>
        <category>Component</category>
      </categories>
      <tags>
        <tag>Fine-Grained Classification</tag>
        <tag>Bilinear CNN</tag>
        <tag>Component</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(CVPR 2018) Learning to segment every thing]]></title>
    <url>%2F2017%2F12%2F22%2FLearning%20to%20Segment%20Every%20Thing%2F</url>
    <content type="text"><![CDATA[Hu R, Dollár P, He K, et al. Learning to segment every thing[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018: 4233-4241. Overview 使用监督学习方式训练object instance segmentation任务，要求数据集有segmentation mask的标注。由于mask的标注成本非常高，现有数据集只有较少mask标注，而物体检测box的标注成本相对较低，现有数据集包含大量box标注。 因此论文提出一种叫做partially supervised (transfer learning)的训练方法，在包含所有box标注和少量mask标注的数据集上训练能够分割所有box标注对象的instance segmentation模型。对maks预测和box预测解耦的Mask R-CNN结构非常适合用于这种训练方法，论文称网络结果为Maskx R-CNN。 论文在COCO和Visual Genome数据集上进行实验，训练出能够检测3000种对象类别的instance segmentation模型。 思想 在视觉语义空间中，邻近的embedding向量在appearance或semantic上相近。可将box head（最后一层）权重参数和mask head（最后一层）权重参数看作embedding向量。同类别的两个向量在appearance上相似，因此可利用transfer从box head embedding向量学习出mask head embedding向量。 利用weight transfer function基于box权重预测mask权重，weight transfer function通过少量mask标注数据进行学习。由于box权重是针对所有box标注对象而言的，因此transfer后的mask权重也能够针对所有box标注对象， 从而能够预测数据集中mask标注对象以外的对象。换而言之，将category specific信息从box detectors迁移到instance mask predictors. 数据集 COCO. 为了模拟partially supervised instance segmentation, 将数据集分为两部分：使用box标注和mask标注、只使用box标注。 Visual Genome. 规模较大，只有box标注信息。 细节 训练数据划分 C=A∪B C. 数据集中所有对象类别 A. 含有mask标注的对象类别 B. 只有box标注的对象类别（已知mask标注，可以很容易得到box标注） Weight Transfer Function w_{det}. box head最后一层中的权重，可看作appearance-based visual embedding w_{seg}. mask head最后一层中的权重 Θ. 学习的参数，class-agnostic w_{det}的三种类型 w_{cls} w_{box} cat(w_{cls}, w_{box}) 训练使用A∪B训练box head，使用A训练mask head和τ. 训练方式可分为两种 State-wise Training 第一阶段只使用A∪B中的box标注训练模型。第二阶段固定conv和box head，使用A中的mask标注训练mask head和τ. End-to-end Joint Training（Mask R-CNN论文中表明multi-stask训练优于分别训练每个任务） Box loss和mask loss都直接回传，但在transfer分支上的mask loss回传至τ后停止（由于只有A的mask loss回传至w_{deg}，不存在B的mask loss，为了保持w_{det}在A和B之间的一致性）。 BaselineMask R-CNN with class-agnostic FCN mask head. 扩展：融合FCN+MLP Mask Head 两种mask head互补 FCN. capture detail MLP. capture gist 对baseline的FCN和论文transfer模型的FCN进行MLP融合来提高效果 Experiments on COCO 为了模拟partially supervised训练，将COCO中80个类别分为A(20, 类别包含为VOC数据集中，voc)和B(60, non-voc) Oracle Model. 同时利用A和B中的mask标注进行训练Mask R-CNN 实验结果 Ablation Experiments τ的输入 MLP融合 训练方式 Experiments on Visual Genome 使用VG数据集的box标注，COCO数据集的mask标注，由于VG没有mask标注，无法计算AP，因为论文直接可视化结果]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Learning</category>
        <category>Transfer Learning</category>
      </categories>
      <tags>
        <tag>Detection</tag>
        <tag>Segmentation</tag>
        <tag>Mask R-CNN</tag>
        <tag>Maskx R-CNN</tag>
        <tag>Weight Transfer</tag>
        <tag>Transfer Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(ICCV 2017 Best Paper) Mask r-cnn]]></title>
    <url>%2F2017%2F12%2F22%2FMask%20r-cnn%2F</url>
    <content type="text"><![CDATA[He K, Gkioxari G, Dollár P, et al. Mask r-cnn[C]//Proceedings of the IEEE international conference on computer vision. 2017: 2961-2969. Overview 论文提出一个用于object instance segmentation的通用框架Mask R-CNN. 该框架在Faster R-CNN的基础上 并行增加了一个预测对象mask的分支（全卷积网络，对每个RoI进行预测） 将RoI Pooling操作改为RoI Align操作（将量化操作改为双线性插值操作） Mask R-CNN 5fps on GPU，论文进行object detection、instance segmentation和human pose estimation实验。 分割任务 Semantic segmentation(pixel labeling task).对每个像素点进行类间分类，不区分类内实例（例如图像中两只狗的像素点都分类为狗，不区分两只狗的像素点） Instance segmentation(object detection task). 检测图像中每个对象，并对每个对象实例进行分割 RoI Pooling的量化操作 Faster R-CNN主要用于物体检测任务，其中的RoI Pooling过程存在两次量化 浮点数RoI缩小feature_stride后，对应到共享特征图上的边界量化 RoI对应特征图进行RoI Pooling操作时，划分H*W区域的边界量化 量化操作会导致共享特征图上的RoI与输入图像之间存在错位问题，对分类和检测任务的影响并不大，但是对像素级别mask预测的影响较大（输入与输出之间并没有pixel-to-pixel alignment）。 RoI Align操作 将RoI Pooling中的量化操作改为双线性插值操作，RoI Align是提高准确度的关键操作。 考虑 Scale (feature_stride)为16的共享特征图 相对于输入图像的预测RoI x 量化操作 计算RoI相对于共享特征图的点坐标x/16.0 量化取整数点坐标round(x/16.0) 在共享特征图上采样该整数点的值 Align操作 计算RoI相对于共享特征图的点坐标x/16.0 在共享特征图上使用双线性插值采样该小数点的值 Mask R-CNN Mask R-CNN分为两个阶段 RPN预测RoIs 并行预测每个RoI的box (class, box offset)和binary mask. 看做两个分支 解耦mask与class FCN. class预测包含在mask预测中，存在类间竞争问题（per-pixel softmax, 多类别交叉熵）。即mask中每个像素点是一个k维softmax向量（或理解为对每个RoI预测一张mask，像素点分为k类型） Mask R-CNN. class预测从mask预测中分离出来，不存在类间竞争问题（per-pixel sigmoid, binary loss）。即mask中每个像素点是k个sigmoid值（或理解为对每个RoI预测k张mask，每张mask中的像素点分为background与foreground两种类型） Multi-task Loss对于每个RoI，有3个loss L_{cls}. log loss L_{box}. smooth L1 L_{mask}. binary cross entropy loss, 只计算对应类别的mask loss. 网络结构 Backbone. ResNet, ResNeXt, FPN Head. cls, box, mask 细节 训练 RoI与gt box的IoU大于等于0.5为正例，其余为负例 采样RoI正负比例1:3 Mask target为RoI与gt mask的交集 shorter side 800 测试 (与训练阶段的区别) 对预测的box进行NMS过滤 选择top 100个预测box进行mask预测 Instance Segmentation 实验结果 Ablationsclass-agnostic mask (29.7AP，m*m mask)与class-specific mask(30.3AP，m*m mask per class)效果相似。 边界框检测使用mask分支的Mask R-CNN比没有使用mask分支的Faster R-CNN&amp;RoI Align效果好 Human Pose Estimation 考虑身体部位共k个关键点（肩膀、肘等），则对每个RoI预测k张mask，每张mask中只有一个像素点为foreground，因此对m*m mask做softmax操作. 实验结果 Cityscape数据集扩展实验]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Object Detection</category>
      </categories>
      <tags>
        <tag>Detection</tag>
        <tag>Segmentation</tag>
        <tag>Mask R-CNN</tag>
        <tag>RoI Align</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(NIPS 2015) Faster R-CNN:Towards real-time object detection with region proposal networks]]></title>
    <url>%2F2017%2F12%2F22%2FFaster%20R-CNN%3A%20Towards%20real-time%20object%20detection%20with%20region%20proposal%20networks%2F</url>
    <content type="text"><![CDATA[Ren S, He K, Girshick R, et al. Faster R-CNN: Towards real-time object detection with region proposal networks[C]//Advances in neural information processing systems. 2015: 91-99. Overview SPPnet和Fast R-CNN通过共享特征图的机制解决了proposal计算冗余的问题，因此网络的瓶颈在于region proposal计算。由于 神经网络通过GPU运算 Region proposal通过CPU运算，Selective Search 2s/image 两者在时间上差别太大，在GPU上重新实现region proposal算法虽然能解决问题，但是并没有利用到计算共享的思想。 因此，论文提出Region Proposal Network (RPN). RPN是全卷积网络，模拟预测每个position的object bound和score, nearly cost free (10ms/image). 模型结构 论文将RPN与Fast R-CNN合并，称为Faster R-CNN. RPN与Fast R-CNN共享输入图像的卷积特征图conv_map. RPN对conv_map进行计算输出RoIs，类似于attention机制 Fast R-CNN基于RoIs进行计算输出对象框、对象置信度 论文使用的VGG16网络，5fps on GPU. Faster R-CNN在VOC 2007/2012和MS COCO数据集达到state of the art。 多尺寸机制 Faster R-CNN 论文中的Faster R-CNN使用了两种网络 ZF net RPN与Faster R-CNN共享5层网络 VGG16 RPN与Faster R-CNN共享13层网络 RPN RPN结构RPN网络有3个Conv层组成 第一层Conv. 256 kernels for ZF net, 512 kernels for VGG16, n*n size (论文使用n=3) 第二层Conv(cls). 2k_anchors kernels, 11 size 第三层Conv(reg). 4k_anchors kernels,11 size Anchors对共享特征图上的每个点模拟生成k个anchors box. 对于一张hw的RPN特征图，共有hw*k个anchor box.上述方法为多尺寸anchors, 因此模型中只使用单尺寸输入图片和单尺寸卷积核。 测试阶段 输入一张图片，图片经过共享卷积层生成共享特征图 共享特征图经过RPN网络生成whk个anchor box (reg)和对应的whk个前景/背景分类概率 (cls) 选出topN个前景概率最大anchor box，并使用nms算法对anchor box进行过滤，得到最终输入到Fast R-CNN网络中的RoI (proposal或anchor box) 训练阶段与测试阶段的区别在于 通过nms生成RoI的数量不同（例如github代码中测试阶段保留300 RoI，训练阶段保留2000 RoI） 计算loss（分类交叉熵和预测框的误差） Loss Function基于gt_box定义anchor box为正例还是负例 与gt_box IoU最高的anchor box为正例 与gt_box IoU大于0.7的anchor box为正例 与gt_box IoU小于0.3的anchor box为负例 计算公式 i表示第i个anchor box 表示ground truth, 正例p_{}为1，负例p_{*}为0 L_{cls}为log loss L_{reg}为L1 loss λ默认为10 x表示预测 x_{a}表示anchor x^{*}表示ground truth 训练细节 随机抽取一张图片中的256个anchor计算一个minibatch中的loss. 在这256个anchor中，正负例数量为1:1。如果一张图片中正例少于128个，使用负例填充 模型使用ImageNet预训练权重 迭代训练每次迭代分为四个步骤 预训练权重初始化模型，训练RPN 使用RPN产生RoI，训练检测网络Fast R-CNN 使用检测网络权重初始化RPN，固定共享层，微调RPN 固定共享层，微调检测网络剩余部分 实现细节 使用单尺寸图片，将最短边resize到s=600. anchor scale采用128128, 256256, 512*512 aspect ratios采用1:1, 1:2, 2:1 对于1000600的图片，共大约20000(6040*9)个anchor。剔除超过边界的anchor后，剩余大约6000 anchor for training。选出前景概率topN的anchor，并使用0.7阈值的NMS算法过滤，2000个用于训练的RoI。 实验 VOC数据集 耗时 Anchor超参数 λ超参数 代码流程图 RoI是相对于输入图像而言的，因此在将RoI和共享特征图输入到RoI Pooling层的同时，还会输入一个scale参数，也称为feature_stride（表示共享特征图与输入图像的比例）。]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Object Detection</category>
      </categories>
      <tags>
        <tag>Detection</tag>
        <tag>Segmentation</tag>
        <tag>Faster R-CNN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(ICCV 2015) Fast r-cnn Segmentation]]></title>
    <url>%2F2017%2F12%2F22%2FFast%20r-cnn%2F</url>
    <content type="text"><![CDATA[Girshick R. Fast r-cnn[C]//Proceedings of the IEEE international conference on computer vision. 2015: 1440-1448. Overview 由于R-CNN和SPPnet存在一些缺陷，本篇论文在R-CNN的基础上进行改进，提出速度更快，准确度更高的Fast R-CNN结构(相对于R-CNN增加了一个RoI pooling层)。 对于特定一张图像A，R-CNN模型利用selective search方法对其提取N个proposal (RoIs)，输入特征提取网络中进行后续运算。由于N个proposal中存在大量重叠的内容，特征提取网络的运算存在冗余。 因此，Fast R-CNN利用特征图共享机制，首先将图像A输入到特征提取网络中得到特征图B，再将RoIs定位到B中，选择相应区域特征图输入到RoI pooling层进行计算。 可看作是在输入图像提取的特征图上进行region proposal，而非在输入图像上进行region proposal后再对proposal提取特征。 Fast R-CNN在PASCAL VOC 2012数据集上 相比于R-CNN，训练速度快9倍，测试速度快213倍. mAP到达66% (R-CNN为62%) 相比于SPPnet，训练速度快3倍，测试速度快10倍 R-CNN缺点 Multi state pipeline. 微调、SVM、bounding box回归 速度慢. 47s/image on GPU Expensive in space and time. 在训练SVM之前，需要将提取特征存入硬盘 SPPnet缺点 Multi stage pipeline 提取的特征需要存入硬盘 Fast R-CNN贡献 更高mAP Single stage 能够更新所有网络层 不需硬盘存储 结构与训练 RoI pooling 输入. 输入图像中的RoI在其特征图上对应的区域（RoI是相对于输入图像而言的，因此将RoI和共享特征图输入到RoI Pooling层的同时，还会输入一个scale参数，也称为feature_stride（表示共享特征图与输入图像的比例））。 指定超参数H*W(论文设置为77)，即RoI pooling层的输出大小。对于hw的输入特征图，RoI pooling层将其划分为H*W的网格，对每个网格进行max pool操作 预训练权重初始化网络 使用ImageNet预训练权重，将 最后一个max pooling层换成RoI pooling层 最后一个 (fc, softmax) 换成2个 (fc, softmax)分支 网络的输入为 图像 对应图像中的RoIs Fine-tuning 采样N张图片，每张图片采样R/N个RoIs. 论文将N设置为2，R设置为128，即对采样的两张图片，各采样64个RoIs 由于所属某张图片的各RoI能够共享从该图片提取的特征图，因此相比与对128张图片各采样1个RoIs的方法快64倍 Multi-task loss p由第一个分支输出，包含对k+1个类别的预测置信度，u为ground truth t由第二个分支输出，包含k个类别的bounding box regression offset预测. v为ground truth, 归一化到标准正态分布 [u≥1] 当u≥1时，值为1，否则为0 λ超参数设置为1 对于一张图片（包含2只猫，一只狗），共产生200个region proposal (RoI)，模型分类器包含4类（猫、狗、鸟和背景） cls loss 每个RoI会产生1个置信度向量[猫，狗，鸟，背景]，直接与 class groud truth向量[p1, p2, p3, p4]计算loss loc loss 每个RoI会产生针对3个类别（猫、狗、鸟）的bounding box偏移量BB猫, BB狗, BB鸟，只计算这个RoI的class ground truth对应的bounding box loss. 例如，如果一个RoI的class ground truth为狗，那么计算BB ground truth与BB狗的loss. 上述为训练阶段的操作，在测试阶段，对N个RoI预测结果使用NMS算法进行过滤。 Mini-batch sampling Mini-batch大小为R=128，每张图片64个RoIs，25%为正样本（IoU≥0.5），其余为负样本（IoU∈[0.1,0.5)，忽略小于0.1的RoI） 0.5概率水平翻转图像，扩大数据集 Scale invariance Brute force. 直接将图片resize 成固定大小 Image pyramid. 在训练阶段，每次随机采样一个特定的pyramid scale。 论文在试验中证明image pyramid对mAP提升很小，但耗时明显增加。因此论文采用第一种方法。 测试阶段 R设置为2000，计算得到每个RoI的预测和bounding box regression 使用R-CNN中的NMS方法减少预测结果RoI的数量（NMS仅在测试阶段使用） 使用Truncated SVD提速对于u*v的权重矩阵，将其分解为 U u*t Σ t*t V v*t将计算量从uv降低至t(u+v)，将t远小于u,v时。因此，可将fc层的W分解为两个fc层 ΣV (no bias) U (包含原fc层中的bias) 实验结果 VOC数据集增大数据集能提高准确度. 耗时 Fine-tune对于较深的网络，只微调fc层的结果明显差于微调卷积层+fc层的结果。 multi-task训练效果 Scale invariancesingle scale和multi scale性能相差不大，但multi scale耗时增加明显。 Softmax与SVM More proposal worse]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Object Detection</category>
      </categories>
      <tags>
        <tag>Detection</tag>
        <tag>Segmentation</tag>
        <tag>Fast R-CNN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(CVPR 2014) Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation]]></title>
    <url>%2F2017%2F12%2F22%2FRich%20Feature%20Hierarchies%20for%20Accurate%20Object%20Detection%20and%20Semantic%20Segmentation%2F</url>
    <content type="text"><![CDATA[Girshick R, Donahue J, Darrell T, et al. Rich feature hierarchies for accurate object detection and semantic segmentation[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2014: 580-587. Overview 论文提出R-CNN模型 (combine region proposals with CNN)，在VOC 2012物体检测任务上达到53.3% mAP，相对于当前best result提高了30%。 由于detection标注数量较少，而classification标注数量较多。因此 首先，基于classification标注信息预训练分类网络 接着，针对detection标注信息，选择合适的分类层替换分类网络中最后的分类层，进行微调 最终，利用该网络（去掉一些fc和softmax层）提取的4096维特征，训练SVM分类器 Modules R-CNN模型包含3个模块: Region Proposal, CNN, SVM. Region Proposals 选用selective search方法： 初始化n个region，组成region集合R 计算每两个region之间的相似度，加入相似度集合S中 选出S中相似度最大的region pair (r_{i}, r_{j})，将r_{i}, r_{j}合并为r_{t} 从S中剔除与r_{i}, r_{j}有关的region pair 计算r_{t}与剩余所有region的相似度，并加入S中 将r_{t}加入到R中 重复[加粗部分]直到S为空 Feature Extraction 输入为mean-subtracted 227*227 RGB，结构由5层CNN和2层FC组成，输出为4096维特征向量。对proposal进行transform的3种方法 将proposal放入tightest框中，并额外加入context信息 将proposal放入tightest框中，不加入context信息 warp 此外，每种方法都是用context padding p像素 测试阶段 使用selective search方法提取2000个region proposal. 最终用贪心non-maximum suppression算法reject proposal预测结果数量。 non-maximum suppression (NMS) 对于一个特定的分类 从region预测结果集合P中，选出最高得分的region A 遍历region集合，剔除与A的IoU大于指定阈值的region 将A存入最终目标结果集合Q中 重复上述3个步骤，直至P为空，则Q为最终所求的预测regions 对N个类别重复上述整个过程。 测试耗时13s/image on GPU, 53s/image on CPU. 主要耗时在于dot product和NMS。在实际中，将一个图像中的2000个proposal组成一个矩阵，因此Feature extraction输出的特征为2000*4096. 训练阶段 训练分为3个阶段：监督预训练、微调、SVM分类器训练。 监督预训练使用auxiliary数据集(ILSVRC2012 classification)预训练分类网络。 Domain-specific fine-tuning 把分类网络的最后一层(1000 way)替换成随机初始化的(N+1背景 way)层。 正样本 与ground-truth的IoU≥0.5的region 负样本 minibatch 128 (32正样本+96负样本) SVM分类器 正样本 ground-truth 负样本 与ground-truth的IoU＜0.3的region. 忽略＞0.3的region Bounding Box Regression 为了提高localization准确度，论文进一步在R-CNN模型中加入了bounding box regression. ground truth和每个proposal的bounding box都由坐标(中心点坐标，宽，高)表示 通过学习一个映射，将proposal box映射到ground truth box. 实验结果]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Object Detection</category>
      </categories>
      <tags>
        <tag>Detection</tag>
        <tag>Segmentation</tag>
        <tag>R-CNN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[条件随机场&非局部均值去噪&双边滤波]]></title>
    <url>%2F2017%2F12%2F19%2F%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA%26%E9%9D%9E%E5%B1%80%E9%83%A8%E5%9D%87%E5%80%BC%E5%8E%BB%E5%99%AA%26%E5%8F%8C%E8%BE%B9%E6%BB%A4%E6%B3%A2%2F</url>
    <content type="text"><![CDATA[如何轻松愉快地理解条件随机场（CRF）非局部均值去噪（NL-means）图像非局部均值滤波的原理Bilateral Filtering(双边滤波) for SSAO 条件随机场 对一系列连续时间段的照片分类时，为了提高分类器的性能，常常将相邻照片的标签信息考虑进来，而非单独考虑某张照片进行分类——条件随机场(CRF). 词性标注中的特征函数 考虑对标注序列评分问题。定义CRF中的特征函数，如 s 需要标注词性的句子 i 句子s中第i个单词 l_{i} 第i个单词标注的词性 l_{i-1} 第i-1个单词标注的词性 特征函数输入0或1，0表示标注序列不符合特征函数的特征。该特征函数仅考虑当前单词i的标签与其前一个单词i-1的标签来进行判断，因此称为线性链CRF。 特征函数到概率 定义好一组特征函数及其权重λ后，即可对标注序列进行评分。 第一个Σ 所有特征函数对某个标注序列的评分求和 第二个Σ 某个特征函数对某个标注序列中每个单词的评分求和 进行指数化和标准化即可得到标注序列的概率值（属于某类的概率） 特征函数例子 l_{i-1}是介词，l_{i}是名词，则为1 l_{i}为副词，且第i个单词以”ly”结尾，则为1 如果i=1，l_{i}为动词，且句子以”?”结尾，则为1 与逻辑回归比较CRF可看做逻辑回归的序列化版本。逻辑回归是用于分类的对数线性模型，CRF是用于序列化标注的对数线性模型。 与HMM比较 HMM也能解决词性标注问题，其思路是生成方式。即已知标注序列的情况下，判断生成该标注序列的概率。 p(l_{i} | l_{i-1}) 转移概率，如l_{i-1}=介词，l_{i}=名字，表示第i-1个词为介词，第i个词为名词的概率 p(w_{i} | l_{i}) 发射概率，如l_{i}=名词，w_{i}=”ball”，表示第i个词为名字，则该词为”ball”的概率 CRF比HMM强大，每个HMM模型都等价于某个CRF. 且CRF能解决许多HMM无法解决的问题。 对HMM取log 与CRF进行比较 如果把HMM中的概率看成CRF中的权重，可发现每个HMM转移概率能够定义成一个对应的CRF特征函数，并乘以权重（HMM的概率）。 HMM中的发射概率同理。 CRF优势 CRF可定义大量不同的特征函数 HMM具有局部性，只考虑当前单词i与其前一个单词i-1. CRF可定义具有全局性的特征函数 HMM中的log概率值小于等于0，即转换成CRF后，其权重受到限制。而CRF中每个特征函数的权重可是任意值 非局部均值去噪 基本思想：当前像素值的估计由图像中与其具有相似领域结构的像素加权平均得到。该算法能够在去噪的同时，最大程度保留图像细节特征。 算法实现 理论上，该算法应搜索整个图像范围，但实际考虑到效率问题，会设定两个固定大小窗口。 搜索窗口（搜索范围） 邻域窗口（y为中心） 邻域窗口在搜索窗口中滑动，根据邻域间相似性确定各像素y的权值w(x,y)。 含噪声图像为v，去噪后的图像为u，其计算方式为 权值计算方式为 V(y) 以y为中心的邻域. ||V(x) - V(y)||^2 两邻域间的距离 Z(x) 归一化系数 h 平滑参数，控制高斯函数的衰减成都，h越大，高斯函数变换越平缓，去噪水平越高，但图像越模糊。h取值应以图像中噪声水平作为依据 其中 Bilateral Filtering(双边滤波) 在滤波算法中，目标点的像素值通常由其周围邻域中的像素值所决定。 2D高斯滤波 2D高斯滤波算法是对其一定范围的邻域内像素值赋以不同高斯权重值，进行加权平均。权重因子基于两像素点之间的空间距离得到，即离目标像素点距离越近，对最终结果贡献越大。 ξ 邻域像素点坐标 x 目标像素点坐标 f 图像 c(ξ, x) 基于两像素点空间距离的权重 k_{d}(x) 单位化 但高斯滤波存在缺陷，只考虑了像素之间的空间位置关系，而没有考虑考虑像素值之间的相似程度，因此会导致边缘信息（图像中不同颜色的区域）丢失。 基于像素值关系的权重 Bilateral中加入了另一部分权重 s(f(ξ), f(x)) 为基于两像素点的像素值相似关系的权重。 Bilateral滤波 综合考虑有 其中 距离超过一定程度的像素点对目标像素点影响很小，可忽略。限定局部子区域后的离散化公式为 可视化 对于带有噪声的图片 蓝框中心为目标像素所在位置，则其对应的高斯权重为 双边权重为 可看出Bilateral加入的相似程度部分将图像左侧与目标像素点的像素值差值过大的点滤去，从而保持了边缘。 进一步可视化，双边滤波后的图像为 原图像为 高斯滤波后的图像为 可看出高斯滤波后的图像存在线性变化边缘，即边界的丢失。而双边滤波后的图像保持了边缘梯度。]]></content>
      <categories>
        <category>Knowledge Note</category>
      </categories>
      <tags>
        <tag>Non-local Means</tag>
        <tag>CRF</tag>
        <tag>Bilateral Filtering</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(CVPR 2018) Non-local Neural Networks]]></title>
    <url>%2F2017%2F12%2F19%2FNon-local%20Neural%20Networks%2F</url>
    <content type="text"><![CDATA[Wang X, Girshick R, Gupta A, et al. Non-local neural networks[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018: 7794-7803. Overview convolutional和recurrent操作建立的block，每次只能处理一个局部区域，基于非局部均值滤波的思想，论文提出具有通用性的非局部组件，能够capture long range dependencies. 非局部组件通过计算所有点的特征加权和作为目标点的响应。非局部操作能够capture相距较远的点在时间和空间上的关系。 论文使用非局部模型（非局部-残差网络） 在Kinetics和Charades视频数据集上做classification任务，性能超过目前的冠军 在COCO数据集上做object detection/segmentation和pose estimation任务，性能有明显提高 Repeated 由于convolutional和recurrent建立的block每次只能处理一个局部区域，因此需要通过重复操作来实现long range dependencies. convolutional操作通过堆叠(stack) CNN层实现 recurrent操作通过记忆+循环实现 但repeated实现方式存在一些缺陷 效率低下 优化困难 信息很难在相距较远的点之间来回传递(multi-hop dependency modeling多跳依赖建模)。（例如，对于100*100的图像，左上角的点和右下角的点在前几层CNN中并无关联） 非局部操作优点 直接计算任意两点之间的关系来capture long range dependencies，忽略点之间的距离 非局部操作高效，仅使用少量的层就能达到betst result, 额外增加的计算量少 保持输入大小不变 能capture空间和时间维度，比3D CNN更高效 Related Work 非局部图像处理图形模型 CRF能够被用于segmentation, 而非局部操作具有通用性，可用于分类和检测。 Self-attention 通过所有点的加权平均计算当前点的响应。self-attention可看做是非局部均值的一种方式。 Interaction Networks 使用pairwise interaction。其变体Relation Network使用到所有pair of positions. Video Classification Architecture 最通常的做法是结合CNN和RNN，但是前馈模型使用3D CNN，将预训练权重的2D卷积核inflating成3D卷积核。此外，optical flow和trajectory能提高性能。 Non-local Neural Networks Formulation通用形式为 i 输出位置点 j 所有位置点 x 输入 y 输出 f pairwise function, 计算点i和点j之间关系，输出为标量 g 计算representation of 点j C 归一化因子 区别 非局部操作涉及到所有位置点，而convolutional操作只处理局部区域，recurrent操作通常只涉及到当前时间状态与最近时刻状态(j=i或j=i) 非局部操作基于不同点之间的关系计算响应，而fc的权重通过学习得到。 非局部操作能够适用不同大小的输入，并且能够任意加入到网络中，而fc的输入大小需要事先固定，且通常只放到网络的最后一层。 Instantiations g函数使用线性embedding 通过2D CNN（空间）或3D CNN（时空）实现。 Pairwise function f函数有多种选择 Gaussian Embedded Gaussian 可写成softmax形式 Dot Product N为x位置点的总数，与Embedded Gaussian的区别在于没有softmax操作。 Concatenation [,]表示concat操作，wf为权重向量，将concated向量投影为标量。 Non-local Block x为residual连接. Block可以任意插入到预训练模型中，并保持模型的初始行为，即将W_{z}权重初始化为0. 当block使用在high level的特征图上，计算量很少。例如，T=4， H=W=14或7. 减少计算量 Θ和φ网络channel数量设置为输入的一半 在g和φ网络后进行下采样，如连接max pooling 层 Video Classification Model 2D ConvNet (C2D) 输入维度为32帧224224，其本质为1kk的卷积核，直接使用ImageNet预训练权重初始化。残差block中的卷积层相当于逐帧处理操作（卷积核和步长在帧的维度上为1）。 Inflated 3D ConvNet (I3D) 2D kk卷积核inflate为3D tkk卷积核。将kk训练权重扩展到t plane，并乘以缩放因子1/t. 两种inflate方式 inflate残差block中的33卷积核为33*3 inflate残差block中第一个11卷积核为31*1 I3D模型优于CNN+LSTM. Non-local network将non-local网络插入到C2D和I3D网络中。 实现细节 Train阶段 ImageNet预训练权重初始化 输入为32帧clips，从原始视频中random crop连续64帧，并丢弃剩余其他帧 输入尺寸为224*224，通过random crop得到 迭代400k次，学习率初始为0.01，每150k迭代减少10% momentum 0.9，权重衰减0.0001，dropout 0.5 fine tune模型是不固定BN层，能够防止过拟合 仅在W_{z}层后插入BN层，scale参数初始化为0，可保持整个non local block为全等映射，即保持预训练模型的初始行为 Inference阶段 均匀地从一整段视频中抽取10个clip，计算softmax得分，最终预测结果为10个softmax得分的平均。 Experiments on Video Classification Kinetics数据集246k训练视频，20k验证视频，包含400个人类动作分类。 Instantiations比较不同类型的非局部操作，得出结论非局部操作类型的影响并不大，性能的提升主要是由于非局部操作，而非attention机制。因此实验中默认使用Embedded Gaussian. Stage to Add Non local Blockres5的空间大小为7*7，因此可能是因为没有足够准确的空间信息，导致准确度略微下降。 使用更多的非局部block一般而言，更多的非局部block能提升性能准确度，multiple非局部block能够实现long range multi-hop communication (多跳交流). 从5 block R50 73.8%与base R101 73.1%，可以看出准确率的提升并不是因为层数加深，而是因为非局部block. 此外，实验也将加入的非局部block替换为标准残差block，发现准确度并没有提升。 时空上的非局部操作 非局部2D ConvNet vs 3D ConvNet 非局部3DConvNet使用I3D_{311}模型 更长的序列将每个输入clips增加到128帧，所有模型的准确度都有提升。 与目前最好的结果比较非局部网络方法在没有使用optical flow和其他技巧的情况下，取得较好结果。 Charades数据集8k训练视频，1.8k验证视频，2k测试视频，包含157个人类动作分类。 Experiments on COCO Detection与Segmentation 在不同规模的模型中加入非局部block均能提高准确度，说明非局部依赖并没有通过增加层数堆叠充分capture. 此外，单层非局部block只增加了不到5%的计算量，使用更多的非局部block结果反而有所下降。 Keypoint Detection]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Architecture</category>
        <category>Component</category>
      </categories>
      <tags>
        <tag>Classification</tag>
        <tag>Component</tag>
        <tag>Attention</tag>
        <tag>Non-local</tag>
        <tag>Video Classification</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(NIPS 2015) Spatial transformer networks]]></title>
    <url>%2F2017%2F12%2F19%2FSpatial%20transformer%20networks%2F</url>
    <content type="text"><![CDATA[Jaderberg M, Simonyan K, Zisserman A. Spatial transformer networks[C]//Advances in neural information processing systems. 2015: 2017-2025. Overview 虽然CNN的效果很好，但是仍然缺乏对数据的空间不变能力，从而限制了计算和参数的效率。因此，论文提出Spatial Transformer Network (STN)。 STN 在网络中对数据显式地进行空间操作（平移、旋转、缩放、裁剪、扭曲）。由于该操作可微，因此模型能够end to end训练。 根据输入数据，动态生成空间操作参数Θ。 网络参数直接通过loss回传进行学习。可直接添加到神经网络模型中，整个训练不需额外的监督信息加入。 空间操作后的数据是与后续特定任务高度相关的。另一方面，变换后的低分辨率数据比原始数据的计算效率更高。 通过对数据进行操作实现不变性，而不是对特征提取器（卷积核）。 适用的任务 classification co-localization spatial attention Spatial Transformers STN包含3部分 (Figure 2) localization network. grid generator. sampler. Localization Network 输入U(h, w, c) 输出空间变换参数Θ 网络可以是任何形式，如FCN、CNN等。仿射变换Θ的参数为6，投影变换参数为8，以及thin plate spline (TPS). 模型对最后一层的weight矩阵初始化为0，bias初始化为[[1, 0, 0], [0, 1, 0]]（仿射变换），即全等变换。 Parameterised Sampling Grid 首先根据采样网格大小（超参数）生成标准网格(t; x,y∈(-1, 1); (h, w, 2)). 利用空间变换参数Θ对其进行变换操作，生成采样网格(s; x,y∈(-1, 1); (h, w, 2)). Differentiable Image Sampling 通用的采样公式可写为 k为通用采样kernel; x, m, y, n为坐标点。Φ为kernel的参数。 对于整数采样kernel，公式简化为 取x+0.5下界整数，δ函数为Kronecker delta函数 对于双线性采样kernel，公式简化为 该公式可导 Spatial Transformer Networks 由于Θ显式地编码了变换，因此也可将Θ传入后续的网络，而非变换后的特征图（或图片）。 可用STN对特征图进行上采样或下采样。但是，用固定的、小空间支持的采样kernel（双线性kernel）进行下采样会造成影响。 STN可级联或并行在网络中。 Experiments Distorted MNIST 数据集distorted方式分为 R 旋转，±90°之间。 RTS 旋转+缩放+平移 P 投影 E 弹性形变（破坏性，不可逆） 所有模型都具有相同数量参数，分别使用3类变换操作：仿射变换(Aff)、投影变换(Proj)、薄板样条变换(TPS)。实验发现TPS最有效。 MNIST Addition 输入两张数字图片(h,w,2)，输出数字的和。 Street View House Numbers 每张图片有1～5个数字。因此，模型采用级联STN，并使用5个独立的softmax分类器，每个分类器包含一个空字符 Fine-Grained Classification CUB-200-2011数据集，模型采用并行STN结构。 Co-localization 使用半监督学习来定位图像中的物体。基于正确定位对象A与正确定位对象B之间的距离，比A与随机定位crop小的假设，构造hinge loss T表示crop，e为编码函数，α为margin，实验设置为1。数据集的构建操作为：将2828的数字图片放在8484背景中，并将从训练集中采样得到的16个随机6*6 crop放入背景中。当预测定位与ground-truth的交集大于0.5时，定义为预测正确。 Higher Dimensionnal Transformer 模型使用3D仿射变换和3D双线性插值操作。 另一种处理方法是：将3D空间投影到2D空间，例如]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Architecture</category>
        <category>Component</category>
      </categories>
      <tags>
        <tag>Classification</tag>
        <tag>Fine-Grained Classification</tag>
        <tag>Component</tag>
        <tag>STN</tag>
        <tag>Attention</tag>
        <tag>Localization</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Transformation]]></title>
    <url>%2F2017%2F12%2F19%2FTransformation%2F</url>
    <content type="text"><![CDATA[仿射变换与投影变换数值方法——薄板样条插值（Thin-Plate Spline）薄板样条函数(Thin plate splines)的讨论与分析关于Thin Plate Spline （薄板样条函数） 仿射变换 6个参数 投影变换 8个参数. 通常采用z=1时的平面。 薄板样条函数 插值 已知函数y=f(x)在N+1个点x1, x2, x3, …上的函数值y1, y2, …，但是未知函数f(x). 可通过插值函数p(x)来逼近f(x). 常用的插值函数有 多项式函数 对于N次p(x)有N+1个参数，由于N+1个参数满足N+1个约束条件，因此可求出p(x). 但N阶多项式必有N-1个极值点，得到的插值函数摆动较大，类似过拟合现象。 样条函数 即分段函数，表示在相邻点x_{k}和x_{k+1}之间用低阶多项式S_{k}(x)进行插值。分段线性插值和三次样条插值都属于样条插值。 TPS (Thin plate splines) 寻找一个通过所有控制点的弯曲最小的光滑曲面 弯曲最小由能量函数定义 对于插值问题：自变量x是2维空间中的点，函数值y也是2维空间中的点，给定N个自变量x和对应函数值y，求插值函数 使得 即求两个插值函数φ。根据能量函数可得TPS插值函数形式 其中c是标量，向量a(2, 1)，向量w(N, 1)，函数向量 φ有N+3个参数，而目前只有N个约束（即y=φ(x)，N对数据）。因此，添加三个约束 有 已知N对数据，即已知X和Y，也就是第1个矩阵和第3个矩阵，求第二个矩阵（薄板样条插值函数的参数） 令 有 即可求得插值函数的参数。更进一步，可将x方向和y方向的插值函数φ通过一个矩阵运算计算 该逆矩阵称为弯曲能量矩阵 TPS应用在STN网络上 对于STN而言，已知 2维空间中的自变量x（第一个矩阵，根据尺寸超参数生成的标准网络中的点） 薄板样条（TPS）插值函数的参数（第二个矩阵，由Localization Network生成） 求 2维空间中的函数值y（第三个矩阵，标准网络经TPS变换后，得到的采样网格。即标准网格中的点映射到输入特征图或图像网格中的点）。]]></content>
      <categories>
        <category>Knowledge Note</category>
      </categories>
      <tags>
        <tag>Transformation</tag>
        <tag>Thin-Plate Spline (TPS)</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(2017) Understanding Matrix capsules with EM Routing]]></title>
    <url>%2F2017%2F12%2F18%2FUnderstanding%20Matrix%20capsules%20with%20EM%20Routing%2F</url>
    <content type="text"><![CDATA[Hui J. Understanding Matrix capsules with EM Routing[J]. Blog.​ Nov, 2017.“Understanding Matrix capsules with EM Routing (Based on Hinton’s Capsule Networks)” Overview 向量capsule缺陷此前提出的capsule结构存在一些缺陷 利用pose向量的长度表示存在概率时，使用了squash函数将向量长度压缩至[0, 1]，这阻碍了一些有意义的目标函数的使用。 使用余弦角度测量两个pose向量之间的agreement，不能很好地区分quit good agreement和very good agreement. 而使用Gaussian cluster能使实现这点。 对于长度为n的pose向量，其变换矩阵有n*n个参数。而对于n个元素的pose矩阵，其变换矩阵只有n个参数。 矩阵capsule结构因此，本文提出一种新的capsule结构，其中包含 a logistic unit，表示该entity存在概率。 a 4*4 pose矩阵，通过学习表示entity与viewer之间的关系。 L层某个$capsule_i$的pose矩阵乘以viewpoint-invariant变换矩阵得到的结果为$capsule_i$对L+1层各$capsule_c$的pose矩阵的vote. Viewpoint-invariant变换矩阵通过学习能够表示part-whole关系。 每个vote都对应一个权重系数$r_i$,权重系数通过EM算法迭代更新。在本文中使用的迭代次数为3. 矩阵capsule结构在smallNORB数据集上相对于目前的state-of-the-art减小了45%的test errors. 并且更能够抵抗白盒对抗攻击。 Introduction Viewpoint与Pose矩阵 Viewpoint的改变会导致图像像素产生较大的变化，但对表示objet和viewer之间关系的pose矩阵而言，只会产生简单的线性影响。 随着viewpoint的改变，pose矩阵以一种协调的方式进行变化，因此不同部位votes的agreement保持恒定。 反向Attention由L+1层的所有$capsule_c$竞争L层的某个$capsule_i$，即权重系数和为1. 而正向Attention是由L层的所有$capsule_i$竞争L+1层的某个$capsule_c$。 EM迭代路由算法将L层各capsule_i看为一个data point，L+1层各$capsule_c$看为一个Gaussian模型。因L层各$capsule_i$的vote路由问题转化为对给定数量data point进行Gaussian聚类问题。例如，眼睛、鼻子、嘴巴的$capsule_i$都vote（聚成一个cluster） L+1层中某个$capsule_c$，即检测到人脸。 计算公式 cost表示某个$capsule_i$属于$caps_c$的一部分的概率。cost越低，则属于的可能性越大。 λ为超参数，b为描述$capsule_c$均值的cost，可学习。 EM路由算法 实验中设置的迭代次数为3. Capsule模型 ReLU+Conv1 5*5 kernel, 32 channel (A=32), stride 2. 输入: (b, c, 32, 32) 输出: (b, 32, 14, 14 ) PrimaryCaps 1*1 kernel, 32 channel (B=32), stride 1. 输入: (b, 32, 14, 14) 输出: (b, 32, 14, 14, 17) ConvCaps1 3*3 kernel (K=3), 32 channel (C=32), stride 2. 输入: (b, 32, 14, 14, 17) 输出: (b, 32, 6, 6, 17) ConvCaps2 3*3 kernel (K=3), 32 channel (D=32), stride 1. 输入: (b, 32, 6, 6, 17) 输出: (b, 32, 4, 4, 17) Class Capsule 可看做h*w kernel, 10 channel (分类类别), stride 1. 该层使用了Coordinate Addition方法，额外加入每个感知区域的xy坐标到vote的前两个元素中。 输入: (b, 32, 4, 4, 17) 输出: (b, 10, 17) Spread Loss 最大化目标类别的activation与其他类别activation的差值。m为margin，从0.2开始线性增长，避免dead capsule. Experiments samllNORB数据集包含5种类别的玩具图片：飞机、车、卡车 人类和动物。每种类别都有18个不同的视角(0-340), 9种高度和6种光照条件。图片大小为9696.实验中将其下采样为4848并做32*32 random crop操作。 更进一步，实验使用训练集不包含的viewpoint进行测试 对抗鲁棒性]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Architecture</category>
        <category>Capsules</category>
      </categories>
      <tags>
        <tag>Classification</tag>
        <tag>Cognitive Neuroscience</tag>
        <tag>EM Algorithm</tag>
        <tag>CapsulesNet</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[EM Algorithm]]></title>
    <url>%2F2017%2F12%2F18%2FEM%20Algorithm%2F</url>
    <content type="text"><![CDATA[EM算法学习(Expectation Maximization Algorithm) 凸函数 上凸函数 f[(a+b)/2] ≥ [f(a)+f(b)]/2 当a≠b时 f[(a+b)/2] &gt; [f(a)+f(b)]/2成立，那么称f(x)为严格的上凸函数，等号成立的条件当且仅当a=b,下凸函数与其类似。 Jensen不等式 如果f是上凸函数，X是随机变量，那么 f(E[X]) ≥ E[f(X)]. 如果f是严格上凸函数，那么 E[f(X)] = f(E[X]) 当且仅当p(X=E[X])，也就是说X是常量。 EM算法 Problem Definition 考虑一个参数估计问题，现有共n个训练样本，需有多个参数θ去拟合数据（高斯混合模型），那么这个log似然函数是 由于Θ中多个参数的某种关系，导致上面的log似然函数无法直接或梯度下降法求最大值时的Θ值。引入隐变量z，并使用Jensen不等式得到下界 (9)式紧下界为等号成立时，即随机变量 为常数。又因为 有 所以(9)成立的条件是 Q(zi)=P(zi|yi, Θ) 即后验概率。 Algorithm Step E步骤 已知（初始化）Θ和样本数据yi，计算Q(zi)。 M步骤 利用Q(zi)简化(9)中的多项式，从而可用梯度下降求偏导更新Θ值。 E步骤 根据Θ计算Q(zi). M步骤 根据Q(zi)更新Θ. … 迭代直至收敛。]]></content>
      <categories>
        <category>Knowledge Note</category>
      </categories>
      <tags>
        <tag>EM Algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(NIPS 2017) Dynamic Routing Between Capsules]]></title>
    <url>%2F2017%2F12%2F18%2FDynamic%20Routing%20Between%20Capsules%2F</url>
    <content type="text"><![CDATA[Sabour S, Frosst N, Hinton G E. Dynamic routing between capsules[C]//Advances in neural information processing systems. 2017: 3856-3866. Overview 基于认知神经科学领域中芒卡斯尔(V.B. Mountcastle)发现的大脑皮层功能柱结构(功能柱-微柱)，论文提出capsule概念。 Capsule定义 Capsule是一组神经元，其activity vector表示一个特定实体(entity)类型(object or object part)的实例化参数(instantiation parameters). Activity vector的长度表示entity存在的概率，方向表示instaniation parameters. Instantiation parameters includes pose (position, size, orientation), deformation, velocity (速度), albedo (反照率), hue, texture etc. Capsule机制L level的activity $capsule_i$ 通过transformation matrices predict L+1 level $capsules_j$的instantiation parameters. 当多个predictions agree, L+1层中某个$capsule_j$ become active. (即高层$capsule_j$代表了低层$capsule_i$的组合，如数字2的多个低层特征由多个$capsule_i$表示，同时满足这些特征时，某个代表这些特征组合的高层$capsule_j$被激活) 在CapsNet中使用了iterative routing-by-agreement机制：对于L+1 level中某个$capsule_j$的 activity vectors ($v_j$)和L level中的一个$capsule_i$对其作出的prediction ($\hat{u_{j|i}}$)而言，两者scalar product ($\hat{u_{j|i}} · v_j$)越大, A越倾向于将其output发送给这个$capsule_j$. CapsNet在MNIST数据集上优于CNN，最高达到99.75%左右。 Introduction 假设 假设，人类的多层视觉系统会在每个fixation上建立一个类似parse tree的东西。 假设，对于一个fixation，parse tree可以通过对一个固定的多层神经网络carve (类似于剪枝)得到。 每层会被划分为多个神经元组(capsule), parse tree中每个node都会对应一个active capsule (即parse tree中每个node代表一层神经网络中的一组神经元，该组神经元可称为capsule). 通过iterative routing机制，每个active capsule都会选择上一层中的某个capsule作为parent node. 对于higher level视觉系统，iterative routing机制将用于解决物体部分组合到整体的过程。 存在性表示方法 logistic unit. length of the vector of instantiation parameters (squashing function, 论文采用). Dynamic Routing 由于capsule输出的是向量，用dynamic routing机制能够确保capsule的输出send to合适的parent node. 对于L level的capsule A和L+1 level的capsule B. A首先计算prediction vector (A的output乘以weight matrix), 然后计算prediction vector与B ouput的scalar product, 反馈给AB之间的coupling coefficient. scalar product越大，对AB之间coupling coefficient的反馈越大，即-AB之间的coupling coefficient增加，A与其他parents之间的coupling coefficients减小(softmax function). 分割重叠数字 由于max pooling的routing机制会忽略local pool中大部分active feature detectors. 而routing by agreement more effective不存在这样的问题，因此能够分割高度重叠的object. Max pooling throw away information about the precise position of the entity within the region. 相比于CNN的改进 使用vector output capsule代替CNN中的scalar output feature detectors. 使用routing by agreement代替max pooling. Coding For low level capsules, location information is place coded by which capsule is active. As we ascend the hierarchy more and more of the positional information is rate coded in the real valued components of the output vector of a capsule. 从place coding到rate coding表明higher level capsule用更高自由度represent更复杂的entities, 即capsule的维度随着层级的增加而增加。 Capsule计算 Squashing function(L level $capsule_i$, L+1 level $capsule_j$) 使得较短向量长度缩放为0，较长向量长度缩放为1. $v_j$. capsule_j的输出向量。$s_j$. $capsule_j$的总输入向量。 即L level中所有$capsule_is$到L+1 level中特定一个$capsule_j$的prediction vector加权和。 Total input &amp; prediction vectors $u_i$. capsule_i的输出向量。$w_{ij}$. weight matrix.$u_{j|i}$. prediction value from a $capsule_i$ to a $capsule_j$.$c_{ij}$. coupling coefficients between a $capsule_i$ and a $capsule_j$. Coupling coefficients $b_{ij}$. the log priors probabilities that $capsule_i$ should be coupled to $capsule_j$. 由两个capsule (i和j)的location和type决定，而非当前input image决定。 通过测量$v_j$和$u_j|i$之间的agreement迭代refine $c_{ij}$. AgreementThis agreement is treated as if it were a log likelihood and is added to the initial logit, $b_{ij}$. Algorithm Margin Loss SVM损失函数 $m^{+}=0.9$, $m^{-}=0.1$ $T_c=1$, iff a digit present suggest $λ=0.5$ CapsNet结构 (Figure 1) Conv1256 kernels, 9x9, 1 stride, ReLu. 输入维度 . (batch_size, 1, 28, 28) 输出维度. (batch_size, 256, 20, 20) PrimaryCapsules 输入维度 . (batch_size, 256, 20, 20) 输出维度. (batch_size, 32, 6, 6, 8) 激活primary capsule过程可看作是图像render的逆过程。 PrimaryCapsules is a convolutional capsule layer, 包含32个通道，每个通道含有一个convolutional 8D capsules. 即每个primary capsule包含8个(9x9, 2 stride) conv unit. PrimaryCapsule共有(32, 6, 6)个capsule输出，每个输出是一个8维向量。 (6x6) grid中的capsule share weights，与CCN的卷积核原理相同。每个capsule输出a grid of vectors，而不是single vector output. DigitCaps 输入维度. (batch_size, 3266, 8) 输出维度. (batch_size, 10, 16) 10个16维的capsule. CapsNet中，只在PrimaryCapsules和DigitCaps层之间routing. 由于Conv1输出是1D, 不存在orientation to agree on, 因此不在Conv1和PrimaryCapsules层之间routing. 使用Adam optimizer, routing logit $b_ij$初始化为0. Reconstruction (Figure 2) 输入维度 . (batch_size, 10*16) 或 (batch_size, 16) 输出维度. (batch_size, 28* 28) Encourage the digit capsules to encode the instantiation parameters of the input digit. 在训练阶段，mask除了ground truth对应capsule之外的所有capsule. 而测试阶段，mask除了length最大的capsule之外的所有capsule. 目标为最小化sum of squared differences. Reconstruction loss乘以0.0005系数。 To summarize, by using the reconstruction loss as a regularizer, the Capsule Network is able to learn a global linear manifold between a whole object and the pose of the object as a matrix of weights via unsupervised learning. As such, the translation invariance is encapsulated in the matrix of weights, and not during neural activity, making the neural network translation equivariance. Therefore, we are in some sense, performing a ‘mental rotation and translation’ of the image when it gets multiplied by the global linear manifold. Capsules on MNIST 实验结果 使用routing和reconstruction regularizer能够提升性能。 CNN: 24.56M parameters. CapsNet: 11.36M parameters. Capsule单个维度代表的意义 (Figure 4)variations包括厚度、斜度和宽度。16维中几乎总有一维代表数字的宽度，一些维度代表combinations of global variation, 其他一些代表localized part of digit. Robustness to Affine Transformations由于natural variance in skew, rotation, style, etc in hand written digits, 使用CapsNet具有健壮性。 训练集 MNISTdigit placed randomly on a black background (平移). 测试集 affNISTMNIST digit with a random small affine transformation CapsNet stop when $train_{acc}=99.23%$, $test_{acc}=79%$.CNN stop when $train_{acc}=99.22%$, $test_{acc}=66%$. Segmenting Highly Overlapping Digit Routing可看做Attention机制 Dynamic routing可以看做是并行的attention机制，L+1 level的每个capsule都会attend一些L level的activive capsules，忽略其他的。因此，在对象重叠的情况下，也能识别多个对象。 The routing by agreement should make it possible to use a prior about shape of objects to help segmentation. It should obviate the need to make higher level segmentation decisions in the domain of pixels. MultiMNIST dataset 两张image各方向移动4个pixel，构成36x36 image. MultiMNIST result shows that each digit capsule can pick up the style and position from the votes it is receiving from PrimaryCapsules layer. 选top 2 length的capsule分别输入decoder中，得到两张digit images. Other Datase CIFAR10 (3 channel) test_acc: 89.4%. drawback 由于CIFAR10 image的背景变化太大，很难model合理大小的网络，导致性能较差。 smallNORB test_acc: 97.3%. SVHN test_acc: 95.7%. Discussion and previous work Capsule将pixel intensities转化为vectors of instantiation parameters of recognized fragments, 然后对fragments作transformation matrices, 进而预测 the instantiation parameters of larger fragments. CNN不足之处 Convolutional nets have difficulty in generalizing to novel viewpoints. CNN具有translation (平移)不变性，但对于affine transformation (平移、旋转、缩放和斜切)不具有。因此，replicating feature detectors on a grid that grows exponentially with the number of dimensions, 或者指数级增加标注训练集。 Capsule Capsule作了一个非常强的假设：对于image中的each location, a capsule表示至多一个entity的instance. Capsule的神经元活动，会随着viewpoint (视角)的变化而变化(同变性equivariance)，而不是消除viewpoint带来的影响(如STN网络)。能够同时处理不同object(或object part)上的不同affine transformation. Routing iteration times]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Architecture</category>
        <category>Capsules</category>
      </categories>
      <tags>
        <tag>Classification</tag>
        <tag>Cognitive Neuroscience</tag>
        <tag>CapsulesNet</tag>
        <tag>Image Reconstruction</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Capsule Extension]]></title>
    <url>%2F2017%2F12%2F18%2FCapsule%20Extension%2F</url>
    <content type="text"><![CDATA[CapsulesNet 的解析及整理 大脑皮层微柱 功能柱 人的大脑皮层厚度为3～4mm，包含28x10^9神经元和相同数量胶质细胞。纵向垂直于皮层表面的细胞组织成微柱，穿越II-VI层，形成一个个结构功能单元。 灵长类的微柱包含80～100个神经元，形成30～50mm直径，深度2～4mm的柱状结构。 在视觉区的纹状皮层，每个柱内神经元数目大约200～250个。十个左右微柱组成一个功能柱。因此，一个功能柱直径大约300～600mm。不同物种的脑容量相差很大(10^3)，但功能柱大小接近。 芒卡斯尔认为功能柱是大脑皮层的基本的结构和功能单元，他有时也把它称为一个微型组件(module). Max Pooling Max pooling丢弃固定大小感知域中(nm-1)/nm的信息。随着层级的增加不断进行max pooling操作，相当于逐步增加感知区域，丢弃的范围逐步增大，丢弃的信息也不断增多，最终只有image整体中各object特征被激活，而位置信息(位置信息针对一些任务并不需要，如分类)被丢弃。 CNN局限性 Capsule Networks Are Shaking up AI — Here’s How to Use ThemCapsule Networks Explained 不具有平移同变性CNN具有translation invariant (平移不变性), 无论如何平移图像中的obj，都能检测到。不变性是通过Pooling下采样实现。但CNN不具有translation equivariance (平移同变性), 无法检测到obj平移的距离方向等变化，即。 由于CNN无法识别各sub-obj之间的相对位置关系，以致下图都被识别为Face。 CNNs work by accumulating sets of features at each layer. It starts of by finding edges, then shapes, then actual objects. However, the spatial relationship information of all these features is lost. 导致下图均被识别为peroson. CNN is also easily confused when viewing an image in a different orientation. 下图被识别为coal black color. CNN与CapsuleNet在识别上图人脸的区别 CNN CapsuleNet 易受白盒对抗攻击需要大量数据进行泛化In order for the ConvNets to be translation invariant, it has to learn different filters for each different viewpoints, and in doing so it requires a lot of data. CNN无法很好地表示人类视觉系统CNN利用filter从low level visual data提取high level information. 而对于人类系统而言，当触发视觉刺激时，大脑的内建机制会将low level visual data route到大脑某些部分。 此外，人类视觉系统会对obj建立coordinate frames，并选择一个参考点，旋转obj. ConvNet与CapsuleNet区别 CapsuleNet (mimics the human vision system) strives for translation equivariance instead of translation invariance, allowing it to generalize to a greater degree from different view points with less training data. Inverse Graphics 图像渲染过程To go from a mesh object onto pixels on a screen, it takes the pose of the whole object, and multiplies it by a transformation matrix. This outputs the pose of the object’s part in a lower dimension (2D). )## 图像逆过程lower dimension –&gt; whole object 权重矩阵因此，可用权重矩阵表示两者之间的关系。这些权重是viewpoint invariant. Meaning that however much the pose of the part has changed we can get back the pose of the whole using the same matrix of weights. 利用reconstruction得到该权重矩阵 矢量神经元 知乎: 如何看待Hinton的论文《Dynamic Routing Between Capsules》 Hinton对CNN的思考 生物神经系统的思考 解剖学上并未发现神经系统存在反向传播及求导的结构。 神经系统具有分层结构，但层数不多。生物系统传导在ms量级，GPU在us量级，同步出现问题。 大脑皮层存在微柱结构(Cortical minicolumn)，其内部含有上百个神经元，并存在内部分层结构，比NN的一层结构更为复杂。 认知神经科学的思考人会不自觉根据物体形状建立坐标框架(coordinate frame), 并通过对坐标框架旋转。 坐标框架的不同会影响人的认知。 坐标框架参与到物体识别过程中，识别过程手空间概念的支配。 CNN不存在坐标框架。Hinton提出猜想：物体与观察者之间的关系（如物体姿态），应该由一整套激活的神经元表示，而不是由单个神经元，或者一组粗编码（coarse-coded，一层中并没有经过精细地组织）的神经元表示。这样才能有效表达坐标框架的先验知识。 CNN的局限性 不变性物体不随变化而变化。如空间不变性。 同变性用变化矩阵进行转换后，物体表示依旧不变。是对物体内容的变换。 CNN对旋转没有不变性。可采用数据增强方式达到旋转不变性。 CNN的不变性通过Pooling实现。 平移和旋转不变性舍弃了坐标框架。 虽然CNN准确率高，但是最终目标应该是对内容的良好表示，从而达到理解内容。 Hinton提出的Capsule Capsule需具备的性质 一层中具有复杂的内部结构。 能表达坐标框架 实现同变性 Capsule神经元Capsule用一组神经元代表一个实体，仅且代表一个实体。 模长代表某个实体（物体或其一部分）出现的概率。 方向/位置代表实体的一般姿态(generalized pose)，报货位置、方向、尺寸、速度、颜色等。 视角变换矩阵CapsuleNet用视角变换矩阵处理场景中两物体间的关联，不改变它们的相对关系。 两种同变性 位置编码 (place-coded)视觉中的内容位置发生较大变化，由不同Capsule表示其内容。 速率编码 (rate-coded)视觉中的内容位置发生较小变化，由相同capsule表示其内容，但是内容有所改变。 高层的capsule有更广的域(domain)，所以低层的place-coded信息到高层会变成rate-coded.]]></content>
      <categories>
        <category>Knowledge Note</category>
      </categories>
      <tags>
        <tag>Capsule</tag>
        <tag>Cerebral Cortex</tag>
        <tag>Cognitive Neuroscience</tag>
        <tag>CNN Limitation</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(CVPR 2017) Image-to-image translation with conditional adversarial networks]]></title>
    <url>%2F2017%2F11%2F03%2FImage-to-Image%20Translation%20with%20Conditional%20Adversarial%20Networks%2F</url>
    <content type="text"><![CDATA[Isola P, Zhu J Y, Zhou T, et al. Image-to-image translation with conditional adversarial networks[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2017: 1125-1134. 本篇论文提出conditionnal GAN (supervised)结构 学习input image到output image之间的映射。 学习特定的loss function用于训练映射，即单独使用L1 loss (或L2 loss)会产生blur现象，而再此基础上进一步使用adversarial loss能够学习到适合特定数据集的loss function, 从而sharpen生成的图像 (判别器D能够判断blurry image为fake)。因此，针对不同任务 (Figure 1)，该方法具有通用性。 Contribution 针对不同任务，cGAN具有通用性。 Achieve good result, 并分析cGAN结构中的一些重要部分 Relative Work Loss类型 Structured loss 每个像素点独立考虑. per-pixel classification loss, regression. Unstructured loss penalize the joint configuration of the output, 如conditional random fields. cGAN的unstructured loss是学习到的。 cGAN 前人也apply GANs in conditional seting, 但是针对特定应用的，而本论文的cGAN提出的是通用框架。 本论文的cGAN使用到了U-Net和PatchGAN. Objective GAN z-&gt;G-&gt;y y-&gt;D-&gt;true or fake cGAN (Figure 2) {x, z}-&gt;G-&gt;y {x, y}-&gt;D-&gt;true or fake 使用L2会产生更严重的blur. 最终的目标函数 没有噪声z的网络会产生一个特定的输出，无法match any distribution, 因此cGAN加入噪声z，但在本篇论文实验中发现，G能够学习到如何ignore 噪声，从而在模型的test阶段也使用dropout产生noise. Network architectures Skip connection of G 在auto-encoder结构中，input的所有信息会在所有layers传输。为了避免这种方法，在AE的基础上添加skip connection, 即U-Net (Figure 3). Markovian D (PatchGAN) L1 loss和L2 loss能够capture low frequencies, 因此需要约束D能够capture high frequency structure，即PatchGAN (N X N patches). D effectively models the image as a Markov random field. Optimization and inference 在test阶段，使用dropout, BN使用 the statistics of the test batch, rather than aggregated statistics of the training batch. instance normalization在图像生成任务上很有效。(batch size为1，使用the statistics of the test batch) Experiments L1产生blur. cGAN sharp imaged，但是存在artifacts. (Table 1) cGAN优于GAN，加上L1 loss后，cGAN也相对较优。 Colorfulness当不确定edge的位置时，L1会产生blur和averrage ( L1 will be minimized by choosing the median of of the conditional probability density function over possible colors.) 从而导致narrower distribution than the ground truth (Figure 7). Analysis of the G (Figure5) Analysis of the D (Figure 6, Table 2) Pixel GAN output 1x1 of D. Image GAN output 256x256(full image size) D. Patch GAN output 70x70(在本实验中) D. Fully-convolutional translation. Patch GAN由于不包含FC层，D和G都可应用与任何大小的图片。(Figure 8)中的G在train阶段使用256x256图片，在test阶段使用512x512图片。 Perceptual validation Semantic segmentationGAN一般用于图像生成，本论文尝试将cGAN用于做segmentation任务，但最终效果并不好。(Figure 10, Table 5)从实验结果可以看出， reconstruction losses like L1 are mostly sufficient. Semantic labels↔photoCityscapes dataset Architectural labels→photoCMP Facades dataset Map↔aerial photoGoogle Maps BW→color photos Edges→photonary Sketch→photo Day→night Failure case]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Image Processing</category>
      </categories>
      <tags>
        <tag>Image Generation</tag>
        <tag>GAN</tag>
        <tag>Image Enhancement</tag>
        <tag>Conditional GAN</tag>
        <tag>Map to Aerial</tag>
        <tag>Reality Enhancement</tag>
        <tag>Style Transfer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(NIPS 2017) Unsupervised Image-to-Image Translation Networks]]></title>
    <url>%2F2017%2F11%2F03%2FUnsupervised%20Image-to-Image%20Translation%20Networks%2F</url>
    <content type="text"><![CDATA[Liu M Y, Breuel T, Kautz J. Unsupervised image-to-image translation networks[C]//Advances in Neural Information Processing Systems. 2017: 700-708. 根据couple theory，基于joint分布可以很容易得到marginal分布，但利用marginal分布得出joint分布十分困难。 因此，本篇论文提出UNIT (UNsupervised Image-to-image Translation) 框架 (Figure 1)，利用shared-latent space假设协助从两个domain的marginal分布 ($p_{X1}(x1)$, $p_{X2}(x2)$)中学习它们之间的joint分布。 UNIT结构涉及到Coupled GAN、VAE、Cycle consistent、weight-sharing constraint. 此外，UNIT结构也能够应用到domain adaption任务上。 Assumptions Unsupervised Marginal $p_{X1}(x1), p_{X2}(x2)$ Supervised Joint $p_{X1,X2}(x1, x2)$ 假设存在shared-latent space，space中的shared latent code z能够同时用于恢复两个domain的图片。 确保满足$F_{1-&gt;2}=G2(E1(x1))$和$F_{2-&gt;1}=G1(E2(x2))$的一个必要条件是cycle consistent，即shared-latent space假设暗含了cycle consistent假设。 在shared-latent space的基础上，进一步假设shared intermediate representation $h$. $G1=G_{L1}*G_{L2}$ $G2=G_{L2}*G_{H}$ $G_{H}: z-&gt;h$, high level $G_{L}: h-&gt;x$, low level z可看作是场景的high-level representation (car in front, trees in back). h可看作是z的特定实现 (car occpy the following pixels). $G_{H,L}$可看作是actual image formation (tree lush green in sunny domain, dark green in rainy domain). Framework 6 subnetworks (Table 1) VAE Reparameterization Weight-sharing 基于shared-latent space假设，enforce weight-sharing约束到两个VAE上：last few layers of E, first few layers of G. 但weight-sharing约束并不能确保两个domain中对应的图片能得到相同的latent code. 因为对于unsupervised方式而言，两个domian中不存在对应的pair能够训练网络输出同样的latent code. 然而，能够通过对抗训练将两个domain中的pair映射到同样的latent code上。 Stream Translation stream $X1-&gt;z1-&gt;X2$ $X2-&gt;z2-&gt;X1$ Reconstruction stream $X1-&gt;z1-&gt;X1$ $X2-&gt;z2-&gt;X2$ GAN 只将对抗训练应用到translation stream上。 Cycle-consistent Cycle-reconstruction stream $X1-&gt;z1-&gt;X2’-&gt;z2-&gt;X1’$ $X2-&gt;z2-&gt;X1’-&gt;z1-&gt;X2’$ To further regularize the ill-posed unsupervised image-to-image translation problem. Loss 交替update 更新D1, D2 (adversarial loss) 更新G1, G2, E1, E2 (VAE loss + CC loss) VAE loss 用Laplacian分布model pG1,pG2. 最小化Negative log-likelihood等价于最小化image和reconstructed image绝对距离。 GAN loss CC loss Experiments Shallow D导致较差性能 (Figure 2b) Weight-sharing层数影响小 (Figure 2c) Negative log likelihood权重越大，acc越高 (Figure 2c) Ablation study (Figure 2d) Remove weight-sharing约束和reconstruction stream，结构变成CycleGAN. Remove cycle-consistent约束. Street image (Figure 3)Sunny to rainy, data to night, summery to snowy. Synthetic to real (Figure 3)Synthetic images SYNTHIA dataset.Real images Cityscape dataset. Dog breed conversion (Figure 4)ImageNet, 利用模板匹配算法extract head regions. Cat species conversion (Figure 5)ImageNet. Face attribute (Figure 6)CelebA dataset. With an attribute constituted the 1st domain, while those without the attribute constituted the 2nd domain. Domain Adaption 用一个domian中的labeled数据训练分类器，并用将该分类器应用到另一个domain数据集上，该数据的labeled在训练中没有使用到。 利用UNIT进行多任务学习: Translate between source domain and target domian. 利用source domain的D提取source domain数据的特征。 共享D1, D2 high-level层的权重。 最小化D1和D2 highest layer提取feature之间的L1 loss. 实验发现spatial context information useful (RGB+normalized xy coordinates). Network Architecture]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Image Processing</category>
      </categories>
      <tags>
        <tag>Face</tag>
        <tag>Image Generation</tag>
        <tag>GAN</tag>
        <tag>Image Enhancement</tag>
        <tag>Map to Aerial</tag>
        <tag>Reality Enhancement</tag>
        <tag>Style Transfer</tag>
        <tag>UNIT</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(ICCV 2017) Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks]]></title>
    <url>%2F2017%2F11%2F03%2FUnpaired%20Image-to-Image%20Translation%20using%20Cycle-Consistent%20Adversarial%20Networks%2F</url>
    <content type="text"><![CDATA[Zhu J Y, Park T, Isola P, et al. Unpaired image-to-image translation using cycle-consistent adversarial networks[C]//Proceedings of the IEEE international conference on computer vision. 2017: 2223-2232. 论文提出Cycle GAN结构，基于unpaired data (Figure 2)，学习domain X到domain Y的映射关系 (Figure 1)。 Cycle GAN能够应用到不同任务上：style transfer, object transfiguration, attribute transfer, and photo enhancement等。 Cycle GAN包含adversarial loss, cycle consistency loss. 对于某些特定任务，Cycle GAN额外包含一个identity loss. 论文假设两个不同domain之间存在underlying relationships，Cycle GAN (Figure 3)从一个image collections X中学习到一些特征，并将这些特征转换到另一个image collections Y上。 仅使用adversarial loss，无法保证生成的图像是有意义的，例如，G可能生成rubbish fool D。此外，生成图像不一定是desired。另一方面，标准的GAN过程可能会导致mode collapse问题：所有输入图像都会被映射到同一个输出图像。因此，模型引入了cycle consistent loss. 对于painting-&gt;photo的任务，为了保留输入- painting的颜色 (Figure 9)，模型引入了identiy loss. Circle GAN可看作是两个auto-encoder：GF和FG。 Related work Image-to-Image Translation (CGAN) I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Generative adversarial nets. In NIPS, 2014 P. Isola, J.-Y. Zhu, T. Zhou, and A. A. Efros. Image-to-image translation with conditional adversarial networks. arXiv preprint arXiv:1611.07004, 2016. Unpaired Image-to-Image Translation (VAE+GAN) M.Y. Liu, T. Breuel, and J. Kautz. Unsupervised image-to-image translation networks. arXiv preprint arXiv:1703.00848, 2017. Neural Style Transfer学习两张特定图片之间的映射，Cycle GAN学习的是两个domain之间的映射。 Loss Adsersarial Loss Loss Identity Loss Implementation D使用70*70 PatchGAN：更少参数，能判别任意大小图像。 将adversarial loss从negative log likelihood改为least square loss. History Buffer of generated images. batch size 1 of scratch. Experiments CoGANM.-. Liu and O. Tuzel. Coupled generative adversarial networks. In NIPS, pages 469–477, 2016. Pixel loss + GANSimGAN (self-regularision loss). Feature loss + GANSimGAN (vgg16 feature loss, instead of RGB loss). BiGANV. Dumoulin, I. Belghazi, B. Poole, A. Lamb, M. Arjovsky, O. Mastropietro, and A. Courville. Adversarially learned inference. arXiv preprint arXiv 2016. Pix2pixCGAN. AMT FCN score (Cycle GAN用的是unsupervised, pix2pix用的是supervised) Pixel classification Ablation Dataset Labels-photo: Cityscapes dataset (Figure 5) Map-aerial photo: Google Maps (Figure 6) Labels-photo: CMP Facade database (Figure 8) Edges-shoes: UT Zappos50K dataset (Figure 8) Style transfer: Flickr, WikiArt (Figure 10) Object transfiguration&amp;season transfer: ImageNet, Flickr (Figure 13) Photo generation from paintings: Monet’s painting, Flickr (Figure 12) Photo enhancement:Flickr (Figure 14) Comparison with Gatys (Figure 15, 16) Failure cases (Figure 17)]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Image Processing</category>
      </categories>
      <tags>
        <tag>Image Generation</tag>
        <tag>GAN</tag>
        <tag>Image Enhancement</tag>
        <tag>Map to Aerial</tag>
        <tag>Style Transfer</tag>
        <tag>CycleGAN</tag>
        <tag>Label to Photo</tag>
        <tag>Edge to Photo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(2013) Auto-Encoding Variational Bayes]]></title>
    <url>%2F2017%2F11%2F03%2FAuto-Encoding%20Variational%20Bayes%2F</url>
    <content type="text"><![CDATA[Kingma D P, Welling M. Auto-encoding variational bayes[J]. arXiv preprint arXiv:1312.6114, 2013. (Figure 1) 由于通过(可观察的)变量X推断(不可观察) latent variables比较困难 (即后验概率分布p(z|x))，论文提出VAE (变分自编码器，variational auto-encoder)结构以及AEVB (自编码变分贝叶斯)算法，即通过SGVB (随机梯度变分贝叶斯，Stochastic Gradient Variational Bayes)估计使得构造的q(z|x)分布近似难以计算的p(z|x)分布。 Contribution 通过reparameterization方法确保梯度能够回传。 通过下界估计(变分贝叶斯推导得出)近似后验概率分布。 知识点延伸 相对熵D(P||Q)=交叉熵H(P, Q) - 熵H(Q). 熵：真实分布P的平均编码长度(大于等于0)。 交叉熵：非真实分布Q的平均编码长度(大于等于[熵])。 相对熵：[交叉熵]与[熵]的差，即多出的编码bit数(大于等于0)。 D(Q||P)优化结果：P(绿色)尽可能包含Q. D(P||Q)优化结果：P(绿色)尽可能包含Q. 变分法：推导得出泛函存在极值的必要条件：欧拉-拉格朗日方程。 平均场定理：利用概率模型Q(x1, x2, …, xn)=Q(x1)Q(x2)…Q(xn)近似所要求的概率模型P(x1, x2, …, xn)=P(x1)P(x2|x1)P(x3|x2, x1)…P(xn|xn-1, …, x1). 变分贝叶斯：结合平均场定理和变分法，求出近似P分布的Q分布。通过最小化目标函数KL(Q||P)，推导出最大化下界估计。利用平均场定理进一步推出需要满足 即 涉及到计算期望: 链接 VAE 链接 SGVB 基于变分贝叶斯推导得出的下界估计，利用平均场定理推出最终需要满足的条件(涉及到期望计算)，但分析期望相关的解仍然存在困难。 因此，论文通过reparameterization方法 (而非平均场定理) 简化下界估计，公式右边两项分别通过decoder和encoder计算得出。 同时，reparameterization能够使得下界估计可导 (即梯度能够回传)。 The Problem Scenario Intractability. 边缘似然p(x)=∫p(z)p(x|z)dz难以计算，导致真实后验概率分布p(z|x)=p(x|z)p(z)/p(x)难以计算(无法使用EM算法)，以及mean-field VB难以计算。另一方面，p(x|z)可通过神经网络非线性解决。 Minibatch训练时，使用Monte Carlo EM非常慢。 论文旨在解决以下3点困难: 近似参数θ. 近似posterior p(z|x). 近似marginal p(x). 即引入encoder q(z|x)近似p(z|x). encoder (构造概率分布φ): q(z|x). decoder (真实概率分布θ): p(x|z). 论文提出一种方法联合训练encoder参数φ和decoder参数θ. The Variational Bound 根据VB可得到公式 (1) L即为变分下界，可写成公式 (2,3) (公式3) 右边两项分别为q分布与真实分布p的差异、根据z重构x的误差。 SGVB estimator和AEVB algorithm 假设一个近似后验概率分布q(z|x)，当不假定条件x时，同样能应用于q(z)。 使用包含 (auxiliary) noise variable ε的可导变换g表示random variable z (公式4) 因此可以构造关于包含z~q(z|x)的函数f的Monte Carlo期望估计 (公式5)： (公式2) 可改写为 (公式6) 考虑到分布近似误差和重构误差，(公式2) 也可改写为 (公式7) 考虑到minibatches训练，可进一步改写为 (公式8) The Reparameterization Trick 假设z服从高斯分布 可写成合理形式 即 Example 假设z的先验分布服从标准正态分布 Experiments 增加latent variable维数不会导致过拟合。 y: loss, x: iteration. Solution of -D(q||p), Gaussian Case MLP’s as probabilistic encoders and decoders MLP (Multi-layer Perceptron)：神经网络。 encoder使用MLP with Gaussian output. decoder使用MLPs with Gaussian or Bernoulli outputs, depending on the type of data. Bernoulli Gaussian]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Architecture</category>
        <category>Network</category>
      </categories>
      <tags>
        <tag>VAE</tag>
        <tag>Network</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(AAAI 2017) Deep Spatio-Temporal Residual Networks for Citywide Crowd Flows Prediction]]></title>
    <url>%2F2017%2F10%2F05%2FDeep%20Spatio-Temporal%20Residual%20Networks%20for%20Citywide%20Crowd%20Flows%20Prediction%2F</url>
    <content type="text"><![CDATA[Zhang J, Zheng Y, Qi D. Deep spatio-temporal residual networks for citywide crowd flows prediction[C]//Thirty-First AAAI Conference on Artificial Intelligence. 2017. 对于公共安全和交通管理而言，预测人群流动具有重要意义，但它受到事件、天气和区域间等复杂因素的影响，因此该篇论文提出一个基于深度学习的方法 (ST-ResNet, Figure 3)综合预测一个城市中各区域人群的流入 (inflow)与流出 (outflow) (Figure 1). ST-ResNet基于时间和空间数据 (spatio-temporal data)，对temprtaol closeness, period和trend 3阶段交通拥挤程度进行建模，并结合external factors对进行预测。论文基于两个数据集Beijing和New York City (NYC)进行实验，并与6个baseline进行对比。 Mobile phone signal of pedestrians. GPS trajectories of vehicles. Complex Factors Spatial dependencies. Region的inflow受到附近regions的outflow影响，甚至受更远的regions影响。同时region的inflow也会影响它自身的outflow. Temporal dependencies. Region的flows也受到近期时间段的影响。例如，8点的交通阻塞可能会影响到9点的交通情况；每个工作日的高峰期相似等。 External influence. 天气条件、事件等。 Contribution 使用Conv来model任意两region之间的spatial dependencies. 使用3个ResNet来model temporal closeness, period和trend的temporal dependencies. Assigning different weights to aggregate 3 outputs of ResNet, and then aggregate external factors. Outperform 6 baselines on Beijing and NYC dataset. Preliminaries Region. 将城市划分为grid map (Figure 2). Flow (2 channels). 归一化到[-1,1]. ST-ResNet (Figure 3) ST-ResNet Include temporal closeness (recent), period (near), trend (distant) and external 4个主要部分。 Temporal closeness (recent), period (near), trend (distant)的ResNet结构相同，其中包含2个Conv和L个Residual (Figure 4). 由于地铁或高速公路会导致两个很远的region具有很强的关联性，因此使用具有层级性的stack Conv来实现，并利用residual的形式来提高stack Conv的收敛性。Relu之前增加了ReLU层。 External通过两层FC. Regard as embedding layer + mapping layer (same shape as Xt). Fusion包含3个可学习的权重参数，乘以对应的3个ResNet输出后相加，其结果再与FC输出相加，经过tanh激活函数。 MSE loss, Adam. Algorithm p: one day. q: one-week. DataSet (Table 1) TaxiBJ. Taxicab GPS and moteorology data. 选择最近4周作为testing data. BikeNYC. NYC Bike system. 选择最近10天作为testing data. (Figure 5) 假期和天气会影响北京办公区的流量。 (Figure 6) recent时间段的相关性更大，办公区周末的流量较低，办公区流量呈现下降趋势，居住区呈现上升趋势。 Baseline HA. 历史inflow,outflow均值。 ARIMA. Auto-Regressive Integrated Moving Average. 用于预测时间序列的值。 SARIMA. Seasonal ARIMA. VAR. Vector Auto-Regressive(VAR), spatio-temporal model. ST-ANN. extracts spatial (nearby 8 regions’ values) and temporal (8 previous time intervals) features, then fed into an artificial neural network. DeepST. State-of-the-art. DNN for spatio-temporal data. 4 variants: DeepST-C, DeepST-CP, DeepST-CPT, and DeepST-CPTM. Focus on temporal dependencies and external factors. Experiments Theano&amp;Keras. Conv2: 2 filters. Evaluation Metric: Root MSE. Related Work Conv: capture spatial dependencies. RNN: capture temporal denpendencies. ConvLSTM: capture spatial and temporal denpendencies. But can not model very long-range temporal denpendencies(period and trend). Future Consider other types of flows: taxi, truck, bus, phone signal, metro card swiping.]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Prediction</category>
      </categories>
      <tags>
        <tag>Prediction</tag>
        <tag>Citywide Crowd Flows</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(2016) Automated Inference on Criminality using Face Images]]></title>
    <url>%2F2017%2F10%2F03%2FAutomated%20Inference%20on%20Criminality%20using%20Face%20Images%2F</url>
    <content type="text"><![CDATA[Wu X, Zhang X. Automated inference on criminality using face images[J]. arXiv preprint arXiv:1611.04135, 2016: 4038-4052. 该篇论文利用supervised machine learning(logistic regression, KNN, SVM, CNN) 对criminal (C) 和non-criminal (N) 面部图像进行分类(准确度最高达到89.51%)，并进行一些实验分析C与N群体之间的区别： N群体内部的面部相似度更大，C群体内部的面部差异更大。 C和N是两个concentric(同心), distinctive的manifold(流形). The variation of C greater than N. 基于面部特征的人为判断会带有偏见、先决条件等，而CV算法并不会存在这些问题。 Data Preparation Dataset包含1856张照片 (1126N+730C, Figure 1). 照片标准: Chinese, male, between ages of 18 and 55, no facial hair, no facial scars, or other markings. N including waiters, construction workers, taxi and truck drivers, real estate agents, doctors, lawyers and professors; half have university degrees. C including the ministry of public security of China, the departments of public security for the provinces of Guangdong, Jiangsu, Liaoning, etc. And the City police department in China. C中 235人是violent crimes (murder, rape, assault, kidnap and robbery), 剩余536人是non-violent crimes (murder, rape, assault, kidnap and robbery). Only the region of the face and upper neck is extracted. 80 × 80 images. 将每张图像的直方图与整个数据集的平均直方图相匹配，从而使得灰度图归一化到同样的强度分布。 Methods 面部关键点特征能够避免signal level和variant of source cameras的影响。论文使用以下四种关键点: Facial landmark point. Facial feature vector, generated by modular PCA. Facial feature vector based on Local Binary Pattern (LBP) histograms. The concatenation of above three feature vectors. (Feature-driven classifiers (LR, SVM, KNN) 3 + Data-driven classifiers (CNN)) 10-fold cross validation = 130 cases Results 不同的source camera拍摄的照片可能会带有不同camera的signatures, 虽然已通过上述的landmark point解决，但在此进一步引入高斯噪声 (mean=0) 来overpower camera signatures. 实验结果与期望的一致: 性能不会出现很大的变化 (Figure 6,7;Table 2, 3). Discriminating Feature 使用Feature Generating Machine (FGM)进行分析与犯罪最相关的面部部位，得出这些特征位于眼角、嘴唇和额头部位 (Figure 8). ρ: 上嘴唇的曲度 d: 内眼角之间的距离 θ: 鼻尖到嘴唇两角的角度 使用Hellinger距离分别计算C和N两者之间的上述3个部位的距离，分别为0.3208, 0.2971, 0.3855. 因此，C和N是存在一定差异的. 按照论文分析结果 (Figure 8, Table 4)脑补了一个极端的罪犯例子 三个特征的直方图 Face Clustering on Manifold 通过平均脸并不能很好地得出C和N群体的区别 (Figure 10)，因此需要在更高维度 (manifold流形和聚类)上进行分析。 公式2分别为cross-class average manifold和in-class average manifold. 计算得到manifold后，使用Isomap进行降维可视化。]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Face</category>
      </categories>
      <tags>
        <tag>Classification</tag>
        <tag>Face</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(NIPS 2012) Imagenet classification with deep convolutional neural networks]]></title>
    <url>%2F2017%2F09%2F29%2FImageNet%20classification%20with%20deep%20convolutional%20neural%20networks%2F</url>
    <content type="text"><![CDATA[Krizhevsky A, Sutskever I, Hinton G E. Imagenet classification with deep convolutional neural networks[C]//Advances in neural information processing systems. 2012: 1097-1105. 该篇论文提出一种CNN结构 (AlexNet)，在ImageNet数据集ILSVRC-2010 (Table 1) 和ILSVRC-2012上进行实验验证其性能：top-1 and top-5 error (Figure 4, Left). 论文中AlexNet在双GPU上进行训练，使用 ReLU替代tanh activation function，提高收敛速度。 Local Response Normalization和Overlapping Pooling提高性能。 以及通过 Data Augmentation Dropout Layer 来Reduce overfitting. Dataset ImageNet数据集包含超过15 million labeled 高分辨率图像，图像可分为22,000 categories. Starting in 2010, as part of the Pascal Visual Object Challenge, an annual competition called the ImageNet Large-Scale Visual Recognition Challenge (ILSVRC) has been held. ILSVRC比赛使用ImageNet数据集中的一部分，大约包含1000个图像类别，每个类别大约1000张图片。数据集分为training、validation、testing. 评测指标分为top-1和top-5 error。之所以使用top-5 error指标是因为，有些图像可以同时分为好几个类别 (数据集的labele可能存在一定误差)。Top-5表示只要预测的类别在该图像前5个类别labele中，就算预测正确。 对数据集图像进行256$\times$256尺寸的下采样处理。由于图像尺寸大小不一，论文首先按照比例rescale图像，将图像shorter sied rescale到256大小，central crop 256$\times$256 patch. 此外，还对图像像素值进行去中心化操作(减均值) ReLU 用ReLU (non-saturating nonlinearities) 替代tanh(saturating nonlinearities) 能提高收敛速度(Figure 1) Local Response Normalization 对各空间点的卷积值在channel维度上作归一化(现在一般使用GoogleNet Version 2提出的Batch Normalization). Response normalization reduces our top-1 and top-5 error rates by 1.4% and 1.2%, respectively. Overlapping Pooling 即stride小于kernel size. This scheme(stride=2, kernel size=3 vs stride=2, kernel size=2) reduces the top-1 and top-5 error rates by 0.4% and 0.3%. 多GPU训练 GTX 580 3GB memory. 由于单块GPU显存太小，因此使用两块GPU训练。如今GTX Titan已到12G. AlexNet 8 layers=5 Conv+3 FC+Softmax (Figure 2). has 60 million parameters and 650,000 neurons. Data Augmentation (方法1) 先对256$\times$256图像进行horizontal reflection，再random crop 224$\times$244 patches，论文将数据集扩大2048倍。 (方法2) Altering the intensities of the RGB channels. 对RGB矩阵进行PCA得到特征值和特征向量(图1)，对特征值乘以一个系数α，α服从mean=0, std=0.1高斯分布。 Dropout Layer Combining the predictions of many different models is a very successful way to reduce test errors, but it appears to be too expensive for big neural networks that already take several days to train. 因此，利用dropout的随机性来改变模型training阶段的内部结构。 论文使用drop rate 0.5, 即50%几率将training阶段的输出设置为0，不参与本次forward和backward过程。 Dropout roughly doubles the number of iterations required to converge. 在testing阶段，将输出乘0.5 (如今的dropout方法，大多数不乘drop rate). 基于vector进行相似性检索 基于倒数第二层FC输出的4096维Vector，计算欧氏距离进行相似图片检索 (Figure 4, Right). 在pixel leve上基于L2进行图片相似检索显然不可行，但是在high-level可用L2进行检索。 此外，4096维vector的欧氏距离计算量太大，使用auto-encoder对vector进一步压缩后再进行相似度检索，能得到更好的效果。]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Architecture</category>
        <category>Network</category>
      </categories>
      <tags>
        <tag>Network</tag>
        <tag>Classification</tag>
        <tag>AlexNet</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(CVPR 2017 Best Paper) Learning from simulated and unsupervised images through adversarial training]]></title>
    <url>%2F2017%2F09%2F19%2FLearning%20from%20simulated%20and%20unsupervised%20images%20through%20adversarial%20training%2F</url>
    <content type="text"><![CDATA[Shrivastava A, Pfister T, Tuzel O, et al. Learning from simulated and unsupervised images through adversarial training[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2017: 2107-2116. 该篇论文提出一种Unsupervised方法学习SimGAN model。该model能够使用大量unlabeled real images来增强labeled synthetic images真实性 (Figure 1). 显而易见，在real images和synthetic images之间存在一个gap，而实际上，Deep Learning做的事情就是学习两者之间的映射关系。 贡献点 使用unlabeled real data来refine synthetic images. 结合adversarial loss和self-regularization loss训练Refiner Network (R). 使用一些key modification来稳定train以及防止R产生artifacts. 分别使用synthetic和refined images训练CNN进行gaze estimation任务，并在MPIIGaze Dataset上测试，进行比较。 训练过程 SimGAN模型包含Refiner Network(R)和Discriminator Network(D) (Figure 2). (Algorithm 1) 对于每个training step，首先训练R $K_g$次，接着训练D $K_d$次。 训练网络的过程: Forward Input$\to$Calc Loss$\to$Backward Loss$\to$Optimize Parameters. Loss Function D包含两部分loss (Formula 2)： Refined images输入D判别为False的loss（输入与Ground-truth的cross entropy loss）。 Real images输入D判别为True的loss. R包含两部分loss (Formula 1,4)： Refined images输入D判断为True的loss（与D中判别其为False形成对抗Adversarial） Synthetic images与refined images之间的L1 loss，乘以权重系数λ(hyper-parameter). Self-regularazition loss Preserve the annoation information of the synthetic images. Local Adversarial Loss Output a probability map (Figure 3) instead of a vertor. Prevent R from over-emphasizing certain image features to fool the current discriminator network, leading to drifting and producing artifacts. History Buffer of Refined Images (Figure 4) Prevent D from only focusing on the latest refined images. 将refined images输入D之前，首先随机选择一半refined images放入Buffer中，再从Buffer中随机选择同样数目的refined images放回。 使用History Buffer的实验结果优于不使用 (Figure 10) Gaze Estimation Task 分别使用synthetic images和refined images训练简单的CNN(输出vector(x,y,z))，并在MPIIGaze Dataset上进行测试。 从实验结果 (Table 2) 能够看出，经过R增强的refined images的distribution更接近real images的distribution. Details R is a ResNet. λ=0.001, kd=1, kg=50. 1.2M synthetic images, 214K real images. More in here Github Tensorflow&amp;Keras：wayaai/SimGAN Pytorch：AlexHex7/SimGAN_pytorch]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Image Processing</category>
      </categories>
      <tags>
        <tag>Image Generation</tag>
        <tag>GAN</tag>
        <tag>Reality Enhancement</tag>
        <tag>SimGAN</tag>
        <tag>Eye</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(CVPR 2017 Best Paper) Densely connected convolutional networks]]></title>
    <url>%2F2017%2F09%2F19%2FDensely%20connected%20convolutional%20networks%2F</url>
    <content type="text"><![CDATA[Huang G, Liu Z, Van Der Maaten L, et al. Densely connected convolutional networks[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2017: 4700-4708. 该篇论文提出一种网络结构DenseNet (Figure 2)，能够在提高性能的同时，大量降低模型的参数及占用内存 (Figure 3)。 DenseNet Block (Figure 1)结构中包含大量skip connection(shortcut)： 解决梯度消失问题 提高feature reusing和propagation. 降低Conv层的filter(kernel)数量，使网络变得更narrow.(由于存在shortcut，在通过网络的时候，不会出现信息丢失的情况。) DenseNet细节包含以下几个主要部分： Transition Layer Growth Rate Bottleneck Layer Compression Transition Layer (图1) 连接在两个Dense Block之间，包含：BN- (1x1) Conv- (2x2) Avg Pooling Growth Rate denoted by $k$. 即$H$层 (Figure 1)中 (3x3) Conv层的filter数量为$k$. Bottleneck Layer 为了降低特征图数量，在H层的头部加上Bottleneck Layer，包含：BN-ReLU-(1x1)Conv. 该Conv层的filter数量小于输入特征图数量，从而达到降低特征图数量的目的 Compression 为了进一步提高模型的紧凑性，可以在Transition Layer中降低特征图数量。即降低其中的 (1x1) Conv层filter数量。 假设输入特征图数量为$m$，存在一个hyper-parameter $θ$ (0&lt;$θ$&lt;1) 使得输出特征图数量降低至$θ_m$，则(1x1) Conv层有$θ_m$ filters.]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Architecture</category>
        <category>Network</category>
      </categories>
      <tags>
        <tag>Network</tag>
        <tag>Classification</tag>
        <tag>DenseNet</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(CVPR 2017) Deep feature flow for video recognition]]></title>
    <url>%2F2017%2F09%2F19%2FDeep%20feature%20flow%20for%20video%20recognition%2F</url>
    <content type="text"><![CDATA[Zhu X, Xiong Y, Dai J, et al. Deep feature flow for video recognition[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2017: 2349-2358. 该篇论文首先说明了Video Recognition的Problem： 直接将图像识别网络应用到视频frame上会产生非常大的计算量。例如，假设图像识别网络处理一张图片需要0.1s，而30 fps的视频而言，处理1s的视频内容需要3s，这显然是不具有可行性的。 因此，论文提出了一个用于Video Recognition的Framework Deep Feature Flow (DFF) (Figure 2)： 仅仅将稀疏key frame输入Feature Network. 其他frame的特征图通过key frame的特征图进行propagation得到。 比较Figure 1中current frame feature maps和propagated feature maps： 将current frame输入Feature Network得到的特征图 对key frame特征图进行propagation得到的特征图 可以看出两类特征图基本类似。此外，可以明显看出Conv层中的#183 filter activate on车， #289 filter activate on 行人和狗。 通过Detection task实验(图1)可看出，DFF相对于Per-frame Network： 提高10倍速度。 Accuracy仅仅从73.9%降低到69.5%. DFF能大幅度提升速度是因为：Flow estimation and feature propagation are much faster than the computationof convolutional features. DFF结构包含3部分网络： Feature Network Flow Network Task Network 以及Inference和Training两个阶段 Feature Network 采用ResNet-50和ResNet-101 pre-trained on ImageNet Classification. Task Network DeepLab for segmentation R-FCN for object detection Inference Scale Function Better approximate the features the spatial warping may be inaccurate due to errors in flow estimation, object occlusion. 基于optical flow对key frame feature map进行bilinear interpolation 得到propagated feature map of current frame Key frame schedule Training (图3) 选取labeled frame作为current frame（一些Dataset只有少数帧有ground-truth），随机选取附近的帧作为key frame： Current frame == key frame. 计算current frame loss(右半部分). Current frame != key frame. 计算key frame loss（左半部分）. Inference阶段固定key frame， 顺序选取current frame. Dataset ImageNet VID for object detection Cityscapes for semantic segmentation Task Network与Feature Network分割 论文做了一个实验，验证将Feature Network中最后多少层放入Task Network中性能最好。从Table 5中可以看出在两个网络完全分离的情况下，性能最好。 DFF Framework的通用性，论文默认采用预留Feature Network中最后1层到Task Network中。 Leaves some tunable parameters after the feature propagation, which could be more general. Experiments Results Future 论文提到未来将会在flow estimation和key frame scheduling两方面进行优化。 此外，论文结尾说，提出的DFF框架可能会成为一个新的研究方向。 个人认为在该框架上做大量的优化工作： 将ResNet换成DenseNet 将shortcut idea引入FlowNet，尝试提高计算速度和准确度 进行多次propogation对结果进行refined，提高准确度 修改Task Network，从而将很多其他图像处理任务（除Detection、Segmentation、Classification外）应用到视频流上。]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Segmentation</category>
      </categories>
      <tags>
        <tag>Video</tag>
        <tag>Optical Flow</tag>
        <tag>FlowNet</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(MICCAI 2015) U-net:Convolutional networks for biomedical image segmentation]]></title>
    <url>%2F2017%2F09%2F19%2FU-net%3A%20Convolutional%20networks%20for%20biomedical%20image%20segmentation%2F</url>
    <content type="text"><![CDATA[Ronneberger O, Fischer P, Brox T. U-net: Convolutional networks for biomedical image segmentation[C]//International Conference on Medical image computing and computer-assisted intervention. Springer, Cham, 2015: 234-241. 该篇论文 在FCN基础上提出U-Net结构 (Figure 1). 提出医疗影像data augmentation. 结合两者能够trained end-to-end from very few images and outperforms sliding-window CNN. Localization: a class label is supposed to be assigned to each pixel. Sliding-window drawbacks Network需要对每个patch单独处理，重叠的patch产生大量冗余，因此非常慢。 Tradeoff between localization accuracy and the use of context. Large patches需要更多pooling层，导致localization accuracy下降，而small patches allow network see only little context. Overlap-tile Strategy (Figure 2) 该策略支持任意大小图片的无缝分割(seamless segmentation)，蓝色区域为输入patch，黄色区域为输出patch(输入图片进行镜像处理). Data Augmentation Shift and rotation invariance. Deformations and gray value invariance. Elastic deformation非常重要，能够有效模拟组织(tissue)最常见的形变方式。 Touching Object Challenge (Figure 3) 分离同种类型接触的细胞。 Propose the use of a weighted loss, where the separating background labels between touching cells obtain a large weight in the loss function. 预先计算ground-truth的weight map, to force the network to learn the small separation borders that we introduce between touching cells. d1,d2: the distance to the border of the nearest and second nearest cell. wc: balance the class frequencies. Initialization Ideally the initial weights should be adapted such that each feature map in the network has approximately unit variance. 论文采用Gaussian 方差srqt(2/$N$), $N$为输入Node数。例如3x3 Conv层64 kernels, 则$N$ = 9 * 64 = 576. Experiment Results]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Medical</category>
        <category>Segmentation</category>
      </categories>
      <tags>
        <tag>Network</tag>
        <tag>Segmentation</tag>
        <tag>Cell</tag>
        <tag>UNet</tag>
        <tag>ISBI</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(2015) Unsupervised representation learning with deep convolutional generative adversarial networks]]></title>
    <url>%2F2017%2F09%2F19%2FUnsupervised%20representation%20learning%20with%20deep%20convolutional%20generative%20adversarial%20networks%2F</url>
    <content type="text"><![CDATA[Radford A, Metz L, Chintala S. Unsupervised representation learning with deep convolutional generative adversarial networks[J]. arXiv preprint arXiv:1511.06434, 2015. 该篇论文提出Deep Convolutional GANs结构 (Figure 1)，使用一些方法来提高train稳定性，并通过实验验证 D的性能 可视化D的特征图 G的Walking in the Latent Space、遗忘性和Vector Arithmetic. 提高训练稳定性的方法 Stride Conv 替代 Pooling Eliminate FC层（相对于GAN中的FC而言） BN层，除G的输出层和D的输入层外，否则导致不稳定 G使用ReLU，输出层使用Tanh。D使用LeakyReLU** 验证D的性能 使用D作为Feature Extractor来classify CIFAR-10和SVHN。 D特征图可视化 不同特征图activate on 不同objects (Figure 5) 。 Walking in the Latent Space G的遗忘性 G能学到不同object的表达,在second highest Conv层(倒数第二层)的特征上，利用logistic regression 预测activate窗户的filters，比较drop out窗户相关filters与否的生成结果。在drop out窗户filter的生成结果中，一些图片去掉了窗户,一些图片生成相似的其他object，如门、镜子 (Figure 6)。 Vector Arithmetic of Z 类似于Word2vec of Mikolov (Figure 7)。Single sample per concept were unstable. Average $Z$ of 3 sample show consistence andstable. Train DCGAN on MNIST We found that removing the scale and bias parameters from batchnorm produced better results for both models. Noise introduced by batchnorm helps the generative models to better explore and generate from the underlying data distribution. Code Code comes from github AaronYALai/Generative_Adversarial_Networks_PyTorch. Code of GAN Code of DCGAN]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Image Processing</category>
      </categories>
      <tags>
        <tag>Face</tag>
        <tag>Image Generation</tag>
        <tag>GAN</tag>
        <tag>Scene</tag>
        <tag>DCGAN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(CVPR 2017) Convolutional neural network architecture for geometric matching]]></title>
    <url>%2F2017%2F09%2F19%2FConvolutional%20neural%20network%20architecture%20for%20geometric%20matching%2F</url>
    <content type="text"><![CDATA[Rocco I, Arandjelovic R, Sivic J. Convolutional neural network architecture for geometric matching[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2017: 6148-6157. 该篇论文 提出CNN结构mimic传统机器学习中的geometric matching算法 (Figure 1):feature extraction, matching, simultaneous inlier detection, model parameter estimation. 使用合成数据进行训练模型，不需人工标注数据（几何变换参数） 模型结构 (Figure 2)模型包含Feature extraction, Matching network和Regression network.图中两个Feature extraction CNN共享同样的参数，即用同一个网络提取A和B的特征。 Feature extraction Use VGG16 network cropped at the pool4 layer (before the ReLU unit), followed by per-feature L2-normalization. Matching network 采用Correlation map computation with CNN feature (Figure 3). 对于特征图A中的某个空间点，计算特征图B中每个空间点与其的相关性（模拟几何变换机器学习算法）。此外，使用channel-wise normalization和ReLu操作amplify the score of the match. 论文中将该方法与常用的Concatenation和Subtraction方法进行比较(Table 2)，证明该方法效果更好。原因是 后续的Regression Network是由一系列Conv层组成，unable to detect long-range matches. 对于相同几何变换的不同图像pair而言，Concatenation和Subtraction会产生不同的输出，会增加Regression Network的难度。 此外，对correlation map进行normalization能够提升4个百分点。 Regression Network 考虑到参数、内存和计算量的问题，Regression Network (Figure 4)采用具有局部感知特性的Conv层，而非FC层。这种方法能够Work是因为对于AB相关性特征图上的某个空间点而言，它包含了B特征图中该点与A特征图中所有空间点的相似性得分，因此虽然使用局部性的Conv，但仍然具有全局性。 Hierarchy of transformations 为了得到更精确的结果，论文提出了一个hierarchy模型 (Figure 5)。该模型包含2个stage。 第一阶段estimate 6 parameters的affine transformation. 第二阶段estimate 18 parameters的thin plate spline transformation. Loss Function $g_i$为uniform grid[-1, 1]，计算变换后的网格之间的距离平方。 Dateset Generate each example by sampling image A from a public image dataset, and generating image B by applying a random transformation $T_θ$GT to image A.]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Image Processing</category>
      </categories>
      <tags>
        <tag>geometric matching</tag>
        <tag>Image Transformation</tag>
        <tag>Image Generation</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(CVPR 2017) Learning from simulated and unsupervised images through adversarial training]]></title>
    <url>%2F2017%2F09%2F18%2Ftemplate%2F</url>
    <content type="text"><![CDATA[Shrivastava A, Pfister T, Tuzel O, et al. Learning from simulated and unsupervised images through adversarial training[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2017: 2107-2116. 该篇论文 贡献点 贡献点 贡献点 贡献点 贡献点 贡献点 贡献点 贡献点]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Image Processing</category>
      </categories>
      <tags>
        <tag>Image Generation</tag>
        <tag>GAN</tag>
        <tag>Reality Enhancement</tag>
        <tag>Eye</tag>
        <tag>SIMNet</tag>
      </tags>
  </entry>
</search>
