<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[(CVPR 2017 Best Paper) Densely connected convolutional networks]]></title>
    <url>%2F2017%2F09%2F19%2FDensely%20connected%20convolutional%20networks%2F</url>
    <content type="text"><![CDATA[Huang G, Liu Z, Van Der Maaten L, et al. Densely connected convolutional networks[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2017: 4700-4708. 该篇论文提出一种网络结构DenseNet (Figure 2)，能够在提高性能的同时，大量降低模型的参数及占用内存 (Figure 3)。 DenseNet Block (Figure 1)结构中包含大量skip connection(shortcut)： 解决梯度消失问题 提高feature reusing和propagation. 降低Conv层的filter(kernel)数量，使网络变得更narrow.(由于存在shortcut，在通过网络的时候，不会出现信息丢失的情况。) DenseNet细节包含以下几个主要部分： Transition Layer Growth Rate Bottleneck Layer Compression Transition Layer (图1) 连接在两个Dense Block之间，包含：BN- (1x1) Conv- (2x2) Avg Pooling Growth Rate denoted by $k$. 即$H$层 (Figure 1)中 (3x3) Conv层的filter数量为$k$. Bottleneck Layer 为了降低特征图数量，在H层的头部加上Bottleneck Layer，包含：BN-ReLU-(1x1)Conv. 该Conv层的filter数量小于输入特征图数量，从而达到降低特征图数量的目的 Compression 为了进一步提高模型的紧凑性，可以在Transition Layer中降低特征图数量。即降低其中的 (1x1) Conv层filter数量。 假设输入特征图数量为$m$，存在一个hyper-parameter $θ$ (0&lt;$θ$&lt;1) 使得输出特征图数量降低至$θ_m$，则(1x1) Conv层有$θ_m$ filters.]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Network</category>
      </categories>
      <tags>
        <tag>DenseNet</tag>
        <tag>Network</tag>
        <tag>Classification</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(CVPR 2017) Deep feature flow for video recognition]]></title>
    <url>%2F2017%2F09%2F19%2FDeep%20feature%20flow%20for%20video%20recognition%2F</url>
    <content type="text"><![CDATA[Zhu X, Xiong Y, Dai J, et al. Deep feature flow for video recognition[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2017: 2349-2358. 该篇论文首先说明了Video Recognition的Problem： 直接将图像识别网络应用到视频frame上会产生非常大的计算量。例如，假设图像识别网络处理一张图片需要0.1s，而30 fps的视频而言，处理1s的视频内容需要3s，这显然是不具有可行性的。 因此，论文提出了一个用于Video Recognition的Framework Deep Feature Flow (DFF) (Figure 2)： 仅仅将稀疏key frame输入Feature Network. 其他frame的特征图通过key frame的特征图进行propagation得到。 比较Figure 1中current frame feature maps和propagated feature maps： 将current frame输入Feature Network得到的特征图 对key frame特征图进行propagation得到的特征图 可以看出两类特征图基本类似。此外，可以明显看出Conv层中的#183 filter activate on车， #289 filter activate on 行人和狗。 通过Detection task实验(图1)可看出，DFF相对于Per-frame Network： 提高10倍速度。 Accuracy仅仅从73.9%降低到69.5%. DFF能大幅度提升速度是因为：Flow estimation and feature propagation are much faster than the computationof convolutional features. DFF结构包含3部分网络： Feature Network Flow Network Task Network 以及Inference和Training两个阶段 Feature Network 采用ResNet-50和ResNet-101 pre-trained on ImageNet Classification. Task Network DeepLab for segmentation R-FCN for object detection Inference Scale Function Better approximate the features the spatial warping may be inaccurate due to errors in flow estimation, object occlusion. 基于optical flow对key frame feature map进行bilinear interpolation 得到propagated feature map of current frame Key frame schedule Training (图3) 选取labeled frame作为current frame（一些Dataset只有少数帧有ground-truth），随机选取附近的帧作为key frame： Current frame == key frame. 计算current frame loss(右半部分). Current frame != key frame. 计算key frame loss（左半部分）. Inference阶段固定key frame， 顺序选取current frame. Dataset ImageNet VID for object detection Cityscapes for semantic segmentation Task Network与Feature Network分割 论文做了一个实验，验证将Feature Network中最后多少层放入Task Network中性能最好。从Table 5中可以看出在两个网络完全分离的情况下，性能最好。 DFF Framework的通用性，论文默认采用预留Feature Network中最后1层到Task Network中。 Leaves some tunable parameters after the feature propagation, which could be more general. Experiments Results Future 论文提到未来将会在flow estimation和key frame scheduling两方面进行优化。 此外，论文结尾说，提出的DFF框架可能会成为一个新的研究方向。 个人认为在该框架上做大量的优化工作： 将ResNet换成DenseNet 将shortcut idea引入FlowNet，尝试提高计算速度和准确度 进行多次propogation对结果进行refined，提高准确度 修改Task Network，从而将很多其他图像处理任务（除Detection、Segmentation、Classification外）应用到视频流上。]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Segmentation</category>
      </categories>
      <tags>
        <tag>Video</tag>
        <tag>Optical Flow</tag>
        <tag>FlowNet</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(MICCAI 2015) U-net:Convolutional networks for biomedical image segmentation]]></title>
    <url>%2F2017%2F09%2F19%2FU-net%3A%20Convolutional%20networks%20for%20biomedical%20image%20segmentation%2F</url>
    <content type="text"><![CDATA[Ronneberger O, Fischer P, Brox T. U-net: Convolutional networks for biomedical image segmentation[C]//International Conference on Medical image computing and computer-assisted intervention. Springer, Cham, 2015: 234-241. 该篇论文 在FCN基础上提出U-Net结构 (Figure 1). 提出医疗影像data augmentation. 结合两者能够trained end-to-end from very few images and outperforms sliding-window CNN. Localization: a class label is supposed to be assigned to each pixel. Sliding-window drawbacks Network需要对每个patch单独处理，重叠的patch产生大量冗余，因此非常慢。 Tradeoff between localization accuracy and the use of context. Large patches需要更多pooling层，导致localization accuracy下降，而small patches allow network see only little context. Overlap-tile Strategy (Figure 2) 该策略支持任意大小图片的无缝分割(seamless segmentation)，蓝色区域为输入patch，黄色区域为输出patch(输入图片进行镜像处理). Data Augmentation Shift and rotation invariance. Deformations and gray value invariance. Elastic deformation非常重要，能够有效模拟组织(tissue)最常见的形变方式。 Touching Object Challenge (Figure 3) 分离同种类型接触的细胞。 Propose the use of a weighted loss, where the separating background labels between touching cells obtain a large weight in the loss function. 预先计算ground-truth的weight map, to force the network to learn the small separation borders that we introduce between touching cells. d1,d2: the distance to the border of the nearest and second nearest cell. wc: balance the class frequencies. Initialization Ideally the initial weights should be adapted such that each feature map in the network has approximately unit variance. 论文采用Gaussian 方差srqt(2/$N$), $N$为输入Node数。例如3x3 Conv层64 kernels, 则$N$ = 9 * 64 = 576. Experiment Results]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Medical</category>
        <category>Segmentation</category>
      </categories>
      <tags>
        <tag>Network</tag>
        <tag>Segmentation</tag>
        <tag>Cell</tag>
        <tag>UNet</tag>
        <tag>ISBI</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(2015) Unsupervised representation learning with deep convolutional generative adversarial networks]]></title>
    <url>%2F2017%2F09%2F19%2FUnsupervised%20representation%20learning%20with%20deep%20convolutional%20generative%20adversarial%20networks%2F</url>
    <content type="text"><![CDATA[Radford A, Metz L, Chintala S. Unsupervised representation learning with deep convolutional generative adversarial networks[J]. arXiv preprint arXiv:1511.06434, 2015. 该篇论文提出Deep Convolutional GANs结构 (Figure 1)，使用一些方法来提高train稳定性，并通过实验验证 D的性能 可视化D的特征图 G的Walking in the Latent Space、遗忘性和Vector Arithmetic. 提高训练稳定性的方法 Stride Conv 替代 Pooling Eliminate FC层（相对于GAN中的FC而言） BN层，除G的输出层和D的输入层外，否则导致不稳定 G使用ReLU，输出层使用Tanh。D使用LeakyReLU** 验证D的性能 使用D作为Feature Extractor来classify CIFAR-10和SVHN。 D特征图可视化 不同特征图activate on 不同objects (Figure 5) 。 Walking in the Latent Space G的遗忘性 G能学到不同object的表达,在second highest Conv层(倒数第二层)的特征上，利用logistic regression 预测activate窗户的filters，比较drop out窗户相关filters与否的生成结果。在drop out窗户filter的生成结果中，一些图片去掉了窗户,一些图片生成相似的其他object，如门、镜子 (Figure 6)。 Vector Arithmetic of Z 类似于Word2vec of Mikolov (Figure 7)。Single sample per concept were unstable. Average $Z$ of 3 sample show consistence andstable. Train DCGAN on MNIST We found that removing the scale and bias parameters from batchnorm produced better results for both models. Noise introduced by batchnorm helps the generative models to better explore and generate from the underlying data distribution. Code Code comes from github AaronYALai/Generative_Adversarial_Networks_PyTorch. Code of GAN Code of DCGAN]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Image Processing</category>
      </categories>
      <tags>
        <tag>Image Generation</tag>
        <tag>GAN</tag>
        <tag>Face</tag>
        <tag>Scene</tag>
        <tag>DCGAN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(CVPR 2017) Convolutional Neural Network Architecture for Geometric Matching]]></title>
    <url>%2F2017%2F09%2F19%2FConvolutional%20neural%20network%20architecture%20for%20geometric%20matching%2F</url>
    <content type="text"><![CDATA[Rocco I, Arandjelovic R, Sivic J. Convolutional neural network architecture for geometric matching[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2017: 6148-6157. 该篇论文 提出CNN结构mimic传统机器学习中的geometric matching算法 (Figure 1):feature extraction, matching, simultaneous inlier detection, model parameter estimation. 使用合成数据进行训练模型，不需人工标注数据（几何变换参数） 模型结构 (Figure 2)模型包含Feature extraction, Matching network和Regression network.图中两个Feature extraction CNN共享同样的参数，即用同一个网络提取A和B的特征。 Feature extraction Use VGG16 network cropped at the pool4 layer (before the ReLU unit), followed by per-feature L2-normalization. Matching network 采用Correlation map computation with CNN feature (Figure 3). 对于特征图A中的某个空间点，计算特征图B中每个空间点与其的相关性（模拟几何变换机器学习算法）。此外，使用channel-wise normalization和ReLu操作amplify the score of the match. 论文中将该方法与常用的Concatenation和Subtraction方法进行比较(Table 2)，证明该方法效果更好。原因是 后续的Regression Network是由一系列Conv层组成，unable to detect long-range matches. 对于相同几何变换的不同图像pair而言，Concatenation和Subtraction会产生不同的输出，会增加Regression Network的难度。 此外，对correlation map进行normalization能够提升4个百分点。 Regression Network 考虑到参数、内存和计算量的问题，Regression Network (Figure 4)采用具有局部感知特性的Conv层，而非FC层。这种方法能够Work是因为对于AB相关性特征图上的某个空间点而言，它包含了B特征图中该点与A特征图中所有空间点的相似性得分，因此虽然使用局部性的Conv，但仍然具有全局性。 Hierarchy of transformations 为了得到更精确的结果，论文提出了一个hierarchy模型 (Figure 5)。该模型包含2个stage。 第一阶段estimate 6 parameters的affine transformation. 第二阶段estimate 18 parameters的thin plate spline transformation. Loss Function $g_i$为uniform grid[-1, 1]，计算变换后的网格之间的距离平方。 Dateset Generate each example by sampling image A from a public image dataset, and generating image B by applying a random transformation $T_θ$GT to image A.]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Image Processing</category>
      </categories>
      <tags>
        <tag>geometric matching</tag>
        <tag>Image Transformation</tag>
        <tag>Image Generation</tag>
      </tags>
  </entry>
</search>
