<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[(CVPR 2018) Low-Shot Learning from Imaginary Data]]></title>
    <url>%2F2019%2F03%2F26%2FLow-Shot%20Learning%20from%20Imaginary%20Data%2F</url>
    <content type="text"><![CDATA[Snell J, Swersky K, Zemel R. Prototypical networks for few-shot learning[C]//Advances in Neural Information Processing Systems. 2017: 4077-4087. 1. Overview 1.1. Motivation human can visualize or imagine what novel objects look like from different views In this paper, it proposes a novel approach to low-shot learning (learning concept from few examples) build on recent progress in meta-learning (learning to learn) combine a meta-learner with a hallucinator producing additional training examples optimize both model jointly proposed hallucinator can be incorporated into a variety of meta-learners 1.2. Low-shot Learning build generative models that can share priors across categories build feature representation that are invariant to intra-class variation triplet loss, contrastIve loss 1.3. Meta Learning try to frame low-shot learning as a ‘learning to learn’ task train a parametrized mapping from training sets to classifiers learner embeds examples into a feature space accumulate statistics over training set using RNN, MAN, MLP 1.3.1. Definition x. test sample S_train. training set p. prediction h. classification algorithm meta-learning is an umbrella term estimate w can be construed as meta-learning 1.3.2. Stage-1. Meta Training estimate w meta-learner has access to a large labeled dataset S_meta in each iteration, sample a classification problem out of S_meta m. the number of class in each iteration n. maximum number of training examples per clas 1.3.3. Stage-2. Meta Testing the labeled training set and unlabeled test examples are given to the algorithm h then algorithm h output class probabilities 1.3.4. Prototypical Networks (PN) d. distance metric w_phi. learned feature extractor 1.3.5. Matching Network (MN) embed all training and test points independently using feature extractor create a contextual embedding of training and test examples using bi-LSTM and attention-LSTM three parameters. w_phi, w_g and w_f 1.3.6. Prototype Matching Networks (PMN) in MN, attention LSTM might find it harder to attend to rare classes combine contextual embedding in MN with the resilience to class imbalance in PN collapse every class to its class mean 1.4. Meta-Learning with Learned Hallucination 1.4.1. Meta-Training BP final loss and update h and G jointly 1.4.2. Meta-Testing hallucinator (G) keeps fixed 1.4.3. Tradeoffs between Base and Novel Classes normal. prior knowledge. overall accuracy remains fairly stable, even as novel class accuracy rises and base class accuracy falls 1.4.4. New Evaluation test examples from novel classes. μ=1 from base classes from both 2. Experiments 2.1. Details train extractor on C_base, then save all feature to disk ResNet-10 and ResNet-50 2.1.1. Meta-learner Architecture PN. 2 MLP + ReLU MN. 1 bi-LSTM for training examples, 1 attention LSTM for test samples PMN same as MN 2.1.2. Hallucinator Architecture 3 MLP + ReLU all hidden layers. 512 for ResNet-10, 2048 for ResNet-50 initialization. block diagonal identity matrices to copy seed example 2.2. Comparison]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Learning</category>
        <category>Low-Shot Learning</category>
      </categories>
      <tags>
        <tag>Bi-LSTM</tag>
        <tag>Attention LSTM</tag>
        <tag>Low-Shot Learning</tag>
        <tag>Meta Learning</tag>
        <tag>Prototypical Network</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(NIPS 2017) Prototypical Networks for Few-shot Learning]]></title>
    <url>%2F2019%2F03%2F26%2FPrototypical%20Networks%20for%20Few-shot%20Learning%2F</url>
    <content type="text"><![CDATA[Snell J, Swersky K, Zemel R. Prototypical networks for few-shot learning[C]//Advances in Neural Information Processing Systems. 2017: 4077-4087. 1. Overview In this paper proposes Prototypical Network (PN) for few-shot learning extend PN to zero-shot learning euclidean distance better than cosine similarly 1.1. Few-Shot Learning c. mean of the support examples for each class classifier must generalize to new classes not seen in the training set given only a small number of examples (support set) of each new class 1.1.1. Episode Training (for example) training set. 200 classes, 10 samples/class each episode. randomly select 5 classes 4 samples/class as support set (GT can be used) 6 samples/class as query set (GT only use to calc loss) 1.1.2. Testing test set. new 20 classes, 20 samples/class each episode. randomly select 5 classes 5 samples/class as support set (GT can be used) 15 samples/class as query set (GT can be used, only to calc acc) 1.1.3. One-Shot Learning only one sample/class in support set 1.2. Zero-Shot Learning c. embed class meta-data v each class comes with meta-data giving a high-level description of the class rather than a small number of labeled examples 1.3. Related Work Matching Network (MN). in one-shot learning, PN=MN 2. Method 2.1. Algorithm zero-shot. 2.2. Episode N_c-way (class), N_S-shot (sample) higher way or shot is beneficial best to train and test with the same shot 3. Experiments 3.1. Datails3.1.1. Omniglot 28x28, gray 1623 characters, 20 samples/character rotate 90, 180. 270 to get different classes 1,200 x 4 classes as training set 1000 randomly generated episodes from test set 3.1.2. miniImageNet 3.1.3. CUB-200 3.2. Network 4 Conv each as [64, 3x3 Conv; BN; ReLU; Maxpooling] 3.3. Comparison]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Learning</category>
        <category>Few-Shot Learning</category>
      </categories>
      <tags>
        <tag>Prototypical Network</tag>
        <tag>Few-Shot Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(NIPS 2016) Matching Networks for One Shot Learning]]></title>
    <url>%2F2019%2F03%2F26%2FMatching%20Networks%20for%20One%20Shot%20Learning%2F</url>
    <content type="text"><![CDATA[Vinyals O, Blundell C, Lillicrap T, et al. Matching networks for one shot learning[C]//Advances in neural information processing systems. 2016: 3630-3638. 1. Overview In this paper, it proposes Matching Nets (MN) maps a small labeled support set and an unlabeled example to its label based on Bi-LSTM and Atten-LSTM experiments on vision (Omniglot, ImageNet) and language (Penn Treebank) devise new data set. miniImageNet (60,000 images, 84x84, 100 classes, 600 examples/class) 1.1. Model1.1.1. Basic Formulation x_i, y_i. labeled small support set a. attention mechanism 1.1.2. Attention Kernel g, f. embedded function c. cosine similarly x^. query image 1.1.3. Full Context Embeddings bi-LSTM S should be able to modify how to embed the test img x^ through f K. number of unrolling steps of LSTM 1.2. Loss Function 1.2.1. Training for each episode, sample L labels sample support set S and query set B 1.2.2. Testing know support set S’ predict label of query set B’ 1.3. Network 4 Conv [64, 3x3 Conv; BN; ReLU; Maxpool] 1.4. Experiments Omniglot ImageNet]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Learning</category>
        <category>One-Shot Learning</category>
      </categories>
      <tags>
        <tag>One-Shot Learning</tag>
        <tag>Bi-LSTM</tag>
        <tag>Attention LSTM</tag>
        <tag>Matching Net</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(ICML 2016) Meta-learning with memory-augmented neural networks]]></title>
    <url>%2F2019%2F03%2F26%2FMeta-learning%20with%20memory-augmented%20neural%20networks%2F</url>
    <content type="text"><![CDATA[Santoro, Adam, Bartunov, Sergey, Botvinick, Matthew, Wierstra, Daan, and Lillicrap, Timothy. Meta-learning with memory-augmented neural networks. In Proceedings of The 33rd International Conference on Machine Learning, pp. 1842–1850, 2016. 1. Overview 1.1. Motivation When new data is encountered, the models must inefficiently relearn In this paper, it proposes Memory-Augmented Neural Network (MANN), such as NTM quickly encode and retrieve new information rapidly assimilate new data, and leverage this data to make accurate predictions after only a few samples without re-training (one-shot learning) introduce a new method (Least Recently Used Access) for accessing an external memory slowly learn an abstract method for obtaining useful representations of raw data, via gradient descent rapidly bind never-before-seen information after a single presentation, via an external memory module 1.2. Meta-Learning Task Methodology offset input. label are shuffled from dataset-to-dataset must learn to hold data samples in memory until the appropriate labels are presented at the next time 1.2.1. Input[batch size, length of an episode, hwc+class_nb] for exmaple, [16, 50, 20201+5] for Omniglot dataset in the paper class_nb at time t is the label of input image at time t-1 1.2.2. Output[batch size, length of an episode, class_nb of an episode] for exmaple, [16, 50, 5]. Only 5 class in an episode whose length is 50 (input images) For a given episode, ideal performance involves a random guess for the first presentation of a class, and use of memory to achieve perfect accuracy thereafter. 1.3. Neural Turing MachinesMemory encoding and retrieval in a NTM external memory module is rapid. 1.3.1. Memory Read r. read memory 1.4. Least Recently Used Access content-based memory writer 1.4.1. Usage Weights wu. usage weights wr. read weights ww. write weights γ. decay parameter, 0.95 in this paper 1.4.2. Least-Used Weights wlu. least-used weights m(v, n). nth smallest element of the vector v 1.4.3. Write Weights sigma. sigmoid function α. learnable gate parameter 1.4.4. Memory Write prior to writing to memory, the least used memory location is set to zero 1.5. Access Module Code Input. (16, 50, 2020)(a) reshape to (50, 16, 2020)(b) each time (16, 20*20) come into Access Module and output M_t (16, 128, 40) c_t (16, 200) h_t (16, 200) r_t (16, 4*40) wr_t (16, 4, 128) wu_t (16, 128) (c) for 50 times, total get M (50, 16, 128, 40) c (50, 16, 200) h (50, 16, 200) r (50, 16, 4*40) wr (50, 16, 4, 128) wu (50, 16, 128) (d) cat r and h to get (50, 16, 200+160)(e) (50, 16, 360) · (360, 5) → (50, 16, 5) → (16, 50, 5)(f) calculate loss, BP and update 2. Experiments 2.1. DataSet2.1.1. Omniglot contains over 1600 separate classes wuth only a few examples per class, aptly leading to it being called the transpose of MNIST apply data augmentation. translate and rotate create new class through 90°, 180° and 270° rotations 1200 classes for training, 423 classes for testing image downscale to 20x20 the classes in testing set are different from classes in training set In testing set, each episode contains unique classes. 2.2. Performance x-axis. training episode. when entering a new episode, the memory will be set to 0 y-axis. testing performance nst Instance. during a certain episode, it is the nst time to see the sample of one class. five-character string label. to reduce the length of the one-hot label, it uses 5-length string to represent labels. 5 position for each contains 5 possible character→ total 5^5 class can be represent MANN with standard NTM access module is worse than with LRU Access 2.3. Persistent Memory Inference As each episode contains unique classes, wipe memory from episode to episode is important It becomes worse without wiping memory 2.4. Curriculum Training first tasked to classify fiftenn classes per episode every 10,000 episodes of training thereafter, the maximum number of classes presented per episode incremented by one]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Learning</category>
        <category>Meta Learning</category>
      </categories>
      <tags>
        <tag>Meta Learning</tag>
        <tag>MANN (Memory-Augmented Neural Network)</tag>
        <tag>Memory</tag>
        <tag>NTM (Neural Turing Machines)</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(IJCAI 2018) A Multi-task Learning Approach for Image Captioning]]></title>
    <url>%2F2019%2F03%2F12%2FA%20Multi-task%20Learning%20Approach%20for%20Image%20Captioning%2F</url>
    <content type="text"><![CDATA[Zhao W, Wang B, Ye J, et al. A Multi-task Learning Approach for Image Captioning[C]//IJCAI. 2018: 1205-1211. 1. Overview In this paper, it proposes Multi-task Learning Approach for Image Captioning (MLAIC) multi-object classification model. regularize CNN encoder syntax generation model image captioning model. benifit from object categorization and syntax knowledge 2. Architecture 2.1. Ground-Truth2.1.1. Object Vector 1 is present. C. the number of categories 2.1.2. Image Description T. sentence length 2.1.3. Combinatory Category Gramma (CCG) 2.2. Shared CNN Encoder image to L vectors (HxWxC–&gt;LxD) L=14x14, D=2048 shared CNN encoder fine-tuned with both captioning and classification 2.2.1. Classification 2.3. Shared LSTM Decoder LSTM_1. top-down visual attention model LSTM_2. language model 2.3.1. LSTM_1 Input. z. image mean feature e^w. previously generated word embedding e_s. previously generated syntax embedding h^(2). previously output of LSTM_2 Output. 2.3.2. ### LSTM_2 Input sigma. feed-forward NN Output 2.4. Multi-Task Learning classification. multi-label margin loss other two task. NLL All 3. Experiments 3.1. Details λ1 = 0.2, λ2 = 0.7, λ3 = 0.1 LSTM_1 = 1000 unit, LSTM_2 = 512 unit word embedding = 512, CCG supertag embedding = 100 beam size = 5 3.2. Comparison 3.3. Ablation]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Cross-Modality</category>
        <category>Caption</category>
      </categories>
      <tags>
        <tag>Caption</tag>
        <tag>NLP</tag>
        <tag>Cross-Modality</tag>
        <tag>Multi-task Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(CVPR 2017) WILDCAT:Weakly Supervised Learning of Deep ConvNets for Image Classification, Pointwise Localization and Segmentation]]></title>
    <url>%2F2019%2F03%2F05%2FWILDCAT%3A%20Weakly%20Supervised%20Learning%20of%20Deep%20ConvNets%20for%20Image%20Classification%2C%20Pointwise%20Localization%20and%20Segmentation%2F</url>
    <content type="text"><![CDATA[Durand T , Mordan T , Thome N , et al. WILDCAT: Weakly Supervised Learning of Deep ConvNets for Image Classification, Pointwise Localization and Segmentation[C]// 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE, 2017. 1. Overview In this paper, it proposes WILDCAT FCN backbone Multi-Map WSL transfer layer Wildcat Pooling 1.1. Related Work Max Pooling (MP) Global Average Pooling (GAP) LSE Pooling Compact Bilinear Pooling 2. Architecture 2.1. FCN ResNet101. output [W/32, H/32, 2048] remove GAP and FC replace with WSL transfer amd wildcat pooling layers 2.2. Multi-map Transfer Layer per class through 1x1 Conv (h, w, c) – (h, w, m*c) 2.3. Wildcat Pooling2.3.1. Classwise Pooling (h, w, mc) – (h, w, c) 2.3.2. Spatial Pooling for a class map (h, w, 1) average k+ max point average k- max point α. trade off hypothesize that maximum scoring regions are more useful for classification With α ＜ 1 Wildcat should focus more on discriminating regions and then better localize features than with α=1 2.4. Training input a single image 2.5. Inference classification. input a single image localization. extract the region with maximum score for each class segmentation. take the class with maximum score at eachlocation independently or apply CRF 3. Experiments 3.1. Details M = 4, α = 0.7 image. 448x448 3.2. Experiments3.2.1. Classification 3.2.2. WSL]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Architecture</category>
        <category>Component</category>
      </categories>
      <tags>
        <tag>Component</tag>
        <tag>Weakly Supervised Learning</tag>
        <tag>Classwise Pooling</tag>
        <tag>Spatial Pooling</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(ICIP 2016) Maxmin convolutional neural networks for image classification]]></title>
    <url>%2F2019%2F03%2F05%2FMaxmin%20convolutional%20neural%20networks%20for%20image%20classification%2F</url>
    <content type="text"><![CDATA[Blot M, Cord M, Thome N. Max-min convolutional neural networks for image classification[C]//2016 IEEE International Conference on Image Processing (ICIP). IEEE, 2016: 3678-3682. 1. Overview In this paper, it proposes to modify the CNN to transfer more information transmit directly the negative detections from a filter in order to prevent the network to learn the opposite filter 1.1. Related Work ReLU PReLU CReLU 1.2. Max-Min Methods 1.3. Experiments]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Architecture</category>
        <category>Component</category>
      </categories>
      <tags>
        <tag>Component</tag>
        <tag>Max-Min Pooling</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(CVPR 2016) Learning Deep Features for Discriminative Localization]]></title>
    <url>%2F2019%2F03%2F05%2FLearning%20Deep%20Features%20for%20Discriminative%20Localization%2F</url>
    <content type="text"><![CDATA[Zhou B, Khosla A, Lapedriza A, et al. Learning deep features for discriminative localization[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2016: 2921-2929. 1. Overview 1.1. Motivation recent work find that various layers of CNN behave as object detectors despite on supervision on the location of the object was provided. This ability is lost when FC are used for classification In this paper revisit global average pooling layers, shed light on how it explicitly enables CNN to have remarkble localization ability despite being trained on image-level labels 1.2. Related Work Weakly-supervised Object Localization global max pooling Visualizing CNNs DeConv 1.3. Class Activation Mapping (CAM) before final ouput layer, perform global average pooling on CNN feature then FC layer to produce the desired output finally, project back the weights of the output layer on CNN features 1.3.1. FormulationFor feature maps (K, h, w) f_k (1, h, w). feature map of kth channel F^k (1, 1, 1). kth channel feature after GAP final FC layer (K, C) w^c (K, 1) S_c (C) simply upsample the class activation map to the size of the input image 1.3.2. GAP vs GMP GMP. encourages to identify one discriminative part GMP. low scores for all image regions except the most discriminative one do not impact the score 2. Experiments 2.1. Weakly-supervised Object Localization find that the localization ability of the networks improved when the last Conv before GAP had a higher spatial resolution (13x13 for AlexNet, 14x14 for VGG, 14x14 for GooLeNet) 2.1.1. Localization value above 20% of the max value of CAM take the BBox covers the largest connected component in the segmentation map (top-5 predicted classes) 2.2. Visualize Class-Specific Units]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Visualization</category>
      </categories>
      <tags>
        <tag>Visualization</tag>
        <tag>Global Average Pooling</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(CVPR 2015) Recurrent convolutional neural network for object recognition]]></title>
    <url>%2F2019%2F03%2F05%2FRecurrent%20convolutional%20neural%20network%20for%20object%20recognition%2F</url>
    <content type="text"><![CDATA[Liang N M , Hu N X . Recurrent convolutional neural network for object recognition[C]// 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE Computer Society, 2015. 1. Overview 1.1. Motivation the visual system recurrent connections are abundant in CNNs context can only be captured in higher layers, but cannot modulate the activities of units in lower layers responsible for recognizing smaller objects In this paper, it proposes Recurrent CNN (RCNN) can integrate the context information unfolded network has multiple path. longer path to learn highly complex features shorter path to help gradient BP experiments on CIFAR-10, CIFAR-100, MNIST and SVHN 1.2. Recurrent Convolutional Layer (RCL) u. forward feature x. recurrent feature f. ReLU g. Local Response Normaliztion (LRN) 2. Experiments 2.1. Baseline 2.2. CIFAR-10 more iteration in RCNN led to both lower training and testing error more iteration in rCNN led to lower training error but higher testing error dropout palyed an important role for RCNN simply increasing K led to better results]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Architecture</category>
        <category>Network</category>
      </categories>
      <tags>
        <tag>Network</tag>
        <tag>Recurrent CNN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(ICML 2014) Recurrent convolutional neural networks for scene labeling]]></title>
    <url>%2F2019%2F03%2F05%2FRecurrent%20convolutional%20neural%20networks%20for%20scene%20labeling%2F</url>
    <content type="text"><![CDATA[Pinheiro P H O, Collobert R. Recurrent convolutional neural networks for scene labeling[C]//31st International Conference on Machine Learning (ICML). 2014 (EPFL-CONF-199822). 1. Overview In this paper, it propose an approach consist of a recurrent CNN allow to consider a large input context while limiting the capacity of the model as the context size increases with the built-in recurrence, the system identifies and corrects its own errors 1.1. Methods In Figure 2. The input is a patch and output is the prediction of centerpixel For the first time ,the feature is zero map]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Segmentation</category>
      </categories>
      <tags>
        <tag>Segmentation</tag>
        <tag>Recurrent CNN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(ECCV 2018) Uncertainty Estimates and Multi-Hypotheses Networks for Optical Flow]]></title>
    <url>%2F2019%2F03%2F05%2FUncertainty%20Estimates%20and%20Multi-Hypotheses%20Networks%20for%20Optical%20Flow%2F</url>
    <content type="text"><![CDATA[Ilg E, Cicek O, Galesso S, et al. Uncertainty estimates and multi-hypotheses networks for optical flow[C]//Proceedings of the European Conference on Computer Vision (ECCV). 2018: 652-667. 1. Overview 1.1. Motivation it is not possible to deploy such a system without information about how reliable the underlying estimates are we should expect an additional estimate of the network’s own uncertainty, such that the network can highlight hard cases where it cannot reliably estimate In this paper compare several strategies and techniques to estimate uncertainty introduce a network utilizing the Winner-Takes-All (WTA, penalize only the best prediction) loss without the need for sampling or ensembles 1.2. Network (e). the proposed network. generate multi-hypothesis (distribution) then merge]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Multiple Hypotheses</category>
      </categories>
      <tags>
        <tag>Multiple Hypotheses</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(ICLR 2018) Ensemble Adversarial Training:Attacks and Defenses]]></title>
    <url>%2F2019%2F02%2F26%2FEnsemble%20Adversarial%20Training%3A%20Attacks%20and%20Defenses%2F</url>
    <content type="text"><![CDATA[Tramèr F, Kurakin A, Papernot N, et al. Ensemble adversarial training: Attacks and defenses[J]. arXiv preprint arXiv:1705.07204, 2017. 1. Overview In this paper show that adversarial training converges to a defenerate global minimum find that adversarial training remains vulnerable to black-box attack, where we transfer perturbations computed on undefended model, as well as to proposed single-step attack introduce Ensemble Adversarial Training that augments training data with perturbations transferred from other model (decouples adversarial example generation from the parameters of the trained model) Ensemble Adversarial Training yields models with strong robustness to black-box attack 1.1. Adversarial Training adversarial training on MNIST yields models that are robust to white-box attacks MNIST dataset is peculiar in that there exists a simple ‘closed-form’ denoising procedure (namely feature binarization) which leads to similarly robust models without adversarial training. This may explain why robustness to white-box attack is hard to scale to tasks such as ImageNet for an average MNIST image, over 80% of the pixels are in {0, 1} and only 6% are in the range [0.2, 0.8]. Thus, for a perturbation with epsilon ≤ 0.3, binarized version of x and x_adv can differ in at most 6% of the input dimension Some prior works have hinted that adversarially trained models may remain vulnerable to black-box attacks an adversarial maxout network on MNIST has slightly higher error on transferred examples than on white-box examples 1.2. Attack Methods FGSM Single-Step Least-Likely Class Method (Step-LL). most effective for adversarial training on ImageNet Iter FGSM and Iter Step-LL proposed randomized single-step attack 1.3. Ensemble Adversarial Training decouple the generation of adversarial example from the model being trained augments training data with adversarial examples crafted on other static pre-trained models 1.4. Experiments adversarial training greatly increases robustness to white-box single-step attacks, but incurs a higher error rate in a black-box setting 1.4.1. Ensemble Training Ensemble Adversarial Training is not robust to white-box Iter-LL and R+Step-LL sample]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Adversarial Attack</category>
      </categories>
      <tags>
        <tag>Adversarial Attack</tag>
        <tag>Ensemble Adversarial Training</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(ICML 2012) Poisoning Attacks against Support Vector Machines]]></title>
    <url>%2F2019%2F02%2F26%2FPoisoning%20Attacks%20against%20Support%20Vector%20Machines%2F</url>
    <content type="text"><![CDATA[Biggio B , Nelson B , Laskov P . Poisoning Attacks against Support Vector Machines[J]. 2012. 1. Overview In this paper, it investigates a family of poisoning attacks against SVM. Such attacks inject specially crafted training data that increasing the SVM’s test error. use a gradient ascent strategy the gradient is computed based on properties of the SVM’s optimal solution 1.1. Attack Category1.1.1. Causative manipulation of training data. such as poisoning attack 1.1.2. Exploratory exploitation of the classifier 1.2. Discussion as an attacker usually cannot directly access an existing training databse but may provide new training data poisoning attacks have been previously studied only for simple anomaly detection methods assume that the attacker knows the learning algorithm and can draw data from the underlying data distribution further assume that attacker knows the training data used by the learner 1.3. Overview the attacker’s goal is to find a point (x_c, y_c), whose addition to training data D_tr maximally decreases the SVM’sclassification accuracy y_c. attacking class the attacker proceeds by drawing a validation data set D_val and maximizing the hinge loss incurred on D_val by the SVM trained on D_tr ∪(x_c, y_c) 1.4. Visualization]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Adversarial Attack</category>
      </categories>
      <tags>
        <tag>Adversarial Attack</tag>
        <tag>Poisoning Attack</tag>
        <tag>SVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(2018) DARCCC:Detecting Adversaries by Reconstruction from Class Conditional Capsule]]></title>
    <url>%2F2019%2F02%2F26%2FDARCCC%3A%20Detecting%20Adversaries%20by%20Reconstruction%20from%20Class%20Conditional%20Capsule%2F</url>
    <content type="text"><![CDATA[Frosst N, Sabour S, Hinton G. DARCCC: Detecting Adversaries by Reconstruction from Class Conditional Capsules[J]. arXiv preprint arXiv:1811.06969, 2018. 1. Overview In this paper present a simple tachnique that allows capsule models (DARCCC) to detect advertisarial images set a threshold on L2 distance between input image and reconstruction from winning capsule same technique works well for CNNs explore a stronger white-box attack (R-BIM) that takes the reconstruction error into account. However the generated adversarial examples do not look like the original image but with a small amount of adder noise experiments on MNIST, Fashion-MNIST and SVHN 1.1. Related Works CapsuleNet has been proven to be more robust to white box attacks while being as weak as CNNs in defending black box attacks (this paper address this shortcoming) 2. Methods 2.1. Histogram of L2 Distance 2.2. Network 2.3. Threshold set it as the 95th percentile of validation distances. That meas FPR on real validation is 5% 3. Experiments 3.1. Block Box Attack CapsuleNet is as weak as CNN 3.2. White Box Attack CapsuleNet is more robust to CNN 3.3. R-BIM R-BIM is significantly less successful than a standard BIM attack in changing the classification CapsuleNet in particular exhibits significant resilience to this attack 3.4. Visualize some generated adversarial examples look like ‘0’. These are not adversarial images at all since they resemble their predicted class to the human eye]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Adversarial Attack</category>
      </categories>
      <tags>
        <tag>Adversarial Attack</tag>
        <tag>Defence</tag>
        <tag>CapsulesNet</tag>
        <tag>CapsuleNet</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(2018) LaVAN:Localized and Visible Adversarial Noise]]></title>
    <url>%2F2019%2F02%2F26%2FLaVAN%3A%20Localized%20and%20Visible%20Adversarial%20Noise%2F</url>
    <content type="text"><![CDATA[Karmon D, Zoran D, Goldberg Y. Lavan: Localized and visible adversarial noise[J]. arXiv preprint arXiv:1801.02608, 2018. 1. Overview In this paper, it proposed a algorithm to generate adversarial patches without covering the main objects of images only covering 2% of the pixels transferable across images and locations fool InceptionV3 model 1.1. Methods1.1.1. Loss Function 1.1.2. To be Universal At each iteration, choose a random image x 1.2. Visualization]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Adversarial Attack</category>
      </categories>
      <tags>
        <tag>Adversarial Attack</tag>
        <tag>Adversarial Patch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(2017) Adversarial patch]]></title>
    <url>%2F2019%2F02%2F26%2FAdversarial%20patch%2F</url>
    <content type="text"><![CDATA[Brown T B, Mané D, Roy A, et al. Adversarial patch[J]. arXiv preprint arXiv:1712.09665, 2017. 1. Overview In this paper, it proposed a methods to generate universal targeted adversarial patches patches can be place anywhere explore what is possible if an attacker no longer restricts to imperceptible changes (the existing defense techniques which focus on defending against small perturbations may bot be robust to larger perturbations) 1.1. Algorithm mask our patch to allow it to take any shape train over a variety of images apply a random translation, scaling, and rotation on the patch in each image 1.2. Experiments Five models: InceptionV3, ResNet50, Xception, VGG16 and VGG19 white box attack (ensemble and single) and black box attack]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Adversarial Attack</category>
      </categories>
      <tags>
        <tag>Adversarial Attack</tag>
        <tag>Adversarial Patch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(CVPR 2016) Inverting Visual Representations with Convolutional Networks]]></title>
    <url>%2F2019%2F02%2F15%2FInverting%20Visual%20Representations%20with%20Convolutional%20Networks%2F</url>
    <content type="text"><![CDATA[Dosovitskiy A, Brox T. Inverting Visual Representations with Convolutional Networks[C]// IEEE Conference on Computer Vision &amp; Pattern Recognition. 2016. 1. Overview In this paper, it proposes a new approach to study image representation by inverting them with an up-convolutional neural network apply to shallow representation. HOG, SIFT, LBP apply to deep representation. DNN 1.1. Conclusion1.1.1. Representation of AlexNet Features from all layers of the network preserve the precise colors and the rough position of objects in the image In higher layers, almost all information about the input image is contained in the pattern of non-zero activations, not their precise values In the layer FC8, most information about the input image is contained in small probabilities of those classes that are not in top-5 network predictions 1.2. Related Work Local Binary Pattern (LBP). LBP features are not differentiable SIFT. keypoint-based representation 1.2.1. Existing Method Based on Gradient Descent invert a differentiable image representation phi using gradient descent, so it can not be applied to LBP optimize the difference between the feature vectors, not the image reconstruction error involve optimization at test time 1.3. Methods1.3.1. Loss Function phi. feature vector w. parameters of CNN 1.3.2. Network 1.3.3. HOG and LBP HOG feature. W/8 x H/8 x 31 LBP feature. W/16 x H/16 x 58 continue process by CNN untill feature size is 64 times smaller than input 1.3.4. Sparse SIFT N keypoints. each keypoint contains: coordinate (x, y), scale s, orientation α, feature descriptor f split image into cells of size d x d, yhis yields W/d x H/d feature. W/d x H/d x (D+5) 1.4. Experiments1.4.1. Metric 1.4.2. Shallow Representation 1.4.3. Deep Representation]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Image Processing</category>
        <category>Inverting</category>
      </categories>
      <tags>
        <tag>Image Generation</tag>
        <tag>Inverting</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(CVPR 2015) Understanding Deep Image Representations by Inverting Them]]></title>
    <url>%2F2019%2F02%2F15%2FUnderstanding%20Deep%20Image%20Representations%20by%20Inverting%20Them%2F</url>
    <content type="text"><![CDATA[Mahendran A, Vedaldi A. Understanding deep image representations by inverting them[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2015: 5188-5196. 1. Overview In this paper, it proposes a general framework to invert representation invert a differentiable image representation using gradient descent first work to study the inverse of CNN image representation 1.1. Related Work DSIFT, HOG Exploring the representation capabilities of the HOG descriptor. show that it is possible to make any two images look nearly identical in SIFT space up to the injection of adversarial noise Intriguing properties of neural networks. demonstrated that adversarial example 1.2. Algorithm try to find the best x involve optimization at test time 1.3. Experiments]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Image Processing</category>
        <category>Inverting</category>
      </categories>
      <tags>
        <tag>Image Generation</tag>
        <tag>Inverting</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(2016) Estimation of ambient light and transmission map with common convolutional architecture]]></title>
    <url>%2F2019%2F02%2F15%2FEstimation%20of%20ambient%20light%20and%20transmission%20map%20with%20common%20convolutional%20architecture%2F</url>
    <content type="text"><![CDATA[Shin Y S , Cho Y , Pandey G , et al. [IEEE OCEANS 2016 MTS/IEEE Monterey - Monterey, CA, USA (2016.9.19-2016.9.23)] OCEANS 2016 MTS/IEEE Monterey - Estimation of ambient light and transmission map with common convolutional architecture[J]. 2016:1-7. 1. Overview 1.1. Motivation the scattering of light from water particles along with the attenuation and change in the color of different wavelengths of ambient light In this paper, it proposes a method for effective ambient light and transmission estimation in underwater image. balance scene recovery model 1.2. Dataset synthetic based on ICL-NUIM and SUM database 1 million patches 1.3. Model multi-scale fusion element-wise summation (reduce computation) Maxout 1.4. Balance Scene Recovery Model1.4.1. Haze Model can not achieve the recovery of the original scene radiance in an underwater environment the attenuation of ambient light in an underwater environment is not only dependent upon the distance travelled and density of particles in the path of the light but also depends on the color/wavelength of the light For instance, the light intensity of red channel rapidly decreases whereas green channel decreases slowly 1.4.2. Proposed Model A_b. balanced ambient light a_b. fixed vector which represents the balanced ambient light inRGB 1.5. Experiments]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Image Processing</category>
        <category>Dehazing</category>
      </categories>
      <tags>
        <tag>Dehazing</tag>
        <tag>Image Processing</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(CVPR 2017) Image Super-Resolution via Deep Recursive Residual Network]]></title>
    <url>%2F2019%2F02%2F15%2FImage%20Super-Resolution%20via%20Deep%20Recursive%20Residual%20Network%2F</url>
    <content type="text"><![CDATA[Tai Y , Yang J , Liu X . Image Super-Resolution via Deep Recursive Residual Network[C]// 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE Computer Society, 2017. 1. Overview In this paper, it proposes Deep Recursive Residual Network (DRNN) adopt residual learning (local + global) recursive to control the model parameters while increasing depth 2x, 6x, 14x fewer parameters than VDSR, DRCN and RED30 1.1. Related Works1.1.1. VDSR high LR to accelerate the convergence speed residual learning + adjustable gradient clipping to sovel gradient explosion problem 1.1.2. DRCN chain strucuture recursive-supervision and skip-connection to mitigate the difficulty of training 1.2. Novelties both global and local residual learning recursive learning. Increase the depth without adding parameters 1.3. DRRN1.3.1. Pre-Activation 1.3.2. Recursive 1.3.3. Architecture 1.3.4. Parameters U. the number of residual unit in a recursive block B. the number of recursive block when U=0, DRRN becomes VDSR depth of DRRN 1.3.5. Loss Function 2. Experiments 2.1. Dataset Set5 Set14 BSD100 Urban100 2.2. Augmentation flipping rotation scale. x2, x3, x4 training sample. 31x31 patches with stride of 21 2.3. Details adjustable gradient clipping γ. current learning rate Θ = 0.01. gradient clipping parameter DRRN with d=20 takes 4 days with 2 Titan X GPUs Metric. PSNR, SSIM, IFCs 0.25s per 288x288 image on a Titan X GPU 2.4. Study of B and U 2.5. Comparison 2.6. Discussion DRRN_NS. no sharing weights DRRN_C. chain strucuture LRL improves VDSR at all depth weight sharing DRRN (recursive strategy) better than without sharing, less prone to overfitting]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Image Processing</category>
        <category>Super Resolution</category>
      </categories>
      <tags>
        <tag>Image Processing</tag>
        <tag>Super Resolution</tag>
        <tag>Recursive Residual Network</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(ICML 2016) Pixel recurrent neural networks]]></title>
    <url>%2F2019%2F02%2F15%2FPixel%20recurrent%20neural%20networks%2F</url>
    <content type="text"><![CDATA[Oord A, Kalchbrenner N, Kavukcuoglu K. Pixel recurrent neural networks[J]. arXiv preprint arXiv:1601.06759, 2016. 1. Overview In this paper, it proposed PixelRNN, a deep neural network that sequentially predicts the pixels in an image along the two spatial dimensions. 1.1. PixelRNN]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Image Processing</category>
      </categories>
      <tags>
        <tag>Image Processing</tag>
        <tag>Image Generation</tag>
        <tag>PixelCNN</tag>
        <tag>PixelRNN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(ECCV 2018) Recurrent Squeeze-and-Excitation Context Aggregation Net for Single Image Deraining]]></title>
    <url>%2F2019%2F02%2F15%2FRecurrent%20Squeeze-and-Excitation%20Context%20Aggregation%20Net%20for%20Single%20Image%20Deraining%2F</url>
    <content type="text"><![CDATA[Li X, Wu J, Lin Z, et al. Recurrent squeeze-and-excitation context aggregation net for single image deraining[C]//Proceedings of the European Conference on Computer Vision (ECCV). 2018: 254-269. 1. Overview In this paper, it proposed RESCAN dilated convolutional SE block to achive different alpha-values to various rain streaks layers multi-stage with RNN. useful information for rain removal in previous stages can guide the learning in later stages 2. Methods 2.1. Rain Model A. global atmospheric light α_0. scene transmission α_i. brightness of a rain streak layer or a haze layer 2.2. Architecture depth = 6 Dilation of L1 to L3 (1, 2, 4) no BN. rain streaks in different layers have different distributions, and remove 40% memory 2.3. Recurrent 2.3.1. Recurrent Version ConvRNN ConvGRU ConvLSTM 2.4. Prediction2.4.1. Additive Prediction 2.4.2. Full Prediction 3. Experiments 3.1. Details patch 64x64 3.2. Ablation Study 3.3. Comparison]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Image Processing</category>
        <category>De-raining</category>
      </categories>
      <tags>
        <tag>Image Processing</tag>
        <tag>De-raining</tag>
        <tag>SE Block</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(NIPS 2018) Non-Local Recurrent Network for Image Restoration]]></title>
    <url>%2F2019%2F02%2F15%2FNon-Local%20Recurrent%20Network%20for%20Image%20Restoration%2F</url>
    <content type="text"><![CDATA[Liu D, Wen B, Fan Y, et al. Non-local recurrent network for image restoration[C]//Advances in Neural Information Processing Systems. 2018: 1673-1682. 1. Overview 1.1. Motivation existing methods do not explicitly use self-similarity properties in images In this paper, it proposed non-local recurrent network (NLRN). 2. Methods 2.1. Non-local Module 2.2. Non-Local Recurrent Network x. input state y. output state s. recurrent state s^0. function of input Image I. x^t = 0, f_input(x) = 0 output state y^t calculate only at time T s^0. add an identity path from the very first state s_corr. add a residual path of deep feature correlation between each location and its neighbourhood from previous state 3. Experiments 3.1. Ablation Study 3.2. Comparison]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Image Processing</category>
        <category>Image Restoration</category>
      </categories>
      <tags>
        <tag>Image Processing</tag>
        <tag>Image Restoration</tag>
        <tag>Non-local</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(CVPR 2018) Future Frame Prediction for Anomaly Detection -- A New Baseline]]></title>
    <url>%2F2019%2F01%2F25%2FFuture%20Frame%20Prediction%20for%20Anomaly%20Detection%20--%20A%20New%20Baseline%2F</url>
    <content type="text"><![CDATA[Liu W, Luo W, Lian D, et al. Future frame prediction for anomaly detection–a new baseline[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018: 6536-6545. 1. Overview 1.1. Motivation In anomaly video detection, almost all existing methods tackle the problem by minimizing the reconstruction errors of training data, which can not guarantee a large reconstruction error for an abnormal event The capacity of DNN is high, and larger reconstruction errors for abnormal events do not necessarily happen Abnormal events are unbounded In this paper, it proposes to tackle the anomaly detection problem within a video prediction framework first work to leverage the difference between predicted future frame and GT first work to introduce a temporal constraint into video prediction task other than spatial constraints (intensity and gradient), also introduce temporal constraint 1.2. Related Work Hand-craft Feature. HOG, HOF Deep Learning. ConvLSTM-AE Video Frame Prediction Least Square GAN 2. Architecture 2.1. Overview 2.2. Contraints Intensity Gradient Motion f. pre-trained FlowNet Adversarial (Least Square GAN) 2.3. Loss Function frame normalize to [-1, 1] frame resize to 256x256 t=4; random clip of 5 sequential frames batch size 4 int, gd, op, adv: 1.0, 1.0, 2.0, 0.05 2.4. Anomaly Detection on Testing Data Mathieu shows that Peak Signal to Noise Ratio (PSNR) is a better way for image quality assessment (higher PSNR → normal) normalize PSNR of all frames in each testing video to the range [0, 1], and calculate the regular score for each frame by 3. Experiments 3.1. Dataset3.1.1. CUHK Avenue Dataset 16 training videos and 21 testing videos 47 abnormal events 3.1.2. UCSD Dataset UCSD Pedestrian 1. 34 training videos and 36 testing videos with 40 irregular events UCSD Pedestrian 2. 16 training videos and 12 testing videos with 12 abnormal events 3.1.3. ShanghaiTech Dataset 330 training videos and 107 testing 130 abnormal events 3.1.4. Toy Dataset 210 frames for training 1242 frames for testing 3.2. Comparison 3.3. Ablation Study Delta. gap between average score of normal frames and that of abnormal frames 3.4. Toy Experiments After observing the pedestrian for a while when the pedestrian has made his or her choice, it becomes predictable and PSNR would go up]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Abnormality Detection</category>
      </categories>
      <tags>
        <tag>Abnormality Detection</tag>
        <tag>VIdeo Frame</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(2018) GANomaly:Semi-Supervised Anomaly Detection via Adversarial Training]]></title>
    <url>%2F2019%2F01%2F25%2FGANomaly%3A%20Semi-Supervised%20Anomaly%20Detection%20via%20Adversarial%20Training%2F</url>
    <content type="text"><![CDATA[Akcay S, Atapour-Abarghouei A, Breckon T P. GANomaly: Semi-Supervised Anomaly Detection via Adversarial Training[J]. 2018. 1. Overview In this paper, it proposes a novel anomaly detection model (output anomaly score) Conditional GAN jointly learn the generation of high-dimensional image space and the inference of latent space only trained on nromal samples 1.1. Problem Definition large training set. M normal samples smaller testing set. N samples (normal + abnormal) model f. learn the normal data distribution and minimizes the output anomaly score phi. threshold (set to 0.2) 2. Methods 2.1. Model 2.1.1. For Abnormal Image G_D is not able to reconstruct the abnormalities the network is modeled only on normal samples during training and its parametrization is not suitable for generating abnormal samples An output X^ that has missed abnormalities can lead to the encoder network R mapping X^ to a vector z^ that has also missed abnormal feature representation, causing dissimilarity between z and z^ 2.2. Loss Function 3. Experiments 3.1. Dataset MNIST (32x32). treating one class being an anomaly, while other as normal class CIFAR10. one as abnormal class University Baggage Anomaly Dataset (UBA) (64x64). abnormal class: knife, gun and gun componen 3.2. Setting λ=50 Metric. AUC, ROC and TPR (true positive rate), FPR 3.3. Comparison Table 1. all approaches show very poor performance for detecting digit 1 as abnormal. This is probably due to the linear shape simplicity of this class such that any model can easily overfit to the data Table 2. The reason for getting relatively lower quantitative results within this dataset is that for a selected abnormal category, there exists a normal class the is similar to the abnormal (plane vs bird, cat vs dog) 3.4. Speed]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Abnormality Detection</category>
      </categories>
      <tags>
        <tag>GAN</tag>
        <tag>Abnormality Detection</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(ICCV 2017) Learning in an Uncertain World:Representing Ambiguity Through Multiple Hypotheses]]></title>
    <url>%2F2019%2F01%2F12%2FLearning%20in%20an%20Uncertain%20World%3A%20Representing%20Ambiguity%20Through%20Multiple%20Hypotheses%2F</url>
    <content type="text"><![CDATA[Rupprecht C, Laina I, DiPietro R, et al. Learning in an uncertain world: Representing ambiguity through multiple hypotheses[C]//Proceedings of the IEEE International Conference on Computer Vision. 2017: 3591-3600. 1. Overview 1.1. Motivation uncertainty arises from the way data is labeled (label of occluded joints in pose) In this paper reformulate existing single-prediction modles as multiple hypothesis prediction (MHP) modles (meta loss) MHP can expose valuable insights outperform SHP experiments on human pose estimation future frame prediction classification (multi-label) segmentation 1.2. Related Work Multiple Choice Learning Multi-label Recognition 2. Methods 2.1. SHP 2.2. MHP2.2.1. Meta Loss Function M. the number of predictions (replicate output layer M times with differ init) Delta. True for 1, False for 0 f_j. the j-th predictor among M predictors Meta loss function can be regarded as original loss function weighted by Deltal 2.2.2. Procedure create M predictors, then forward each sample build y_i(x) compute gradient and update 2.2.3. Relax Delta solve the problem: predictor may be initialized so far from the target labels y that all y lie in a single Voronoi cell k Additionally, drop out predictions with some low probability (1%) to introduce some randomness in the selection of the best hypothesis, such that weaker predictions will not vanish during training 2.2.4. Hyper-parameter M almost every method that models posterior probabilities needs some form of hand-tuned model parameter (k-means, MDNs) 3. Experiments 3.1. Pose SHP. 59.7% 2-MHP. 60.0% 5-MHP. 61.2% 10-MHP. 62.8% with increasing number of predictions the method is able to model the output space more and more-precisely 3.2. Future Frame Predictions 3.3. Classification if an image contains two bikes and a person, every time the image is sampled during training it will be labeled either as bike or person with 50% 3.4. Segmentation MHP (70.3%) vs MCL (69.1%)]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Multiple Hypotheses</category>
      </categories>
      <tags>
        <tag>Pose</tag>
        <tag>Multiple Hypotheses</tag>
        <tag>Frame Prediction</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(NIPS 2018 Spotlight) Training Neural Networks Using Features Replay]]></title>
    <url>%2F2019%2F01%2F12%2FTraining%20Neural%20Networks%20Using%20Features%20Replay%2F</url>
    <content type="text"><![CDATA[Huo Z, Gu B, Huang H. Training neural networks using features replay[C]//Advances in Neural Information Processing Systems. 2018: 6659-6668. 1. Overview 1.1. Motivation the computational time of the backward pass is about twice of the computational time of the forward pass existing methods removing backward locking work poorly when DNN is deep In this paper propose a novel parallel-objective formulation for the objective function of the neural network introduce features reply algorithm prove it is guaranteed to converge experiments to demonstrate the proposed method achieves faster convergence, lower memory consumption and better generalisation error 2. Algorithm 2.1. Formulation parallel-objective loss function optimal solution 2.2. Breaking Dependency2.2.1. New Formulation each module is independent and updated at the same time module k keeps a history of its input with size K-k+1 2.2.2. Approximation 2.2.3. Gradient inside Each Module 2.2.4. Pass Gradient module k sends to the gradient below module k-1 for the computation of the next iteration 2.3. Procedure 3. Experiments 3.1. Implementations CIFAR-10 and CIFAR-100 ResNet random cropping, random horizontal flipping and normalizing K modules is distributed across K GPUs 3.2. Sufficient Direction Constant σ divide ResNet164 and ResNet101 into 4 modules σ &gt; 0 all the time. Assumption 1 is satisfied such that Algorithm 1 is guaranteed to converge to the critical points of for the non-convex problem σ of lower modules are small at the first half epochs. The variation of σ indicates the difference between the descent direction of FR and the steepest descent direction small σ at early epochs. help method escape from saddle points and find better local minimum large σ at final epochs. prevent method from diverging 3.3. Memory Consumption 3.4. Generalization]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Architecture</category>
        <category>Optimization</category>
      </categories>
      <tags>
        <tag>Optimization</tag>
        <tag>Back Propagation</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(CVPR 2018 Best Paper) Taskonomy:Disentangling Task Transfer Learning]]></title>
    <url>%2F2019%2F01%2F12%2FTaskonomy%3A%20Disentangling%20Task%20Transfer%20Learning%2F</url>
    <content type="text"><![CDATA[Zamir A R, Sax A, Shen W, et al. Taskonomy: Disentangling Task Transfer Learning[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018: 3712-3722. 1. Overview 1.1. Motivation relationships among different visual tasks In this paper proposes a fully computational approach for modeling the structure of space of visual task exploit it to reduce the demand for labeled data 1.2. Related Works self-supervised learning unsupervised learning meta-learning multi-task learning domain adaption 2. Methods 2.1. Definitions γ. limited supervision budget T (target). set of task want to solve S (source). set of task can be trained V=T∪S. task dictionary T∩S. task want to solve but can play as source T-T∩S. task can not trained (target only) S-T∩S. (source only) edge. between a group of source and target tasks, represent feasible case weight. prediction of its performance use these edges to estimate the globally optimal transfer policy to solve T Four Steps 2.2. Stage I: Task-Specific Modeling Encoder-Decoder 2.3. Stage II: Transfer Modeling learn a readout function E_s. encoder D_Θ. readout function f_t. gt of task t for image I. the performance of D_{s→ t} is a useful metric as task affinity use shallow fully convolutional network combinatorial explosion of higher-oder transfer. using beam search to sample 2.4. Stage III: Ordinal Normalization using Analytic Hierarchy Process (AHP) aggregating the raw losses/evaluations L_{s→ t} from transfer function into a matrix is problem. vastly different scale and live in different spaces naive solution of linearly rescale each fail 2.5. Step IV: Computing the Global Texonomy edge (hypergraph) Boolean Integer Programming (BIP) to find global policy 2.6. Constrains each target task has exactly one transfer in supervision budget is not exceeded]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Learning</category>
        <category>Transfer Learning</category>
      </categories>
      <tags>
        <tag>Transfer Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(ECCV 2016) Identity mappings in deep residual networks]]></title>
    <url>%2F2019%2F01%2F12%2FIdentity%20mappings%20in%20deep%20residual%20networks%2F</url>
    <content type="text"><![CDATA[He K, Zhang X, Ren S, et al. Identity mappings in deep residual networks[C]//European Conference on Computer Vision. Springer, Cham, 2016: 630-645. 1. Overview In this paper, it analyzed the propagation formulations behind the residual building blocks identity mapping is better pre-activation is better 1.1. Identity Mapping 1.2. Pre-Activation pre-activation (BN + ReLU) can normalize the signal of residual + F(x) which is not normalized]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Architecture</category>
        <category>Component</category>
      </categories>
      <tags>
        <tag>Component</tag>
        <tag>Identity Mapping</tag>
        <tag>Pre-activation</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(CVPR 2018) Convolutional Neural Networks with Alternately Updated Clique]]></title>
    <url>%2F2019%2F01%2F12%2FConvolutional%20Neural%20Networks%20with%20Alternately%20Updated%20Clique%2F</url>
    <content type="text"><![CDATA[Yang Y, Zhong Z, Shen T, et al. Convolutional Neural Networks with Alternately Updated Clique[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018: 2413-2422. 1. Overview In this paper, it proposed CliqueNet improving information flow both forward and backward connections between any two layers in the same block combination of recurrent structure and feedback mechanisim 1.1. Contribution CliqueNet multi-scale feature strategy experiments on five datasets 1.2. Related Work1.2.1. Network Multi-column networks Deeply-Fused Nets GooLeNet (WRN) wide residual networks FractalNet ResNet DenseNet (DPN) dual path networks 2. Methods 2.1. Clique Block recurrent feedback structure ensures that the communication is maximized among all layers in the block while concat feature, the weight matrix of corresponding layers also are concat 2.2. Extra Techniques2.2.1. Attention Transition only add to transition layer 2.2.2. Bottleneck and Compression introduce bottleneck to block introduce compression to feature of loss function before global pooling 2.3. Implementation 3. Experiments 3.1. Details Dropout 0.2 after each Conv 3.2. Comparison 3.3. Stage I vs Stage II]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Architecture</category>
        <category>Network</category>
      </categories>
      <tags>
        <tag>Network</tag>
        <tag>CliqueNet</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(ICCV 2017) Learning feature pyramids for human pose estimation]]></title>
    <url>%2F2019%2F01%2F12%2FLearning%20feature%20pyramids%20for%20human%20pose%20estimation%2F</url>
    <content type="text"><![CDATA[Yang W, Li S, Ouyang W, et al. Learning feature pyramids for human pose estimation[C]//The IEEE International Conference on Computer Vision (ICCV). 2017, 2. 1. Overview 1.1. Motivation pyramid methods widely used at inference time learning feature in DCNN still not well explored existing weight initialization schemes (MSR, Xavier) are not proper for layers with branches In this paper, it proposed Pyramid Residual Module (PRMs) subsample ratios in a multi-branch network weight initialization schemes 1.2. Contribution PRM weight initialization scheme observe that the problem of activation variance accumulation introduced by identity mapping may be harmful in some scenario 1.3. Related Work1.3.1. Human Pose Estimation graph structures build on handcraft features. pictorial structure, loopy structure regression Gaussian peaks in score maps image pyramid. computation, memory 1.3.2. Multiple-layers of DCNN plain network. VGG, AlexNet multi-branch. inception, ResNet, ResNeXt 1.3.3. Weight Initialization layer-by-layer pretraining strategy Gaussian distribution. μ=0, σ=0.01 Xavier. sound estimation of the variance of weight assume that weights are initialized close to zero, hence the nonlinear activation (sigmoid, Tanh) can be regarded as linear function initialization scheme for rectifier networks All above are derived for plain networks with only one branch. 1.4. Dataset MPII LSP 2. Framework 2.1. Pyramid Residual Modules PRM-B has comparable performance with others feature with smaller resolution contain relatively fewer information, we use fewer feature channels for branches with smaller scales C. the number of pyramid levels f_c. transformation for c-th pyramid level, design as bottleneck 2.2. Fractional Max-Pooling pooling reduces the resolution too fast fractional max-pooling to approximate the smoothing and subsampling process s_c. subsampling ratio ∈ [2^{-M}, 1] set M = 1, C = 4 3. Training and Inference BN is less effective because of the small minibacth due to the large memory consumption of networks. 3.1. Loss Function for k-th body joint z_k=(x_k, y_k), ground-truth score map S_k is generated from a Gaussian with mean z_k and variance Σ 3.2. Forward Propagation(Assumption μ=0)To make the variances of the output y_l approximately the same for different layers, the condition must be satisfied in initialization, a proper variance for W_l should be α. depends on activation function, 0.5 for ReLU, 1 for Tanh and Sigmoid C_i. the number of input in l-th layers n_i. the number of elements in x_c, c=1,…, C_i 3.3. Backward Propagation C_o. the number of output Special Case. C_o=C_i = 1 for plain network. 3.4. Output AccumulationDrawbacks. identity mapping keeps increasing the variances of responses when the network goes deeper, which increase the difficulty of optimization. the identity mapping will be replaced by Conv to reduce or increase the channels, which can reset the variance to small value 1x1 Conv-BN-ReLU to replace identity mapping, which can stops the variance explosion find that breaking the variance explosion can provide a better performance 4. Experiments 4.1. Details 256x256 cropped scaling, rotation, flipping and adding color noise mini-batch 16 test. six-scale pyramid with flipping 4.2. Comparison 4.3. Ablation Study4.3.1. Variant 4.3.2. Scale of Pyramid and Weight initialization 4.3.3. Controlling Variance Explosion [w]88.5 vs [w/o]88.0 vs [baseline] 87.6]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Body</category>
        <category>Pose</category>
      </categories>
      <tags>
        <tag>Pose</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(CVPR 2017) Spiking Deep Residual Network]]></title>
    <url>%2F2019%2F01%2F12%2FSpiking%20Deep%20Residual%20Network%2F</url>
    <content type="text"><![CDATA[Hu Y , Tang H , Wang Y , et al. Spiking Deep Residual Network[J]. 2018. 1. Overview 1.1. Motivation big challenge to train a very deep SNN In this paper, it proposed an efficient approach to build a spiking version of ResNet Spiking ResNet. converting a trained ResNet to a network of spiking neurons shortcut normalization mechanism layer-wise error compensation. reduce error caused by discretistion accumulating sampling error caused by discretisation remains a crucial problem for deep SNNs converted from ANNs 1.2. SNN solution to bridge the gap between performance and computational costs and theoretically can approximate any function as ANNs communicate each other via discrete events (spikes) instead of continuous-valued activations emulated by neuromorphic hardware (TrueNorth, SpiNNaker, Rolls). several orders of magnitude less energy consumption than by contemporary computing hardware fits to process input from AER-based (adress-event-representation) sensors that have low redundancy, low latency and high dynamic range , , such as DVC (dynamic vision sensor), auditory sensor (silicon cochlea) main challenge. find an efficient training algorithm that overcome the discontinuity of spiking and achieve comparable performance to ANN 1.3. Related Workthe indifferentiability of SNN, it is hard to direclty apply classic learning methods of ANN. directly learn from spikes with BP-like algorithm by approximating the threshold function with linearity STDP (spiking-timing-dependent plasticity), rectangular STDP, exponential STDP convert pre-trained CNN to SNN add noise during training can improve the tolerance of errors caused by approximation ReLU eliminates all negative activations and continuous value in ANNs equals the firing rate of neuron in SNN conver several successful techniques in ANNs to SNN version, such as sparse coding, dropout, stacked auto-encoder conver common operations in ANNs such as max-pooling, softmax, BN and Inception-Modules to spiking equivalents usually focus on shallow network 1.4. Residual residual networks are equivalent to RNNs and RNNs are biologically plausible models of vision cortex biological finding indicate that it takes much less time to alter the weights of synapse than creating a new synaptic connection with residual learning, we can re-train falsely learned knowledge by simply suppressing synaptic weights in stacked layers, without introducing new synaptic connection if we interpret the addition as integration of incoming information, then each residual block in pre-activation structure is actually integrating the output information from every residual block that is lower than itself, which is similar to the integration of high level knowledge and low level knowledge in biological system 2. Methods 2.1. Overview ReLU can approximate the firing rate of spiking IF neuron Batch Normalization replace x by the output from previous layer x_n = Wx_{n-1} + b. In spiking network, directly regulating the weight and bias of previous layer. Weight Normalization λ^l. max 99.9% activation at layer l instead of the actual max activation as using the actual max activation is more prone to be susceptible to outliers 2.2. Problem spiking residual network is a directed acyclic graph. In order to scale the activation appropriately, we must also take its input from the shortcut connection into consideration as the conversion introduces new synaptic weights in shortcuts sampling error caused by discretisation is the major factor for degradation 2.3. Shorcut Normalization max_1, max_2, max_3 denote max activations in ReLU1, ReLU2, ReLU3 (W_1, b_1), (W_2, b_2) in Conv1, Conv2 2.4. Compensation of Sampling Error shallow network. sampling error has no significant impact on performance deep network. actual max firing rate declines sampling error can be reduced by slightly enlarge the weights at each layer apply a compensation factor λ to each layer to reduce sampling errors by slightly enlarge the weights compensation factor λ is searched in a region of (1, τ_max). τ_max is the reciprocal firing max firing rate at last layer 3. Experiments 3.1. Comparison 3.2. Shortcut Normalization 3.3. ResNet vs CNN 3.4. Error Compensation]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Architecture</category>
        <category>Network</category>
      </categories>
      <tags>
        <tag>Network</tag>
        <tag>Spiking ResNet</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(CVPR 2017) Aggregated residual transformations for deep neural networks]]></title>
    <url>%2F2019%2F01%2F12%2FAggregated%20residual%20transformations%20for%20deep%20neural%20networks%2F</url>
    <content type="text"><![CDATA[Xie S, Girshick R, Dollár P, et al. Aggregated residual transformations for deep neural networks[C]//Computer Vision and Pattern Recognition (CVPR), 2017 IEEE Conference on. IEEE, 2017: 5987-5995. 1. Overview 1.1. Motivation transition from feature engineering to network engineering human effort has been shifted to designing better network architecture for learning representation important strategy of Inception model is split-transform-merge In this paper, it proposed ResNeXt Network aggregate a set of transformations with the same topology homogeneous multi-branch architecture increase cardinality (the size of the set of transformation) is more effective than going deeper or wider 1.2. Related Work1.2.1. Multi-branch Convolutional Network Inception. multi-branch ResNet. two-branch 1.2.2. Group Convolution channel-wise convolution 1.2.3. Compressing Convolutional Network decomposition. at spatial or channel 1.2.4. Ensembling 2. Method 2.1. Simple Neurons Inner Product. can be recast as a combination of splitting, transforming and aggregating. D. the number of channel x=[x_1, x_2, …, x_D] 2.2. Aggregated Transformations Network-in-Neuron. replace the elementary transformation (wx) with a more generic function All T_i have the same topology. T. arbitrary function; projects x into low-dimension embedding and then transforms it C. the size of the set of transformations 2.3. Relation to Grouped Convolutions 2.4. Depth ≥ 3 The block depth must ≥ 3. 2.5. Capacity Left. 25664 + 336464 + 64*256 ≈ 70k parameters and proportional FLOPs Right. C(256d + 33dd + d256) ≈ 70k, when C=32, d=4 3. Experiments 3.1. Cardinality vs Width with complexity preserved, increasing cardinality at the price of reducing width starts to show saturating accuracy more training data will enlarge the gap of validation error, shown in ImageNet-5K set the saturation is caused by the complexity of dataset, not the capacity of models 3.2. Cardinality vs Deeper/Wider incrase cardinality better 3.3. w/o Residual 3.4. Comparison 3.5. Detection]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Architecture</category>
        <category>Network</category>
      </categories>
      <tags>
        <tag>ResNeXt</tag>
        <tag>Network</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(NIPS 2018 Spotlight) Probabilistic U-Net for Segmentation of Ambiguous Images]]></title>
    <url>%2F2019%2F01%2F05%2FProbabilistic%20U-Net%20for%20Segmentation%20of%20Ambiguous%20Images%2F</url>
    <content type="text"><![CDATA[Kohl S, Romera-Paredes B, Meyer C, et al. A probabilistic u-net for segmentation of ambiguous images[C]//Advances in Neural Information Processing Systems. 2018: 6965-6975. 1. Overview 1.1. Motivation many real-world vision problems suffer from inherent ambiguities which are common in medical imaging applications a group of graders typically produces a set of diverse but plausible segmentation In this paper consider the task of learning a distribution over segmentations given an input propose generative segmentation modle (UNet+Conditional VAE) producing a unlimited number of plausible hypotheses experiments on LIDC-IDRI and Cityscape 1.2. Contribution the proposed framework provides consistent segmentation maps instead of pixel-wise probabilities induce arbitrarily complex output distributions including the occurrence of very rare modes sampling is computationally cheap allow quantitative performance evaluation 1.3. Dataset 1.3.1. LIDC-IDRI 4 annotation per input contain 1018 lung CT scans from 1010 lung patients with annotation from 4 experts (split to 772 patients train, 144 validation, 144 test) resample (0.5mm x 0.5mm) and cropped 2D images (180x180 pixels) centered at the lesion positions. get 8882 train, 1996 validation, 1992 test 1.3.2. Cityscapes total 19 different semantic classes create ambiguities by artifical random flips of five classes to newly introduced classes sidewalk to sidewalk2 with probability of 8/17 person to person2 with 7/17 car with 6/17 vegetation with 5/17 road with 4/17 500 test, split off 274 as validation from 2975 train 2. Methods 2.1. Prior Net &amp; UNet w. parameters of prior net m. the number of segmentations to predict of an image X Θ. parameters of UNet f_comb. 1x1 Conv with parameters psi during m times, z and feature of UNet can be reused, only f_comb needs to be re-evaluated 2.2. Posterior Net v. parameters of posterior net, that learn to recognize a segmentation variant, given the raw image X and gt segmentation Y) and to map this to a position μ with some uncertainty sigma in the latent space S. prediction 2.3. Metric Y, Y’. independent samples from gt distribution S, S’. independent samples from the predicted distribution 2.3.1. In LIDC m = 4. gt samples n samples from model 3. Experiments 3.1. Baseline 3.2. Comparison 3.3. Visualization]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Multiple Hypotheses</category>
      </categories>
      <tags>
        <tag>Lung</tag>
        <tag>Medical</tag>
        <tag>Segmentation</tag>
        <tag>Multiple Hypotheses</tag>
        <tag>UNet</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(MICCAI 2018) DeepASL:Kinetic Model Incorporated Loss for Denoising Arterial Spin Labeled MRI via Deep Residual Learning]]></title>
    <url>%2F2019%2F01%2F05%2FDeepASL%3A%20Kinetic%20Model%20Incorporated%20Loss%20for%20Denoising%20Arterial%20Spin%20Labeled%20MRI%20via%20Deep%20Residual%20Learning%2F</url>
    <content type="text"><![CDATA[Ulas C, Tetteh G, Kaczmarz S, et al. DeepASL: Kinetic Model Incorporated Loss for Denoising Arterial Spin Labeled MRI via Deep Residual Learning[C]//International Conference on Medical Image Computing and Computer-Assisted Intervention. Springer, Cham, 2018: 30-38. 1. Overview 1.1. MotivationArterial spin labeling (ASL) allows to quantify the cerebral blood flow (CBF) by magnetic labeling of the arterial blood water, but suffers from an inherently low-signal-to-noise ratio (SNR). In this paper FCN to learn residual from noisy perfusion-weighted image incorporate the CBF estimation model in the loss function during training 1.2. Model Input. 2D noisy gray image patches 8 Conv2D (48 channels, 3x3, following pReLU) + 1 Conv2D (no activation) training. 18000 patch pairs of 40x40, batch size 500, 200 epoch, LR 0.0001 1.3. CBF Estimation Model β. brain-blood partition coefficient T_{1b}. longitudinal relaxation time of blood α. labeling efficiency τ. label duration PLD. post-label delay SI_{PD}. proton density weighted image ΔM. perfusion-weighted images 1.4. Loss Function N. noisy f_t. reference CBF value for each voxel 1.5. Experiments]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Medical</category>
        <category>Cerebral</category>
      </categories>
      <tags>
        <tag>Medical</tag>
        <tag>Segmentation</tag>
        <tag>Cerebral Blood Flow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(2017) Accurate Lung Segmentation via Network-Wise Training of Convolutional Networks]]></title>
    <url>%2F2019%2F01%2F05%2FAccurate%20Lung%20Segmentation%20via%20Network-Wise%20Training%20of%20Convolutional%20Networks%2F</url>
    <content type="text"><![CDATA[Hwang S, Park S. Accurate Lung Segmentation via Network-Wise Training of Convolutional Networks[M]//Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support. Springer, Cham, 2017: 92-99. 1. Overview In this paper, it proposed network-wise training 3 Conv + 6 Resblock (15 layers) dilation = 3 for the end of two residual blocks conv-BN global stride = 4 fewer parameters 1.1. Details one fold (124 odd numbers) and one fold (123 even number) one fold train, another test, then calc avg resize to 256x256 1.2. Experiments]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Medical</category>
        <category>Lung</category>
      </categories>
      <tags>
        <tag>Lung</tag>
        <tag>Medical</tag>
        <tag>Segmentation</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(CVPR 2018) Squeeze-and-excitation networks]]></title>
    <url>%2F2019%2F01%2F05%2FSqueeze-and-excitation%20networks%2F</url>
    <content type="text"><![CDATA[Hu J, Shen L, Sun G. Squeeze-and-excitation networks[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2018: 7132-7141. 1. Overview In this paper, it propsed SE block (Squeeze-and-Excitation). adaptively recalibrates channel-wise feature responses by explicity modelling interdependencies between channels SE network 2. Methods 2.1. Squeeze: Global Information Embedding global pooling 2.2. Excitation: Adaptive Recalibration FC + ReLU FC + Sigmoid 2.3. Example 2.4. Implementation 3. Experiments 3.1. Ablation Study 3.2. Comparison]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Architecture</category>
        <category>Component</category>
      </categories>
      <tags>
        <tag>Segmentation</tag>
        <tag>Component</tag>
        <tag>SE Block</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(CVPR 2017) Dilated Residual Networks]]></title>
    <url>%2F2019%2F01%2F05%2FDilated%20Residual%20Networks%2F</url>
    <content type="text"><![CDATA[Yu F, Koltun V, Funkhouser T. Dilated residual networks[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2017: 472-480. 1. Overview 1.1. Motivation progressively reduce resolution is harmful for segmentation, even classification In this paper, it proposed DRN (Dilated Residual Network) increase the resolution of output feature maps without reducing the receptive filed of individual neurons removing gridding artifacts introduce by dilation 2. Dilated Residual Networks remove the stride-2 in layer-4 and layer-5 set dilation = 2 of block_i in layer-4, i≥2 set dilation = 2 of block_1 in layer-5 set dilation = 4 of block_i in layer-5, i≥2 3. Degridding removing max pooling adding layers removing residual connections 4. Experiments]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Architecture</category>
        <category>Network</category>
      </categories>
      <tags>
        <tag>Segmentation</tag>
        <tag>Network</tag>
        <tag>DRN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(CVPR 2018) Context encoding for semantic segmentation]]></title>
    <url>%2F2019%2F01%2F05%2FContext%20encoding%20for%20semantic%20segmentation%2F</url>
    <content type="text"><![CDATA[Zhang H, Dana K, Shi J, et al. Context encoding for semantic segmentation[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018: 7151-7160. 1. Overview 1.1. Motivation recent works exploit FCN + dilated/atrous Conv and utilize multi-scale features and refining boundaries dilated/atrous Conv isolates the pixels from the global scene context In this paper, in proposed Context Encoding Module explore global contextual information selectively highlight class-dependent featuremaps improve performance with only marginal extra computation cost 14 layers network extend the Encoding Layer to capture global feature statistics for understanding semantic context 1.2. QuestionIs capturing contextual information the same as increasing the receptive filed size? 1.3. Contribution Context Encoding Module incorporating Semantic Encoding Loss (SE-loss). SE-loss can regularize training, enforce network learning of semantic context EncNet. 3.5M parameters 2. Methods 2.1. Context Encoding encoder semantics. output of encoding layer apply aggregation instead of concatenation 2.1.1. Feature Attention FC + Sigmoid 2.1.2. SE-Loss unlike per-pixel loss, SE-loss consider big and small objects equally 2.2. Context Encoding Network pretrained Dilated Residual Network (dilated strategy at stage 3 and 4) 3. Experiments 3.1. Details output size 1/8 bilinear upsample 8 times of prediction random shuffle training samples, discard last mini-batch random flip + random scale (0.5~2) + random rotate (-10~10) + crop to fix size if padding needed Implement Synchronized Cross-GPU BN in Pytorch using NVIDIA CUDA &amp; NCCL toolkit K = 32 batchsize 16 3.2. Comparison 3.3. Ablation Study]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Architecture</category>
        <category>Component</category>
      </categories>
      <tags>
        <tag>Segmentation</tag>
        <tag>Context Encoding</tag>
        <tag>Componet</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(CVPR 2017) Deep TEN:Texture Encoding Network]]></title>
    <url>%2F2019%2F01%2F05%2FDeep%20TEN%3A%20Texture%20Encoding%20Network%2F</url>
    <content type="text"><![CDATA[Zhang H, Xue J, Dana K. Deep ten: Texture encoding network[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2017: 708-717. 1. Overview In this paper, it proposed Deep Texture Encoding Network (Deep-TEN) with novel Encoding Layer encoding layer. generalizes robust residual encoder 1.1. Contribution encoding layer. learnable residual encoding layer Deep-TEN. feature extraction, dictionary learning and encoding representation learn together 2. Methods 2.1. Residual Encoding Model X. visual descriptor C. learned codebook (learn K x D) r. residual vector E. output S. smoothing factor (learned, Concat K vectors and L2-norm. when K = 1, c = 0, it simplifies to sum pooling can deal with any input size 2.2. Deep-TEN Multi-size Training. 352x352, 320x320 pretrained ResNet50 K = 32 weights of C and s randomly initialized with uniform distribution ±(1/√k) 3. Experiments 3.1. Multi-size vs Single-size training 3.2. Comparison]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Architecture</category>
        <category>Component</category>
      </categories>
      <tags>
        <tag>Segmentation</tag>
        <tag>Componet</tag>
        <tag>DeepTEN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(MICCAI 2018) SLSDeep:Skin Lesion Segmentation Based on Dilated Residual and Pyramid Pooling Networks]]></title>
    <url>%2F2019%2F01%2F05%2FSLSDeep%3A%20Skin%20Lesion%20Segmentation%20Based%20on%20Dilated%20Residual%20and%20Pyramid%20Pooling%20Networks%2F</url>
    <content type="text"><![CDATA[Sarker M M K, Rashwan H A, Akram F, et al. Slsdeep: Skin lesion segmentation based on dilated residual and pyramid pooling networks[C]//International Conference on Medical Image Computing and Computer-Assisted Intervention. Springer, Cham, 2018: 21-29. 1. Overview In this paper, it proposed SLSDeep encoder. dilated residual layer (DRN) decoder. pyramid pooling Negative Log Likelihood (NLL) + End Point Error (EPE) 2. Methods layer1~layer4. four pretrained DRN 2.1. Loss Function α=0.5 NLL EPE u0, u1. first derivatives of u in x and y direction 3. Experiments 3.1. Dataset ISBI 2016. 900 train, 379 test; size range [542x718~2848x4288] ISBI 2017. 2000 train, 150 valid, 600 test 3.2. Metrics Specificity Sensitivity Jaccard Index Dice coefficient Accuracy 3.3. Details LR. decoder 0.01, encoder 0.001 poly learning rate policy. 0.9 batchsize. 16 epoch. 100 TITANX 12G, 20 hours 3.3.1. Data Augmentation random scale. 0.5~1.5 random rotation. -10~10 resize to 384x384 for training 3.4. Comparison]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Medical</category>
        <category>Skin Lesion</category>
      </categories>
      <tags>
        <tag>Medical</tag>
        <tag>Segmentation</tag>
        <tag>Skin Lesion</tag>
        <tag>SLSDeep</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(NIPS 2018 Oral) Generalisation of structural knowledge in the Hippocampal-Entorhinal system]]></title>
    <url>%2F2019%2F01%2F04%2FGeneralisation%20of%20structural%20knowledge%20in%20the%20Hippocampal-Entorhinal%20system%2F</url>
    <content type="text"><![CDATA[Whittington J, Muller T, Mark S, et al. Generalisation of structural knowledge in the hippocampal-entorhinal system[C]//Advances in Neural Information Processing Systems. 2018: 8484-8495. 1. Overview 1.1. Motivation a central problem to understanding intelligence is the concept of generalisation hippocampal-entorhinal system is known to be important for generalisation In this paper, it proposes that to generalise structural knowledge, the representations of the structure of the world (how entities in the word relate to each other) need to be separated from representations of the entities themselves ANN embedded with hierarchy and fast Hebbian memory representations can effectively utilise memories shows a preserved relationship between entorhinal gird and hippocampal place cells across environments explicitly represented structure can be combined with sensory information in a conjunctive code unique to each environment. Thus sensory observations are fit with prior learned structural knowledge, leading to generalisation 1.2. Contribution1.2.1. Neuroscience find an interpretation of grid cells, place cells and remapping that offers a mechanistic understanding for the hippocampal involvement in generalisation of knowledge across domains results suggest spatial representations found in the brain may be an instance of a more general coding mechanism organising knowledge across multiple domains 1.2.2. Machine Learning build a network where fast Hebbian learning interacts with slow statistical learning this allow to learn representations whereby memories are not only stored in a Hebbian network for one-shot retrieval within domain, but also benefit from statistical knowledge that is shared across domains - allowing zero shot inference 1.3. Generally implement its proposal in an ANN tasked with predicting sensory observations when walking on 2D graph worlds, where each vertex is associated with a sensory experience To make accurate predictions, the agent should learn the underlying hidden strucutre of graphs unsupervised learning. providing the network with only sensory observations and actions place cells form a conjunctive representation between sensory identity and strucuture. This conjunctive representation forms a Hebbian memory, which bridges structure and identity, allowing the same structural code to be reused across environments combine fast Hebbian learning of episodic memories, with gradient descent which slowly learns to extract statistics of these memories 1.4. Details propose that the statistics of memories in hippocampus are extracted by cortex propose that future hippocampal representations/memories are constrained to be consistent with the learned structural knowledge choose memory storage and addressing to be computationally biologically plausible (rather than using other types of differentiable memory more akin to RAM) , as well as using hierarchical processing. This enables our model to discover representations that are useful for both navigation and addressing memories 1.5. In Neuroscience generalisation of statistical structure (the relationships between objects in the world) imbues an agent with the ability to fit things/concepts together that share the same statistical structure, but differ in the particularities hippocampus is known to be important for generalisation, memory, problems of causality, inferential reasoning, transitive reasoning, conceptual knowledge representation, one-shot imagination and navigation in spatial navigation there is a good undertanding of neuronal representations in both hippocampus (place cell, landmark cells) and medial entorhinal cortex (grid cell, border cell, object vector cell) place cells and grid cells have had a radical impact in neuroscience, leading to the 2014 Noble Prize in Physiology and Medicine place and grid cells are similar in that they have a stable firing pattern for specific regions of space place cells only fire in a single (or couple) localtion in a given environment whereas grid cells fire in a regular lattice pattern tiling the space. These cells cemented the idea of a ‘cognitive map‘, where an animal holds an internal representation of the space it navigates other entorhinal cell types (border, object vector cells) appear to have disparate roles in coding space remapping (traditionally thought to be random) → the space cell code is different for two structurally identical environments 2. Model consider an agent passively moving on a 2D graph, observing a non-unique sensory stimulus (an image) on each vertex if the agent wishes to undertand its environment then it should maximise its model’s probability of observing each stimulus trained on many environments sharing the same strucure (2D graph) one approach to this problem: have an abstract representation of space encoding relative locations, and then place a memory of what stimulus was observed at that (relative) location since the agent understands where it is in space, this allows for accurate state predictions to previously visited nodes even if the agent has never travelled along that particular edge before (Figure 2c) grid cell as base for constructing abstract representation of space place cell representations for the formation of fast episodic memories posit that this (place cells forms a conjunction) is done hierarchically across spatial frequencies, such that the higher frequency statistics can be repeatedly used across space.This reduces the number of weights that need to be learnt grid cells to be recurrent through time view the hippocampal-entorhinal system as one that performs inference 2.1. Model Summary the model is a neural network and learns strucuture across tasks optimise end-to-end via backpropagation through time the central (attractor) network employs Hebbian learning to rapidly remember the conjunction a generavie temporal model learns how to use the Hebbian memory most efficiently given the common statistics of transitions across worlds 2.2. Notation a layer of activations with vector notations Element s. index for sensory j. index for phases 2.3. Generative Model g. grid cell p. place cell M. agent’s memory a. action Θ. parameters of generative model x. one-hot vector where each of its n_s elements represent a sensory identity g&amp;p (learned instead of hard-coded). come in different frequencies (hierarchies) indexed by superscript f 2.3.1. Grid Cells to predict where we will be, we can transition from our current location based on our heading (path integration, Fig 2c) f. functions specific to the distribution in question connections in D_a are from low frequency to the same or higher frequency only (or alternatively only within frequency). separate into hierarchical scales so that high frequency statistics can be reused across lower frequency statistics, i.e. learning and knowledge is reused across space 2.3.2. Place Cells for retrieving memories stored memories are extracted via an attractor network (Fig 2b) using as input - i.e. grid cells act as an index for memory extraction 2.3.3. Data categorical distribution sum over phases f_c*. MLP choose f* to be 0 (only include highest frequency) 2.4. Inference Network posterior is intractable so approximated to phi. parameters of the inference network learn Θ and phi by maximising the ELBO with the VAE fra 2.4.1. Place Cells 2.4.2. Grid Cells 2.5. Hebbian Memories when enter a new environment, memory is reset to be empty (zeros) memories of place cell representations are stored in Hebbian weights between place cells (M_t) allow rapid learning when entering a new environment p^. place cells generated from inferred grid cells λ&amp;η. the rate of forgetting and remembering connections from high to low frequencies are set to zero, so that memories are retrieved hierarchically best results. when two separate matrices were used x(^). retrieved memory with the sensorium as input to the attra 2.5.1. Retrieval attractor network τ. iteration of the network α. decay term h_0. input, from grid cells or sensorium (depending on for generative or inference), dimensions scaled appropriately output. retrieved memory (place cell code) 2.6. Model Implication believe that using more biologically realistic computational mechanisms (Hebbian Memory instead of LSTM) will facilitate further incorporation of neuroscience-inspired phenomena, such as successor representations or replay 2.7. Details although presented a Bayesian formulation, best resuts were obtained by only using the means of the above distributions first item. cross entropy loss other item. squared error loss between inferred and generated variables 5 different frequencies. n_f as [10, 10, 8, 6, 6] environment square. [8, 10, 12] agent changes to a new environment after 2000~5000 steps a_t. up, down, left, right, stay still time truncated to 25 steps two separate memory matrices. use additional memory module in grid cell inference Typically after 200-300 environments, the agent has fully learned the strucuture (~50000 gradient updates) remove a_t from the generative model so that the generative model can more easily capture the true underlying transition statistics place-like representations are learned in the attractor grid-like representations are learned in the generative temporal model 2.8. graphic For the gird cells of a certain frequency (Inference) (1, 30) downsample to (1, 10) combined with (1, 10) sensory cell to get (1, 100) cells (Constrain) connections from high to low frequency are set to 0 3. Experiments For Fig 4b middle and right grid representations that are shifted versions of each other, as in the brain the separation into different phases (same frequency) means that two conjunctive place cells that respond to the same stimulus, will not necessarily be active simultaneously - each cell will only be active when their corresponding grid phase is active thus one can uniquely code for the same stimulus in many different locations Across two environments, a given stimulus may occur at the same grid phase but at a different location]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Neuron</category>
      </categories>
      <tags>
        <tag>Cortex</tag>
        <tag>Hippocampal</tag>
        <tag>Hebbian Memory</tag>
        <tag>Hippocampal-Wntorhinal System</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(NIPS 2018 Oral) Dendritic cortical microcircuits approximate the backpropagation algorithm]]></title>
    <url>%2F2019%2F01%2F04%2FDendritic%20cortical%20microcircuits%20approximate%20the%20backpropagation%20algorithm%2F</url>
    <content type="text"><![CDATA[Sacramento J, Costa R P, Bengio Y, et al. Dendritic cortical microcircuits approximate the backpropagation algorithm[J]. neural information processing systems, 2018. 1. Overview In this paper, it proposed dendritic cortical microcircuits to approximate the backpropagation algorithm introduce a multilayer neuronal network model with simplified dendritic compartments in which error-driven synaptic plasticty adapts the network towards a global desired output errors originate at apical dendrites and occur due to a mismatch between predictive input from lateral interneurons and activity from actual top-down feedback demonstrates the learning capabilities in regression and classification task show analytically that it approximates the error BP algorithm 1.1. In Neuroscience growing evidence demonstrates that deep neural networks outperform alternative frameworks in accurately reproducing activity patterns observed in the cortex although recent developments have started to bridge the gap between neuroscience and AI, how the brain could implement a BP-like algorithm remains an open question understanding how the brain learns to associate different areas to successfully drive behavior is of fundamental importance. However, how to correctly modify synapses to achieve this has puzzled neuroscientists for decades (synaptic credit problem) 1.2. Structure of Pyramidal Cells neocortical pyramidal cells can be defined as somatic, basal and apical integration zones 1.3. Future Work this paper only focus on a specific interneurons type (SST) as a feedback-specific interneurons. There are many other type interneurons, such as PV (parvalbumin-positive) mediate a somatic excitation-inhibition balance and competition consider multiple subsystems (neocortex and hippocampus) that transfer knowledge to each other and learn at different rates 2. Algorithm 2.1. General Methods interpret a brain area as being equivalent to a layer propose that prediction errors that drive learning in BP are encoded at distal dendrites of pyramidal neurons, which receive top-down input from downstream brain areas errors arise from the inability via lateral input from local interneurons (somatostatin-expressing; SST) to exactly match the top-down feedback from downstream cortical areas cross-layer feedback onto SST cells originating at the next upper layer provide a weak nudging signal, as a conductance-based somatic input current model weak top-down nudging on a one-to-one basis: each interneuron is nudged towards the potential of a corresponding upper-layer pyramidal cell recude pyramidal output neurons to two-compartment cell feedforward. ignore the lateral and top-down weights from the model 2.2. Interneurons receive input from lateral pyramidal cells onto their own basal dendrites integrate this input on their soma project back to the apical compartments of same-layer pyramidal cells predominantly driven by pyramidal cells within the same layer 2.3. Somatic Membrane Potentials Evolve in Time k. layer index g. fixed conductance sigma. controls the amount of injected noise lk. leak ξ. background activity is modelled as Gaussian White Noise 2.4. Dendritic Compartmental Potentials phi. neuronal transfer function B. basal dendrite A. apical dendrite 2.5. Teaching Current an interneuron at layer k permanently receives balanced somatic teaching excitatory and inhibitory input from a pyramidal neuron at layer k+1 on a one-to-one basis (u_{k+1}^P as target) the interneuron is nudged to follow the corresponding next layer pyramidal neuron 2.6. Synaptic Learning Rules2.6.1. In Previous Works w. individual synaptic weight η. learning rate u. somatic potential v. postsynaptic dendritic potential phi. rate function r. presynaptic input 2.6.2. In this Paper v_rest = 0. resting potential ^. take into acount dendritic attenuation factors of different compartment 2.7. Self-predicting no external target is provided to output layer neurons the lateral input from interneurons cancels the internally generated top-down feedback and renders apical dendrites silent this paper demonstrate that synaptic plasticity in self-predicting nets approximates the weight changes prescribed by BP only W^IP and W^PI are changed at that stage the interneurons will learn to mimic the layer-(k+1) pyramidal neurons Once learning of the lateral interneurons has converged, the apical input cancellation occurs irrespective of the actual bottom-up sensory input help speed-up learning, but is not essential 3. Experiments 3.1. Self-predicting &amp; Training A. top-down weight = - i-to-p weight C. aligned 45° 3.2. Regression Task Network. 30-50-10 (with 10 hidden layer interneurons) learn to approximate a random nonlinear function implemented by a heldaside feedforward network of 30-20-10 W^PP, W^PI, W^IP all are random initialized with a uniform distribution top-down weight matrix is kept fixed (top-down and interneurons-to-pyramidal synapses need not to be changed) relax the bottom-up vs top-down weight symmetry 3.3. Classification Network. 784-500-500-10, initialized to a random but self-predicting state top-down and i-to-p weights were kept fixed epical compartment voltages remained approximately silent when output nudging was turned off, reflecting the maintenance of a self-predicting state throughout learning both pyramidal and interneurons are first initialized to their bottom-up prediction state output layer neurons are nudged to their desired target, yielding updated somatic potentials k=N-1 to 1]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Neuron</category>
      </categories>
      <tags>
        <tag>Classification</tag>
        <tag>Back Propagation</tag>
        <tag>Dendrite</tag>
        <tag>Cortex</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(CVPR 2018 spotlight) Iterative Visual Reasoning Beyond Convolutions]]></title>
    <url>%2F2018%2F12%2F27%2FIterative%20Visual%20Reasoning%20Beyond%20Convolutions%2F</url>
    <content type="text"><![CDATA[Chen X, Li L J, Fei-Fei L, et al. Iterative visual reasoning beyond convolutions[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018: 7239-7248. 1. Overview 1.1. Motivation current recognition systems lack the capability to reason beyond stack of Conv with large receptive fields reasoning via top-down modules (UNet) or explicit memories local pixel-level reasoning lacks a global reasoning power assume enough examples of relationships in training data but not. (relationships grow exponentially and most reasoning requires learning from few or no example) a good image understanding is usually a compromise between background knowledge learned a prior and image-specific observations In this paper, it proposed a novel framework for iterative visual reasoning (incorporate both spatial and semantic reasoning) local module. use parallel updated spatial memory (pixel-level reasoning) global graph-reasoning module knowledge graph. node→class; edge→ different types of semantic relationships region graph. node→region; edge→spatial relationships assignment graph. assign regions to classes roll-out iteratively and cross-feed predictions combine the prediction with attention mechanism Dataset. ADE, Visual Genome (VG) and COCO 1.2. Related Works Visual Knowledge Base. accumulate structured knowledge automatically from the web Context Modeling Relational Reasoning. symbolic approaches apply neural networks to the graph structured data regularize the output of networks with relationships 2. Methods 2.1. Local Module S. 1 x 512 x h x w f. logits before sotfmax input feature. mid-level feature (Layer_3) + high-level feature (f) (s_r) feature of each region is crop and resize to 7x7 memory of GRU parallel update. a matrix to keep track of how much a region has to a memory cell memory S contains two-dimensional image structure and the location information 2.2. Global Graph Reasoning Module spatial path + semantic path input feature. mid-level feature (Layer_4, after avg) + high-level feature (f) 2.2.1. Region-Region relationship of edge. left/right and top/bottom (pixel-level distance and normalize to [0,1]) △=50 bandwidth closer regions are more correlated 2.2.2. Region-Class propagate beliefs from region to class backward from class to region rather than only linking to the most confident class, it chooses full softmax score p 2.2.3. Class-Class commonsense knowledge. is-kind-of, is-part-of other relationships. actions, prepositions The end-goal is to recognize regions better, all the class nodes should only be used as intermediate “hops” for better region representations. Use three stacks of below operations with residual connections 2.2.4. Spatial Path M_r (R, D). nodes of region A_e (R, R). adjacency matrix of edge type e W_e. weight 2.2.5. Semantic Path map regions to classes combine intermediate features AM_rW with class features M_c aggregate features from multiple types of edges between classes 2.2.6. Merge first to propagates semantic information back to regions 2.3. Iterative Reasoning &amp; Cross-feed both the local and global features are concated together to update the memories S_{i+1} and M_{i+1} using GRU 2.4. Attention N = 2I + 1; I is iteration times 2.5. Loss Function plain ConvNet loss L_0 local module loss L^l global module loss L^g final prediction loss with attention L_f 2.6. Re-weight for Hard Sample 3. Experiments 3.1. Dataset ADE ( parts annotations) Visual Genome (relationship annotation) COCO 3.2. Details use provided ground-truth location evaluation. classification accuracy (AC) and average precision (**AP); per-class and per-instance** word vectors of fastText algorithm roll-out the reasoning modules 3 times (more iterations do not offer more help) 3.3. Main Results deeper network and larger image size can only help ~1%, less than ensembles proposed models achieve higher per-class metric gains than per-instance ones, indicating that rare classes get helped more 3.4. Ablation In the local module, spatial memory S is critical In the global module, removing reasoning module R steeply drops performance, whereas further removing memory M dose not hurt much 3.5. Visualization]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Graph Network</category>
      </categories>
      <tags>
        <tag>Classification</tag>
        <tag>Graph Network</tag>
        <tag>Reasoning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(CVPR 2018) GLoMo:Unsupervisedly Learned Relational Graphs as Transferable Representations]]></title>
    <url>%2F2018%2F12%2F27%2FGLoMo%3A%20Unsupervisedly%20Learned%20Relational%20Graphs%20as%20Transferable%20Representations%2F</url>
    <content type="text"><![CDATA[Yang Z, Dhingra B, He K, et al. Glomo: Unsupervisedly learned relational graphs as transferable representations[J]. arXiv preprint arXiv:1806.05662, 2018. 1. Overview 1.1. Motivation most existing methods mainly focus on learning generic feature vector, ignore more structured graphical representation In this paper, it proposed GLOMO (graph from low-level unit modeling) learning generic latent relational graphs that capture dependencies between pairs of data units experiments on question answer, natural language inference, semantic analysis, and image classification 2. Methods Training graph predictor g. produce set of graphs G = g(x) graphs G. LxTxT L. number of layers Testing G = g(x’) G fed as input to the downstream network to augment training, multiply G with task-specific features 2.1. Graph Predictor key CNN query CNN 2.2. Input of Feature Predictor features affinity maxtrix. G_l v. compositional function such as GRU 2.3. Object Function context prediction 2.4. Desiderata separate networks g and f employ a dquared ReLU to enforce sparse connections in the graphs hierarchical graph representation 2.5. Latent Graph Transfer product of all affinity matrices from the first layers to the l-th layer take a mixture of all the graphs 2.6. Implementation H. specific task feature M. mixture graph []. concat also adopt multi-head attention 3. Experiments]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Learning</category>
        <category>Transfer Learning</category>
      </categories>
      <tags>
        <tag>Transfer Learning</tag>
        <tag>Graph</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(CVPR 2017) The more you know:Using knowledge graphs for image classification]]></title>
    <url>%2F2018%2F12%2F27%2FThe%20more%20you%20know%3A%20Using%20knowledge%20graphs%20for%20image%20classification%2F</url>
    <content type="text"><![CDATA[Marino K, Salakhutdinov R, Gupta A. The more you know: Using knowledge graphs for image classification[J]. arXiv preprint arXiv:1612.04844, 2016. 1. Overview 1.1. Motivation human has ability to acquire knowledge about the world and use knowledge to reason about the visual world Gated Graph Neural Network (GGNN) suffers computational issues when graph is large In this paper proposed Graph Search Neural Network (GSNN) mitigates the computational issues using knowledge improves performance on image classification 1.2. Contribution GSNN. incorporate potentially large knowledge graph into end-to-end system 2. Methods 2.1. GGNN input. graph of N nodes output. every graph node or global output h_{v}^{t}. hidden state for node v at time step t x_v. problem specific annotation A_v. adjacency matrix of graph for node v W, U. learned parameters (1). Initialization (2). propagation updates from adjacent nodes (3-6). combine the information from adjacent nodes and current hidden state of the nodes to compute the next hidden state output network (g). FC network 2.2. GSNN propagation net importance net output net start with initial nodes in the graph based on the likelihood of the concept being present (faster R-CNN) add adjacent node in, learn a per-node score function to estimates the importance of these nodes importance network. assign target importance value to each node, correspond to gt is 1, neighbours are γ, two-hop away γ^2 add top P nodes to expanded set at final step T, compute the per-node-output and re-order and zero-pad the outputs into the final classification net]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Graph Network</category>
      </categories>
      <tags>
        <tag>Graph Network</tag>
        <tag>Zero Shot Learning</tag>
        <tag>GSNN</tag>
        <tag>GGNN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(ICLR 2018) Relational inductive biases, deep learning, and graph networks]]></title>
    <url>%2F2018%2F12%2F27%2FRelational%20inductive%20biases%2C%20deep%20learning%2C%20and%20graph%20networks%2F</url>
    <content type="text"><![CDATA[Battaglia P W, Hamrick J B, Bapst V, et al. Relational inductive biases, deep learning, and graph networks[J]. arXiv preprint arXiv:1806.01261, 2018. 1. Overview In this paper argue that combinational generalization must be a top priority for AI advocate integrative approaches explore how using relational inductive biases within deep learning architecture present graph network. strong relational inductive bias; manipulate structured knowledge and produce structured behaviors discuss how graph networks can support relational reasoning and combinational generalization 1.1. Relational Inductive Biasesimpose constraints on relationships and interactions among entities in a learning process. 1.1.1. over Sets and Graphs Invariant to ordering 1.2. Graph Networks takes a graph as input perform computations over the structure return a graph as output 1.2.1. Strong Relational Inductive Biases express arbitrary relationships among entities invariant to permutation per-edge and per-node functions are reused across all edges and nodes 1.2.2. Definitions u. global attribute V. set of nodes E. set of edges v_i. the ith node e_k. the kth edge r_k. receiver of the kth edge s_k. sender of the kth edge 1.3. Design Principles flexible representation configurable within-block structure composable multi-block architecture 1.4. Discussion combinatorial generalization. shared computations across the entities and ralations to reason never-before-seen system question. where do get graphs come from that graph networks operate over?]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Graph Network</category>
      </categories>
      <tags>
        <tag>Overview</tag>
        <tag>Graph Network</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(CVPR 2018) Zero-shot Recognition via Semantic Embeddings and Knowledge Graphs]]></title>
    <url>%2F2018%2F12%2F27%2FZero-shot%20Recognition%20via%20Semantic%20Embeddings%20and%20Knowledge%20Graphs%2F</url>
    <content type="text"><![CDATA[Wang X, Ye Y, Gupta A. Zero-shot recognition via semantic embeddings and knowledge graphs[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018: 6857-6866. 1. Overview 1.1. Motivationtwo paradigms of transferring knowledge use implicit knowledge representation (semantic embedding) use explicit knowledge bases or knowledge graph In this paper based on Graph Convolutional Network (GCN) predict visual classifier for each category use both (imexplicit) semantic embeddings and the (explicit) categorical relationships to predict the classifier 1.2. Related Works Zero-Shot Learning attribute semantic embeddings knowledge graph 2. Methods 2.1. GCN A [n x n]. normalized, binary adjacency matrix of graph X [n x k]. feature matrix W [k x c]. weight matrix Z [n x c]. output n. the number of category; node of hte graph ReLU 2.1.1. Training Time use first m entities X = {x_1, x_2, …, x_n}, n entities embedding Y = {y_1, y_2, …, y_n} y_i ∈ {1, …, C} C. the number of labels 2.1.2. Testing Time use n-m entities 2.2. GCN for Zero-Shot Learning Input. set of category’s embedding vector Output. visual classifier for each input category (node) visual feature. extract by fixed pre-trained net, dimension D classifier. dimension D for each node 6-layer GCN A direct way. input x_i, output w_i based on m training pairs, but m is small. 2.2.1. Loss function ground-truth classifier weights learned from training images 2.3. Details LeakyReLU (0.2) leads to faster convergence L2-Normalized classifier is important find the last layer classifiers of the ImageNet pre-trained networks are naturally normalized 3. Experiments 3.1. Dataset relationships and graph (common sense knowledge rules) from Never-Ending Language Learning (NELL) images from Never-Ending Image Learning (NEIL) construct a new knowledge graph based on NELL and NEIL (1.7M object entities, 2.4M edges) use Breadth-first search (BFS), maximum length 7 hops 3.2. Ablation Study3.2.1. Baseline more performance gain as our graph size increases 3.2.2. Missing Edge knowledge graph chave redundant information with 14k nodes and 97k edges connecting them 3.2.3. Random Graph 3.2.4. Depth of GCN optimization becomes harder as the network goes deeper 3.2.5. Differences between Word Embeding and Classifier 3.2.6. Is Word Embedding Methods Crucial 3.3. Comparison]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Graph Network</category>
      </categories>
      <tags>
        <tag>Graph Network</tag>
        <tag>Zero Shot Learning</tag>
        <tag>Semantic Embedding</tag>
        <tag>Knowledge Graph</tag>
        <tag>Transfer Knowledge</tag>
        <tag>GCN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(ICLR 2018) Defense-GAN:Protecting classifiers against adversarial attacks using generative models]]></title>
    <url>%2F2018%2F12%2F21%2FDefense-GAN%3A%20Protecting%20classifiers%20against%20adversarial%20attacks%20using%20generative%20models%2F</url>
    <content type="text"><![CDATA[Samangouei P, Kabkab M, Chellappa R. Defense-gan: Protecting classifiers against adversarial attacks using generative models[J]. arXiv preprint arXiv:1805.06605, 2018. 1. Overview In this paper, it proposed Defense-GAN methods train to model the distribution of unperturbed images does not assume knowledge of the process for generating the adversarial examples effective against both white-box and black-box attacks 1.1. Defense Type modify the training data: adversarial trainingmodify the training procedure of the classifier to reduce the magnitude of gradients: defensive distilationremove the adversarial noise 1.2. Attack Type FGSM RAND+FGSM C&amp;W Iterative FGSM Jacobian-based Saliency Map Attack (JSMA) Deepfool 1.3. Algorithm]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Adversarial Attack</category>
      </categories>
      <tags>
        <tag>Adversarial Attack</tag>
        <tag>Defence</tag>
        <tag>DefenseGAN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(ICLR 2018) Thermometer encoding:One hot way to resist adversarial examples]]></title>
    <url>%2F2018%2F12%2F21%2FThermometer%20encoding%3A%20One%20hot%20way%20to%20resist%20adversarial%20examples%2F</url>
    <content type="text"><![CDATA[Buckman J, Roy A, Raffel C, et al. Thermometer encoding: One hot way to resist adversarial examples[J]. 2018. 1. Overview In this paper proposed thermometer encoding to defense adversarial attacks demonstrate the robustness with experiments on the MNIST, CIFAR-10, CIFAR-100 and SVHN datasets. 1.1. One-Hot Encoding 1.2. Thermometer Encoding (B, 3, H, W)→ (B, 3*levels, H, W) (B, 3*levels, H, W)→ ResNet 1.3. Experiments]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Adversarial Attack</category>
      </categories>
      <tags>
        <tag>Adversarial Attack</tag>
        <tag>Defence</tag>
        <tag>Thermometer Encoding</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(ICLR 2018) PixelDefend:Leveraging Generative Models to Understand and Defend against Adversarial Examples]]></title>
    <url>%2F2018%2F12%2F21%2FPixelDefend%3A%20Leveraging%20Generative%20Models%20to%20Understand%20and%20Defend%20against%20Adversarial%20Examples%2F</url>
    <content type="text"><![CDATA[Song Y, Kim T, Nowozin S, et al. Pixeldefend: Leveraging generative models to understand and defend against adversarial examples[J]. arXiv preprint arXiv:1710.10766, 2017. 1. Overview 1.1. Motivation adversarial examples mainly lie in the low probability regions of the training distribution In this paper, it proposed PixelDefend methods using statistical hypothesis testing, find modern neural density models are good at detecting imperceptible perturbations 63% to 84% for Fashion MNIST 32% to 70% for CIFAR-10 1.2. Contribution show that generative models can be used for detecting adversarially perturbed images and observe that most adversarial exmaples lie in low probability regions introduce a novel family of defend methods. PixelDefend (one of this family) CIFAR-10 performance 1.3. Attack Methods Random Perturbation FGSM BIM (Basic Iterative Methods) DeepFool CW 1.4. Defense Methods1.4.1. Change Network &amp; training procedure Adversarial Training. FGSM adversarial examples (most commonly used) train with BIM has witness success in small datases, but has reported failure in larger ones Label Smoothing (defensive distillation). convert one-hot labels to soft targets correct class 1-ε; wrong class ε/(N-1) 1.4.2. Modify Adversarial Examples Feature Squeezing. reduces the color range from [0, 255] to a smaller value, then smooths the image with a median filter 1.5. Datasets Fashion MNIST CIFAR-10 1.6. Model ResNet VGG 1.7. Detecting Adversarial Examples bit per dimension the distribution of log-likelihood p-values (compute by PixelCNN) 1.8. PixelDefend trade-off. choose ε_defend overestimate ε_attack 1.9. Adaptive PixelDefend ε_defend = 0. input image probability is below a threshold value otherwise ε=manually chosen setting 1.10. Defensive Attack with BIM. unrooling the PixelCNN is too deep, lead to vanishing gradient. Moreover, time consuming to attack (10 hours to generate 100 attacking images with one TITAN Xp GPU) optimization problem was not amenable to gradient descent PixelCNN and Classifier are trained separately and have independent parameters 1.11. Experiments p-values after defend Comparision]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Adversarial Attack</category>
      </categories>
      <tags>
        <tag>Adversarial Attack</tag>
        <tag>Defence</tag>
        <tag>PixelDefend</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(2017) Obfuscated Gradients Give a False Sense of Security:Circumventing Defenses to Adversarial Examples]]></title>
    <url>%2F2018%2F12%2F21%2FObfuscated%20Gradients%20Give%20a%20False%20Sense%20of%20Security%3A%20Circumventing%20Defenses%20to%20Adversarial%20Examples%2F</url>
    <content type="text"><![CDATA[Athalye A, Carlini N, Wagner D. Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples[J]. arXiv preprint arXiv:1802.00420, 2018. 1. Overview In this paper identify three types of obfuscated gradient (shattered gradient, stochastic gradient, vanishing/exploding gradients) propose Backward Pass Differentiable Approximation (BPDA) to overcome obfuscated gradient 1.1. Dataset MNIST&amp;CIFAR-10. untargeted ImageNet. 1000 randomly selected, targeted attacker-targeted, defender-untargeted 1.2. Network MNIST. 5 Conv CIFAR-10. ResNet ImageNet. InceptionV3 1.3. Attacker white-box (but not test-time randomness) 1.4. Obfuscated Gradient Shattered Gradient. non-differentiable, nonexistent, incorrect Stochastic Gradient. randomized Exploding&amp;Vanishing Gradient. multiple iteration 2. Attack Methods 2.1. Shattered Gradient2.1.1. Simple preprocessor g() satisfy g(x)≈x. 2.1.2. BPDA find a differentiable approximation g() such that (f_i: non-differentiable layer) forward. through f_i(x) backward. replacing f_i(x) with g(x) 2.2. Stochastic Gradient apply Expectation over Transformation (EOT) 2.3. Exploding&amp;Vanishing Gradient2.3.1. Reparameterization For f(g(x)), g performs optimation loop make a change-of-variable find differentiable h 3. Experiments 3.1. Adversarial Training has been shown to be difficulty at ImageNet scale. Adversarial Machine Learning at Scale training exclusively on l∞ adversarial examples provides only limited robustness to adversarial examples under other distortion metrics. Attacking the madry de-fense model with L1-based adversarial examples 3.2. Shattered Gradient thermometer encoding. BPDA-backward cropping&amp;rescaling. EOT bit-depth. BPDA-identity JPEG. BPDA-identity TVM. EOT+BPDA Quilting. EOT+BPDA 3.3. Stochastic Gradient SAP (random dropout at each layer). EOT 3.4. Vanishing&amp;Exploding Gradient PixelDefend Defense-GAN 3.5. Results]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Adversarial Attack</category>
      </categories>
      <tags>
        <tag>Adversarial Attack</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(2017) The robust manifold defense:Adversarial training using generative models]]></title>
    <url>%2F2018%2F12%2F21%2FThe%20robust%20manifold%20defense%3A%20Adversarial%20training%20using%20generative%20models%2F</url>
    <content type="text"><![CDATA[Ilyas A, Jalal A, Asteri E, et al. The robust manifold defense: Adversarial training using generative models[J]. arXiv preprint arXiv:1712.09196, 2017. 1. Overview 1.1. Motivation the natural image manifold is low-dimensional but the noisy is very high dimensional In this paper, it proposed a pre-processing step that projects on the range of a generative model using gradient descent (Invert and Classify, INC) robust against first-order, substitute model and combined adversarial attacks show that adversarial training on the generative manifold can make the classifier robust to these attacks INC + deep image prior 1.2. Contribution INC. robust against a wide variety of attacks. first-order, substitute models and enhanced attacks combining the two formulating min-max optimization problem DIP-INC. without pretrained 2. Methods 2.1. INC 2.2. DIP-INC The number of steps was empirically tuned in our experiments and depends on the power of the adversary 3. Experiments 3.1. INC-MNIST 3.2. INC-CelebA 3.3. DIP-INC ImageNet]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Adversarial Attack</category>
      </categories>
      <tags>
        <tag>Adversarial Attack</tag>
        <tag>Defense</tag>
        <tag>Deep Image Prior</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(CVPR 2018) Art of singular vectors and universal adversarial perturbations]]></title>
    <url>%2F2018%2F12%2F21%2FArt%20of%20singular%20vectors%20and%20universal%20adversarial%20perturbations%2F</url>
    <content type="text"><![CDATA[Khrulkov V, Oseledets I. Art of singular vectors and universal adversarial perturbations[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018: 8562-8570. 1. Overview In this paper, it proposed a algorithm for constructing universal perturbation compute the (p,q)-singular vectors of the Jacobian matrices of hidden layers based on 64 images, the perturbation with more than 60% fooling rate on 50000 images dataset investigate a correlation between maximum singular value and fooling rate 1.1. (p-q)-singular gector 1.2. Jacobian f_i. the i-th hidden layer q-norm (p,q)-singular vector of J_i(x) 1.3. Iterative Methods Instead of evaluating and storing the full matrix A, we use only the macvec function of A (given an input vector v, computes an ordinary product Av without forming the full matrix A, O(n) complexity) Power Methods algorithm to compute (p,q)-singular vectors 1.4. Generalized Power Method x. ε A. Jacobian matrix p,q. hyper-parameter when stacking J vertically for each x_j randomly choose a subset of images 1.5. Stochastic Power Methods 1.6. Efficient Implementation of the Matvec Function 1.7. Perturbation]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Adversarial Attack</category>
      </categories>
      <tags>
        <tag>Adversarial Attack</tag>
        <tag>Universal Adversarial Perturbations</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(CVPR 2018) Generative Adversarial Perturbations]]></title>
    <url>%2F2018%2F12%2F21%2FGenerative%20Adversarial%20Perturbations%2F</url>
    <content type="text"><![CDATA[Poursaeed O, Katsman I, Gao B, et al. Generative adversarial perturbations[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018: 4422-4431. 1. Overview In this paper, it proposed generative models for creating adversarial examples can produce image-agnostic and image-dependent perturbation for targeted and untargeted attacks demonstrate that similar architecture can achive impressive results in fooling both classification and semantic segmentation models faster than iterative methods at inference time 1.1. Type of perturbation Universal. fixed Image-dependent. vary for different images targeted untargeted 1.2. Contribution unifying framework. universal and image-dependent state-of-art performance in universal perturbations first to present effective targeted universal perturbation faster than iterative and optimization-based methods, the order of milliseconds 1.3. Related Work1.3.1. Universal Perturbations iterates over samples in a target set, aggregate image-dependent perturbation and normalize the results to build universal perturbation add image-dependent perturbations and clip the results 1.3.2. Image-dependent Perturbation optimization-based FGSM Iterative Least-Likely Class adversarial examples are sensitive to the angle and distance 2. Generative Adversarial Perturbation 2.1. Universal Perturbation U. scale to have a fixed norm trained with fooling loss. the combination of fooling and discriminatice loss lead to sub-optimal 2.2. Image-dependent Perturbation generate perturbation instead of adversarial example giving us better control over the perturbation magnitude 2.3. Fooling Multiple Network 3. Experiments L_{oo}. make use of the maximum permissible magnitude at each pixel]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Adversarial Attack</category>
      </categories>
      <tags>
        <tag>Adversarial Attack</tag>
        <tag>Universal Adversarial Perturbations</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(CVPR 2018) Defense against Universal Adversarial Perturbations]]></title>
    <url>%2F2018%2F12%2F21%2FDefense%20against%20Universal%20Adversarial%20Perturbations%2F</url>
    <content type="text"><![CDATA[Akhtar N, Liu J, Mian A. Defense against universal adversarial perturbations[C]Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018: 3389-3398. 1. Overview In this paper, it proposed the first framework to defend against universal adversarial perturbation learn a Perturbation Rectifying Network (PRN) as pre-input layers perturbation detector trained on the Discrete Cosine Transform (DCT) 1.1. Contribution PRN methods to compute synthetic image-agnostic perturbations to train PRN separate perturbation detector learned from DCT 1.2. Related Work FGSM DeepFool (iterative) adversarial transformation network ensemble foveation distillation JPG compression SafetyNet. detect and reject adversarial example 1.3. Problem Formulation I_c. clean image ρ. perturbation δ. fooling ratio (set 0.8) ξ. 2000 for l_2, 10 for l_oo detector rectifier I_{ρ/c}. perturbation image or clean image 2. Methods 2.1. PRN Θ. weights b. bias l*. predicted by joint network l. predicted by target network implemented as 5-ResNet blocks cross entropy loss Adam with momentum (0.9, 0.999) LR 0.01 decay by 10% eack 1K iteration batch size 64 2.2. Synthetic Perturbation Generation fooling ratios for the synthetic perturbation is lower than the original ones but in an acceptable range help in early convergence and better performance of the PRN 2.3. Perturbation Detection DCT based compression can also be exploited to reduce the network fooling ratio under the universal perturbation B. SVM F. compute log-absolute values of the 2D-DCT coefficients of the gray-scaled image if detect perturbation else 3. Experiments 3.1. Metric 3.2. Results]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Adversarial Attack</category>
      </categories>
      <tags>
        <tag>Adversarial Attack</tag>
        <tag>Defence</tag>
        <tag>Universal Adversarial Perturbations</tag>
        <tag>Adversarial Detection</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(NIPS 2016) Hierarchical Object Detection with Deep Reinforcement Learning]]></title>
    <url>%2F2018%2F09%2F13%2FHierarchical%20Object%20Detection%20with%20Deep%20Reinforcement%20Learning%2F</url>
    <content type="text"><![CDATA[Bellver M, Giroinieto X, Marques F, et al. Hierarchical Object Detection with Deep Reinforcement Learning[J]. Advances in Parallel Computing, 2016 1. Overview In this paper, it trained an intelligent agent to decide where to look proposal strategy. with and w/o overlap extract feature strategy. zoom and crop-pool overlap+zoom better cast the problem as a Markov Decision Process (MDP) 1.1. Related Work RL has been applied to Classification, Captioning, Activity Recognition Region proposal is expensive (R-CNN) AttentionNet. cast detection as iterative classification SSD Active Object Localization (agent). 2. Methods 2.1. MDP Formulation state. descriptor of current region + memory vector (last 4 actions, 6 * 4=24 dimension) action. 5 movement + terminal reward. τ=0.5 η=3 2.2. Q-learning 2.3. Proposal Strategy overlap. 0.75 2.4. Model 2.5. Exploration-Exploration ε-greedy policy. start with ε=1, decrease until ε=0.1 in step of 0.1 start with random actions, and at each epoch the agent takes decisions relying more on the already learnt policy to help the agent learn terminal action, we force it each time the current region has a IoU&gt;0.5 2.6. Training Parameters DQN Adam 1e-6 50 epoch discount factor. γ=0.9 2.7. Experiments Replay(s, a, r, s’) consecutive experiences in this paper is very correlated, lead to inefficient and unstable learning to solve it, we use random minibatches. 1000 experiences and batch size 100 3. Experiments 3.1. Region Proposal with less than 3 step, can almost approximate all objects we can detect 3.2. Zoom vs Crop-Pool Zoom better]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Reinforcement Learning</category>
      </categories>
      <tags>
        <tag>Detection</tag>
        <tag>DQN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(ICLR 2018) Mitigating adversarial effects through randomization]]></title>
    <url>%2F2018%2F08%2F16%2FMitigating%20adversarial%20effects%20through%20randomization%2F</url>
    <content type="text"><![CDATA[Xie C, Wang J, Zhang Z, et al. Mitigating adversarial effects through randomization[J]. arXiv preprint arXiv:1711.01991, 2017. 1. Overview In this paper, it exploited randomization at inference time to mitigate adversarial effects random resize random padding can defense both single-step and iterative attacks no additional training or fine-tuning very few additional computations compatible with other adversarial defense methods 1.1. Relative Work FGSM DeepFool Carlini&amp;Wagner (C&amp;W) Z(x)_i. the logits output for class i k. control the confidence gap between theadversarial class and true class 2. Methods resize. from 299x299 to MxM. M∈[299, 331) pad. to 331x331 total 12528 patterns 3. Experiments 3.1. Attack Scenarios vanilla attack. attacker do not know the existence of the randomization layers; target model is the original network single-pattern attack. attacker know the existence of the randomization layers; target model is the original network + randomization layers with only one predefined pattern ensemble-pattern attack. know; original network + randomization layers with an ensemble of predefined patterns 3.2. Accurate Drop 3.3. Vanilla Attack Scenario mitigate both single-step and iterativ 3.4. Single-Pattern Attack Scenario for single-step attacks, random layers are less effective on mitigating adversarial effects for a larger ε for iterative attacks, random layers perform well 3.5. Ensemble-Pattern Attack Scenario adversarial examples generated under ensemble-pattern attack scenario are much stronger 3.6. One Pixel Padding from 330x330 to 331x331 adversarial examples generated by single-step attacks have strong transferability, but still cannot attack the defense model adversarial examples generated by iterative attacks are much less transferable between different padding pattern 3.7. One Pixel Resizing from 330x330 to 331x331 resize the imagesby only 1 pixel can effectively destroy the transferability of adversarial examples by both single-step and iterative step]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Adversarial Attack</category>
      </categories>
      <tags>
        <tag>Adversarial Attack</tag>
        <tag>Defence</tag>
        <tag>Ensemble</tag>
        <tag>Randomization</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(CVPR 2018) Deflecting adversarial attacks with pixel deflection]]></title>
    <url>%2F2018%2F08%2F16%2FDeflecting%20adversarial%20attacks%20with%20pixel%20deflection%2F</url>
    <content type="text"><![CDATA[Prakash A, Moran N, Garber S, et al. Deflecting adversarial attacks with pixel deflection[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018: 8571-8580. 1. Overview 1.1. Motivation image classifier tend to be robust to natural noise adversarial attacks tend to be agnostic to object location most attacks search the entire image plane for adversarial perturbation without regard for the lacation of the image content In this paper, it proposed pixel deflection + wavelet to defense attack force the image to match natural image statistics use semantic maps to obtain a better pixel to update locally corrupts the image by redistributing pixel values via a process we term pixel deflection then waelet-based denoising operation to softens the corruption 1.2. Related Work1.2.1. Attack FGSM IGSM L-BFGS. minimize L2 distance between the image and adversarial example Jacobian-based Saliency Map Attack (JSMA). modify the pixels which are most salient, targeted attack Deep Fool. untargeted, approximate the classifier as a linear decision boundary then find the smallest perturbation needed to cross that boundary Carlini&amp;Wagner (C&amp;W). Z_k: the logits of a model for a given class k 1.2.2. Defense ensemble distillation transformation quilting + TVM foveation-based mechanism. crop the image around the object and then scale it back to the original size random crop + random pad 2. Pixel Deflection most deep classifiers are robust to the presence of natural noise, such as sensor noise 2.1. Algorithm random sample a pixel replace it with another randomly selected pixel from within a small square neighbourhood even changing as much as 1% of the original pixels does notalter the classification of a clean image 2.2. Distribution of Attacks 2.3. Targeted Pixel Deflection In natural image, many pixels do not correspond to a relevant semantic object and are therefore not salient to classification 2.3.1. Robust Activation Map an adversary which successfully changes the most likely class tends to leave the rest of the top-k classes unchanged 38% of the time the predicted class of adversarial images is the second highest class of the model for the clean ima 3. Wavelet Denoising 3.1. Hard Thresholding all coefficients with magnitude below the threshold are set to zero 3.2. Soft Thresholding 3.3. Adaptive Thresholding VisuShrink. N pixels and σ of noise BayesShrink. model the threshold for each wavelet coefficient as a Generalized Gaussian Distribution (GGD) 4. Methods corrupt the image with pixel deflection soften the impact of pixel deflection convert image to YCbCr which has denoising advantages to the wavelet project the image into the wavelet domain (use db1, but db2 annd haar have similar results) soft threshold the wavelets using BayesShrinks convert image back to RGB 5. Experiments]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Adversarial Attack</category>
      </categories>
      <tags>
        <tag>Adversarial Attack</tag>
        <tag>Defence</tag>
        <tag>PixelDeflection</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(2017) Adversarial Transformation Networks:Learning to Generate Adversarial Examples]]></title>
    <url>%2F2018%2F08%2F16%2FAdversarial%20Transformation%20Networks%3A%20Learning%20to%20Generate%20Adversarial%20Examples%2F</url>
    <content type="text"><![CDATA[Baluja S, Fischer I. Adversarial transformation networks: Learning to generate adversarial examples[J]. arXiv preprint arXiv:1703.09387, 2017. 1. Overview 1.1. Motivation existing methods either directly computing gradients of solving an optimization on the image pixels In this paper, it proposed Adversarial Transformation Network (ATN) in a self-supervised manner to generate adversarial examples fast to excute 2. Methods 2.1. ATN transform an input into an adversarial example against a target network or set of networks focus on targeted, white-box ATNs f. target network g. parameter vector 2.2. Training L_X. loss function in the input space or perceptual loss L_Y. specially-formed loss on the output space β. weight to balance 2.3. Inference even faster than the single-step gradient-based methods, so long as 2.4. Loss Functions r(*). reranking function 2.5. Reranking Function simplest way. set r(y, t) = onehot(t) α＞1. specify how much larger y_t should be than the current max classification]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Adversarial Attack</category>
      </categories>
      <tags>
        <tag>Adversarial Attack</tag>
        <tag>Defence</tag>
        <tag>Adversarial Transformation Network</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(CVPR 2018) Defense against Adversarial Attacks Using High-Level Representation Guided Denoiser]]></title>
    <url>%2F2018%2F08%2F16%2FDefense%20against%20Adversarial%20Attacks%20Using%20High-Level%20Representation%20Guided%20Denoiser%2F</url>
    <content type="text"><![CDATA[Liao F, Liang M, Dong Y, et al. Defense against adversarial attacks using high-level representation guided denoiser[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018: 1778-1787. 1. Overview 1.1. Motivation small residual perturbation is amplified to a large magnitudein top layers of the models In this paper, it proposed high-level representation guided denoiser (HGD) as a defense for image classification more robust to white-box and black-box trained on small subset of images and generalize well to other images and unseen classes transfer to defend models other than the one guiding it 1.2. Related Works1.2.1. Attack Methods box-constrained L-BFGS FGSM IFGSM 1.2.2. Defense Methods augmentation with perturbation data (time consuming). even improve accuracy of clean image on some datasets, but not found on ImageNet preprocessing denoising auto-encoder, median filter, averaging filter, Gaussian low-pass filter, JPEG compression two-step defense model. detect adversarial input, and then reform it based on the difference between the manifolds of clean and adversarial examples gradient masking effect deep contrastive network knowledge distillation saturating networks 2. Methods 2.1. Pixel Guided Denoiser (PGD) 2.2. High-level Representation Guided Denoiser (HGD) Feature Guided Denoiser (FGD). l=-2 layer, unsupervised logits guided denoiser (LGD). l=-1 layer, unsupervised class label guided denoiser (CGD). supervised 3. Experiments 3.1. PGD DAE performance significantly drops in clean images denoising loss and classification accuracy of PGD are not so consistent analyze the layer-wise perturbations of the target model activatedby PGD denoised images LGD perturbation at the final layer is much lower than PGD and adversarial perturbations and close to random perturbation 3.2. HGD HGD is more robust to white-box and black-box than PGD and ensV3 the difference between these HGD methods is insignificant learning to denoise only is much easier than learning the coupled task of classification and defense 3.3. HGD as an Anti-adversarial Transformer LGD does not suppress the total noise as PGD does, but adds more perturbations to the image *. adversarial perturbation ^. predicted perturbation the slope of PGD‘s line ＜ 1. PGD only removes a portion of the adversarial noises the slope of LGD’s line ＞ 1. the estimation is very noisy which leads to high pixel-level noise]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Adversarial Attack</category>
      </categories>
      <tags>
        <tag>Adversarial Attack</tag>
        <tag>Defence</tag>
        <tag>HGD</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(ICLR 2018) Countering Adversarial Images using Input Transformations]]></title>
    <url>%2F2018%2F08%2F16%2FCountering%20Adversarial%20Images%20using%20Input%20Transformations%2F</url>
    <content type="text"><![CDATA[Guo C, Rana M, Cisse M, et al. Countering adversarial images using input transformations[J]. arXiv preprint arXiv:1711.00117, 2017. 1. Overview In this paper, it investigates strategies that defend against adversarial-example attacks on image-classification system by transformation bit-depth reduction JPEG compression total variance minimization image quilting total variance minimization and image quilting are very effective defenses 1.1. Related Works1.1.1. Model-Specific Strategy enforce model properties. invariance and smoothness 1.1.2. Model-Agnostic remove adversarial perturbation from input JPEG compression image re-scaling DeepFool Carlini-Wagner’s L2 attack 1.2. Type of Attack Model A. trained on clean image Model B. trained on transformed image gray-box. adversarial(Model A) + transform + Model A black-box. adversarial(Model A) + transform + Model B 2. Defence 2.1. Image Cropping-Rescaling alter the spatial positioning of adversarial perturbation crop and rescale images at training time average prediction over random image crops 2.2. Bit-Depth Reduction simple quantization to remove small perturbation perform compression at quality level 75 2.3. Total Variance Minimization combine with pixel dropout select random set of pixel by Bernoulli random variable. maintain pixel use total variance minimization to construct image z 2.4. Image Quilting synthesize images by piecing together small patches that are taken from a database of image patches 3. Experiments 3.1. Setting pixel dropout rate = 0.5 λ_{TV} = 0.03 quilting patch ize 5x5 3.2. Gray-Box TV and quilting successful defend against adversarial examples from all four attacks quilting severely impact the model’s accuracy on non-adversarial images 3.2.1. Transformation at Training and Testing bit-depth reduction and JPEG compression are weak defenses 3.3. Black-Box 3.3.1. Ensemble and Model Transfer gain 1~2% in classification accuracy by ensemble 4. Discussion randomness is particularly crucial in developing strong defenses TV and quilting are stronger defenses than deterministic denoising procedures (bit-depth, JPEG, non-local mean) adversarial traning focuses on a particular attack, but transformation-based defenses generalize well]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Adversarial Attack</category>
      </categories>
      <tags>
        <tag>Adversarial Attack</tag>
        <tag>Defence</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(CVPR 2018) Boosting Adversarial Attacks with Momentum]]></title>
    <url>%2F2018%2F08%2F16%2FBoosting%20Adversarial%20Attacks%20with%20Momentum%2F</url>
    <content type="text"><![CDATA[Dong Y, Liao F, Pang T, et al. Boosting adversarial attacks with momentum[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018: 9185-9193. 1. Overview 1.1. Motivation most of existing adversarial attacks can only fool a black-box model with low success rate In this paper, it proposed a broad class of momentum-based iterative algorithm stabilize update diretions escape from poor local maximum more transferable adversarial examples alleviate the trade-off between the white-box attacks and the transferability apply momentum iterative algorithms to an ensemble of models (further improve the success rate for black-box attacks) 1.2. Generally Transferability. different machine learning models learn similar decision boundaries around a data point. one-step gradient-based methods more transferable 1.3. Contribution momentum iterative gradient-based methods study several ensemble approaches first to show models obtained by ensemble adversarial training with a powerful defense ability are also vulnerable to the black-box attacks 1.4. Related Works1.4.1. Attack Methods one-step iterative optimization-based methods. lack the efficacy in black-box attacks just like iterative methods 1.4.2. Defense Methods inject adversarial examples into training procedure ensemble adversarial training 2. Methods the assumption of linearity of the decision boundary around the data point may not hold when the distortion is large. FGSM. underfit iterative FGSM. overfit 2.1. MI-FGSM g_t. gather the gradient of the first t iterations with a decay factor μ μ=0. MI-FGSM→ iterative FGSM gradient normalized by L1 distance, the scale of the gradients in different iterations varies in magnitude 2.2. MI-FGSM for Ensemble Model ensemble in logits (input values to softmax). perform better ensemble in prediction probability ensemble in loss 2.3. Extension L2 distance targeted attacks 3. Experiments 3.1. Single Model maximum perturbation = 16 μ = 1 3.1.1. Decay Factor μ μ=1. adds up all previous gradients to perform the current update 3.1.2. The Number of Iteration when increasing the number of iterations, the success rate of I-FGSM against a black-box model gradually decreases 3.1.3. Update Direction update direction of MI-FGSM is more stable than I-FGSM (larger cosine similarity) stabilized updated directions make L2 norm of the perturbation lager, which helpful for transferability 3.1.4. The Size of Perturbation α = 1 3.2. Ensemble ensemble in logits perform better MI-FGSM transfer better]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Adversarial Attack</category>
      </categories>
      <tags>
        <tag>Adversarial Attack</tag>
        <tag>MI-FGSM</tag>
        <tag>Ensemble</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(ICCV 2017) Adversarial Examples for Semantic Segmentation and Object Detection]]></title>
    <url>%2F2018%2F08%2F16%2FAdversarial%20Examples%20for%20Semantic%20Segmentation%20and%20Object%20Detection%2F</url>
    <content type="text"><![CDATA[Xie C, Wang J, Zhang Z, et al. Adversarial examples for semantic segmentation and object detection[C]//Proceedings of the IEEE International Conference on Computer Vision. 2017: 1369-1378. 1. Overview In this paper, it proposed Dense Adversary Generation (DAG) optimize a loss function over a set of pixels/proposals for generating adversarial perturbations consider all targets simultaneously and optimizes the overall loss function 150~200 iteration change IOU to preserve more proposals. generating adversarial example is more difficult in detection then segmentation (O(N*N) vs O(N), N pixels) add two or more heterogeneous perturbations significantly increase the transferability 1.1. Related Work FGSM universal adversarial perturbation trained a network to generate adversarial examples for a target model ensemble-based approaches to generate adversarial examples with stronger transferability forveation-based mechanism to alleviate adversarial examples defensive distillation train network on adversarial examples ensemble adversarial training methods 2. Methods t_n. n-th target in the image X. image f_{l}. the score of l class l_n. the class of target t_n l_n’. the any class except the gt class l_n T. activate target set (predict right) normalize r γ = 0.5 r. sum of all r_m X^. the mean image of X. often X-X^ 3. Details target set selection is more difficult in detection task.when the adversarial perturbation r is added t the original image X, a different set of proposals may be generated according to the net input X+r. incrase the threshhold of NMS in RPN(IOU: 0.7 to 0.9 → 300 proposals to 3000 proposals) 4. Experiments 4.1. Comparison permuted perturbations cause negligible accuracy drop indicate that it is the spatial structure of r, instead of its magnitude, contributes in generating adversarial examples 4.2. Control Output 4.3. Denseness of Proposals 4.4. Convergence 4.5. Perceptibility K. number of pixels segmentation. 2.6e-3, 2.5e-3, 2.9e-3, 3e-3 on FCN-Alex, FCN-Alex, FCN-VGG, FCN-VGG detection. 2.4e-3, 2.7e-3, 1.5e-3, 1.7e-3 4.6. Transferability 4.6.1. Cross-Training Transfer perturbation learn from one network to another network with same architecture but different dataset 4.6.2. Cross-Network Transfer through different network structures 4.6.3. Cross-Task Transfer perturbation generated from one task to attack another task drop significantly between same network (FCN-VGG, FR-VGG-07) 4.6.4. Combining Heterogeneous Perturbations add multiple adversarial perturbation often works better than adding a single source of perturbation]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Adversarial Attack</category>
      </categories>
      <tags>
        <tag>Segmentation</tag>
        <tag>Adversarial Attack</tag>
        <tag>Detection</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(CVPR 2018) On the Robustness of Semantic Segmentation Models to Adversarial Attacks]]></title>
    <url>%2F2018%2F08%2F16%2FOn%20the%20Robustness%20of%20Semantic%20Segmentation%20Models%20to%20Adversarial%20Attacks%2F</url>
    <content type="text"><![CDATA[Arnab A, Miksik O, Torr P H S. On the robustness of semantic segmentation models to adversarial attacks[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018: 888-897. 1. Overview 1.1. Motivation adversarial examples has not been extensively studied on multiple, large-scale dataset and structureed prediction task In this paper evaluate adversarial attacks on model semantic segmentation models, using two large-scale datasets 1.2. Observation DeeplabV2 more robust than other approach multiscale networks more robust to multiple different attacks and white-box attacks adversarial examples are less effective when processed at different scales input transformation only more robust to attack which not take these transformation into account mean filed CRF Inference increases robustness to untargeted adversarial attacks 2. Adversarial Examples 2.1. Solution y_t. target label c. positive scalar L-BFGS. expensive, require several minutes to produce a single attack 2.2. FGSM single-step, untargeted attack minimise the l_oo norm of the perturbation bounded by the parameter ε 2.3. FGSM II single-step, choose the target class as the least likely class predicted by network 2.4. Iterative FGSM iterative manner, untargeted clip(a,ε). make sure each element a_i of a in range [a_i -ε, a_i+ε] α. step-size 2.5. Iterative FGSM II iterative manner, targeted y_ll. least likely class predicted by the network 3. Adversarial Defenses and Evaluation training with adversarial examples generated by single-step methods conferred robustness to other single-step attacks with negligible performance difference to normally trained networks on clean inputs however, the adversarially trained network was still as vulnerable to iterative attacks as standard models currently, no effective defense to all adversarial attacks exist state-of-art methods should be preferred in settings where both accuracy and robustness are a priority 4. Experiments Set-up number of iteration. min(ε+4, ceil(1.25ε)) step-size α=1 l_oo of ε. {0.25, 0.5, 1, 2, 4, 8, 16, 32} 5. Experiments 5.1. The Robustness of Different Networks resnet-based (skip-connection) more robust to single-step attack none perform well on iterative FGSM II attack, but tend not to transfer to other models, less useful in practical black-box attacks 5.2. Model Capacity and Residual Connections lightweight models are affected similarly as heavyweight models adding context module of Dilated-Net onto the Front-end slightly reducdce robustness across all ε values on Cityscapes 5.3. Multiscale Processing perturbations generated from the multiscale and 100% resolutions of Deeplabv2 transfer the best multiscale version Deeplabv2 is the most robust to white-box attacks FCN8 with multiscale inputs is more rbust to white-box attacks adversarial attacks generated at a single scale, are no longer as malignant when processed at another network process images at multiple scales more robust 5.4. Input Transformation JPEG recompression Gaussian Blur HSV Jitter Grayscale JPEG and Gaussian Blur providing substantial benefits 5.5. Expectation over Transformation (EOT) Attack T. the distribution of transformation functions t 5.6. Transferability of Input Transformation]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Adversarial Attack</category>
      </categories>
      <tags>
        <tag>Segmentation</tag>
        <tag>Adversarial Attack</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(CVPR 2018) Feature Generating Networks for Zero-Shot Learning]]></title>
    <url>%2F2018%2F06%2F22%2FFeature%20Generating%20Networks%20for%20Zero-Shot%20Learning%2F</url>
    <content type="text"><![CDATA[Xian Y, Lorenz T, Schiele B, et al. Feature generating networks for zero-shot learning[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2018: 5542-5551. 1. Overview 1.1. Motivation data imbalance between seen and unseen classes In this paper synthesizes CNN features conditioned on class-level semantic information WGAN + classification loss exploit generated CNN features to train softmax classifier 1.2. Contribution f-CLSWGAN experiments on five dataset generate deep feature to improve ZSL 1.3. Related Works1.3.1. GAN GAN cGAN DCGAN InfoGAN WGAN 1.3.2. ZSL and GZSL learn compatibility between images and classes classification 2. Methods 2.1. Definitions x. image feature y. seen label c(y). label embedding u. unseen label 2.2. f-GAN D. [0, 1] with MLP + sigmoid 2.3. f-WGAN α ~ U(0, 1) D. no sigmoid 2.4. f-CLSWGAN 2.5. Classification Multimodal Embedding Softmax 3. Experiments 3.1. Dataset 3.2. Details sentence. character-based CNN-RNN attribute. G and D. MLP + LeakyReLU no BN the final layer of G. ReLU λ=10, β=0.01 3.3. Evaluation T1. average per-class top-1 accuracy harmonic mean 3.4. Comparison 3.5. Ablation Study]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Learning</category>
        <category>Zero-Shot Learning</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>Cross-Modality</tag>
        <tag>Classification</tag>
        <tag>GAN</tag>
        <tag>Zero-Shot Learning</tag>
        <tag>Feature Generation</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(CVPR 2018) An empirical evaluation of generic convolutional and recurrent networks for sequence modeling]]></title>
    <url>%2F2018%2F06%2F22%2FAn%20empirical%20evaluation%20of%20generic%20convolutional%20and%20recurrent%20networks%20for%20sequence%20modeling%2F</url>
    <content type="text"><![CDATA[Bai S, Kolter J Z, Koltun V. An empirical evaluation of generic convolutional and recurrent networks for sequence modeling[J]. arXiv preprint arXiv:1803.01271, 2018. 1. Overview In this paper, it demonstrated that simple CNN (TCN) outperforms RNN, such as LSTM TCN. longer memory, more accurate, simper RNN. not paralleled 1.1. Background1.1.1. Application part-of-speech tagging and semantic role labelling sentence classification document classification machine translation audio synthesis language modeling 1.1.2. Architecture LSTM GRU ConvLSTM Quasi-RNN dilated RNN 2. TCN 2.1. Architecture input. sequence of any length output sequence of the same length no gating mechanism longer memory 1D Conv with padding receptive fileds dilation factor d kernel size k network depth n d=2^n to make sure to hit each input within the effective history 2.2. Advantage parallelism flexible receptive filed size stable gradient low memory requirement for training variable length inputs 2.3. Disadvantage data storage during evaluation potential parameter change for a transfer of domain 3. Experiments 3.1. Details gradient clipping helped convergence [0.3, 1] find that TCN insensitive to hyperparameter changes, as long as the effective history size is sufficient 3.2. Comparison 3.3. Adding Problem 3.4. Controlled Experiments]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Architecture</category>
        <category>Network</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>Network</tag>
        <tag>TCN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(CVPR 2018) Attention is all you need]]></title>
    <url>%2F2018%2F06%2F22%2FAttention%20is%20all%20you%20need%2F</url>
    <content type="text"><![CDATA[Vaswani A, Shazeer N, Parmar N, et al. Attention is all you need[C]//Advances in Neural Information Processing Systems. 2017: 6000-6010. 1. Overview 1.1. Motivation dominant sequence transduction models based on RNN or CNN RNN can not parallel, computational complexity, hard to learn long-range dependencies In this paper, it proposed Transformer based solely on attention mechanism without regard to their distance attention to achieve global dependencies 1.2. Related Work reduce sequential computation self-attention (intra-attention) end-to-end memory network. based on recurrent attention mechanism 2. Methods encoder. [x_1, …, x_n]→ [z_1, …, z_n] decoder. [z_1, …, z_n]→ [y_1, …, y_m] 2.1. Encoder N = 6 d_model = 512 two sub-layers + layer normalizaion + residual multi-head self-attention position-wise FC feed-forward network 2.2. Decoder N = 6 insert a third sub-layer. perform multi-head attention over the output of the encoder stack 2.3. Attention 2.3.1. Scaled Dot-Product Attention 2.3.2. Multi-Head Attention h = 8 d_k = d_v = d_{model} / h = 64 Found it beneficial to linearly project the queries, keys and values h times with different, learned linear projections to d_k, d_k and d_v dimensions. 2.4. Multi-Head Attention encoder-decoder attention. Q from previous decoder, K-V from final encoder self-attention in encoder self-attention in decoder Mask out all value which correspond to illegal connection to prevent leftward information flow in the decoder to preserve the auto-regressive property 2.5. Position-Wise FC or 2 1x1Conv inner-layer d_{ff} = 2048 input d_{model} = 512 2.6. Position Encoding since no RNN or CNN, need to inject information about the token’s position use sine and cosine function 3. Experiments 3.1. Details sentence encoded by byte-pair encoding sentence pairs were batched together by approximate sequence length Adam; LR increase linearly by first warmup_steps then decrease Dropout 0.1. 3.2. Comparison 3.3. Ablation Study B. reducing d_k hurts model quality C,D. bigger model better, dropout helpful in avoiding over-fitting E. position embedding nearly identical to sinusoid]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Architecture</category>
        <category>Network</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>Network</tag>
        <tag>Transformer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(CVPR 2018) Visual Question Reasoning on General Dependency Tree]]></title>
    <url>%2F2018%2F06%2F22%2FVisual%20Question%20Reasoning%20on%20General%20Dependency%20Tree%2F</url>
    <content type="text"><![CDATA[Cao Q, Liang X, Li B, et al. Visual question reasoning on general dependency tree[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018: 7249-7257. 1. Overview 1.1. Motivation recent work tried to explicit compositional processes to assemble multiple sub-task embedded in the questions, but rely on hand-craft or annotation In this paper, it proposed ACMN (Adversarial Composition Modular Network) adversarial attention module. local visual evidence for each word residual composition module. construct a dependency tree for each question clausal predicate relation modifer relation enforce each parent node not explore new regions by masking out attentive regions of its child nodes at each step 1.2. Contribution interpretable reasoning VQA system adversarial attention module. enforce efficient visual evidence mining for modifier relations residual composition module. integrat knowledge of child nodes for clausal predicate relations 1.3. Related Work1.3.1. VQA CNN-LSTM attention, stacked attention, co-attention joint embedding compact bilinear method 1.3.2. Reasoning Modal database queries 2. Adversarial Composition Modular Network generate tree by universal Stanford Parser prune the leaf-nodes that are not noun M. modifier relation P. clausal predicate relation x. node x_c1, x_c2, …, x_cn. n child node of x v. spatial feature through pre-trained network w. word embedding through a Bi-LSTM set of modules f. apply on each word; share weights input of f. (v, w, child’s output) output of f. (attention region att_out, hidden feature h_out) 2.1. Adversarial Attention Module filter child nodes ∈ M sum attention map of these child nodes 1 - summation attention map to get mask mask x v output new attention map att_out get h’ based on att_out 2.2. Residual Composition Module sum hidden features h of child ∈ P concat summation of h and h’ add it with all children‘ h 2.3. Overview the nodes with modifier relations M can modify their parent node by referring to a more specific object the nodes with clausal predicate relations P can enhance the representation output feature h_root → 3 MLP to get y 3. Experiments 3.1. Dataset CLEVR Sort-of-CLEVR VQAv2 3.2. Details 224x224 image max tree hight 13 for CLEVR 3.3. Comparison 3.4. Ablation Study]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Cross-Modality</category>
        <category>VQA</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>Cross-Modality</tag>
        <tag>Reasoning</tag>
        <tag>VQA</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(CVPR 2016) Learning deep representations of fine-grained visual descriptions]]></title>
    <url>%2F2018%2F06%2F22%2FLearning%20deep%20representations%20of%20fine-grained%20visual%20descriptions%2F</url>
    <content type="text"><![CDATA[Reed S, Akata Z, Lee H, et al. Learning deep representations of fine-grained visual descriptions[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2016: 49-58. 1. Overview 1.1. Motivationattribute-based zero shot attribute do not provide a natural language interface fine-grained recognition requires commensurately more attribute natural language provides a flexible and compact way of encoding only the salient visual aspects for distinguishing categories In this paper, it proposed word or character based methods word-CNN-RNN character-CNN-RNN 1.2. Related Work fine-grained classification zero-shot image and video caption 2. Methods 2.1. Embedding2.1.1. Objective 2.1.2. Inference compatibility classifier 2.1.3. Learning since 0-1 loss is discontinue 2.2. Text-based ConvNet image width is 1 pixel, channel number is equal to the alphabet size 2.3. Convolutional Recurrent Net (CNN-RNN) only CNN lacks a strong temporal dependency along the input text sequence low-level temporal features learned by efficiently with fast CNN temporal structure can still be exploitd at the more abstract level of mid-level features final encoded feature is the average hidden unit activation over the sequence 3. Experiments 3.1. Dataset CUB Flowers 3.2. Details crop horizontal flip length of words 30. length of character 201 RMSprop 3.3. Comparison]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Architecture</category>
        <category>Network</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>Cross-Modality</tag>
        <tag>Network</tag>
        <tag>Word-CNN-RNN</tag>
        <tag>Character-CNN-RNN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(ICCV 2017) Face Synthesis from Visual Attributes via Sketch using Conditional VAEs and GANs]]></title>
    <url>%2F2018%2F06%2F22%2FFace%20Synthesis%20from%20Visual%20Attributes%20via%20Sketch%20using%20Conditional%20VAEs%20and%20GANs%2F</url>
    <content type="text"><![CDATA[Di X, Patel V M. Face synthesis from visual attributes via sketch using conditional vaes and gans[J]. arXiv preprint arXiv:1801.00077, 2017. 1. Overview In this paper, it proposed Attribute2Sketch2Face A2S. CVAE S2S. GAN S2F. GA 1.1. Motivation stage-wise learning AUDeNet. dense UNet-based 1.2. Related Work Image-to-Image VAE GAN Autoregression CVAE CGAN CycleGAN Wasserstein Distance StackGAN CVAE+GAN 2. Methods 2.1. Attribute-to-Sketch (A2S) encoder q_Φ. encode sketch and encode attribute encoder q_β. encode noise and encode attribute only texture attribute 2.2. Sketch-to-Sketch (S2S) UNet. long skip to preserve low-level features Dense. short skip to maximize information flow D. patch-based only texture attribute VGG of Conv1_2 2.3. Sketch-to-Face (S2F) attribute consist of both color and texture 2.4. Testing 3. Experiments 3.1. Dataset CelebA LFW CUHK use pencil-sketch synthesis method to generate the sketch images from the face images on CelebA and LFW which lack of sketch select attribute of texture and color 3.2. Ablation Study w/o attribute in A2S w/o S2S w/o attribute concat from S2F 3.3. Metric Inception Score Attribute L2. extract attributes from MOON attribute prediction 3.4. Comparison 3.5. Synthesis fix noise z fix attrbute]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Cross-Modality</category>
        <category>Image-Text</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>Cross-Modality</tag>
        <tag>Face</tag>
        <tag>Image Generation</tag>
        <tag>GAN</tag>
        <tag>Attribute to Face</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(ICCV 2017) Semantic image synthesis via adversarial learning]]></title>
    <url>%2F2018%2F06%2F22%2FSemantic%20image%20synthesis%20via%20adversarial%20learning%2F</url>
    <content type="text"><![CDATA[Reed S E, Akata Z, Mohan S, et al. Learning what and where to draw[C]//Advances in Neural Information Processing Systems. 2016: 217-225. 1. Overview In this paper, it attempted to synthesis and meet two requirements disentangle the semantic information from two modalities and generate new images from the combined semantics. realistic while matching the target text description maintain other image features that are irrelevant to the text description 1.1. Related Work deterministic networks VAE autoregression VAE GAN 2. Methods 2.1. Architecture CA technique from StackGAN residual. output image would retain similar structure of the source image 2.2. Adaptive Loss for Semantic Image Synthesis +. positive -. negative 2.3. Loss Function 2.4. Improving Image Feature Representation pretrained VGG of conv4 2.5. Visual-Semantic Text Embedding pair-wise ranking loss 3. Experiments 3.1. Details 0.0002 Adam with 0.5 momentum, decrease by 0.5 batch size 64 flipping, rotating, zooming, cropping 3.2. Comparison 3.3. Interpolation 3.4. Variety]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Cross-Modality</category>
        <category>Image-Text</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>Cross-Modality</tag>
        <tag>Image Generation</tag>
        <tag>GAN</tag>
        <tag>Semantic Image Synthesis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(NIPS 2016) Learning what and where to draw]]></title>
    <url>%2F2018%2F06%2F22%2FLearning%20what%20and%20where%20to%20draw%2F</url>
    <content type="text"><![CDATA[Reed S E, Akata Z, Mohan S, et al. Learning what and where to draw[C]//Advances in Neural Information Processing Systems. 2016: 217-225. 1. Overview 1.1. Motivation existing methods synthesize images based on global constraints (class label and caption), do not provide control over pose or object location In this paper, it proposed Generative Adversarial What-WHere Network (GAWWN) synthesize images given instructions describing that content to draw in which location condition on coarse location. Implemented using STN condition on part location. set of normalized (x, y) coordinates 1.2. Contribution novel architecture for text- and location-controllable image synthesis text-conditional object part completion model enabling a streamlined user interface for part locations 1.3. Related Work CNN (deterministic) VAE, convolutional VAE, recurrent VAE (probabilistic) GAN STN 1.4. Future Work learn the obj and part location in an unsupervised or weakly supervised way 2. GAWWN 2.1. Bounding-Box-Conditional Text-to-Image Model replicate text embedding spatially to form a MxMxT feature map warp spatially to fit into the normalized bounding box coordinates (outside the box are all zeros) 2.2. Keypoint-Conditional Text-to-Image Model location key points are encoded into a MxMxK spatial feature map (channels correspond to the part) max replicate depth 2.3. Conditional Keypoint Generation ModelNot optimal to require users to enter every single keypoint of the parts of object they wish to be drawn. it would be useful to have access to all of the conditional distributions of unobserved ketpoints given a subset of observed keypoints and the text description. 3. Experiments 3.1. Details text embedding. char-CNN-GRU 0.0002 Adam, batch size 16 3.2. With Bounding Box 3.3. Via Keypoints 3.4. Comparison]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Cross-Modality</category>
        <category>Image-Text</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>Cross-Modality</tag>
        <tag>Image Generation</tag>
        <tag>GAN</tag>
        <tag>Text to Image</tag>
        <tag>Location-Controllable</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(ICCV 2017) Stackgan:Text to photo-realistic image synthesis with stacked generative adversarial networks]]></title>
    <url>%2F2018%2F05%2F30%2FStackgan%3A%20Text%20to%20photo-realistic%20image%20synthesis%20with%20stacked%20generative%20adversarial%20networks%2F</url>
    <content type="text"><![CDATA[Zhang H, Xu T, Li H, et al. Stackgan: Text to photo-realistic image synthesis with stacked generative adversarial networks[C]//IEEE Int. Conf. Comput. Vision (ICCV). 2017: 5907-5915. 1. Overview 1.1. Motivation existing methods fail to contain details and vivid object parts instability of training GAN the limited number of training text-image pairs often results in sparsity in the text conditioning manifold and such sparsity makes it difficult to train GAN In this paper, it proposed StackGAN decompose the hard problem into more manageable sub-problems stage I. sketch the primitive shape and colors, low-resolution stage II. details Conditioning Augmentation Technique. smoothness in the latent conditioning manifold 1.2. Contribution StackGAN Conditioning Augmentation (CA) 1.3. Related Work1.3.1. Generative Model VAE Pixel RNN GAN energy-based GAN 1.3.2. Conditional Image Generation variable such as attributes or class label image-to-image. photo editing, domain transfer, SR 1.3.3. Series of GAN 2. StackGAN 2.1. Conditioning Augmentation latent space for text embedding usually high, limited amount of data causes discontinuity in the latent data manifold CA yields more training pairs, smoothness over conditioning manifold and avoid overfitting 2.2. Stage-I GAN set λ = 1 I_0. real image 2.3. Stage-II GAN s_0. LR generated by stage-I two stages share the same text encoder and different CA 2.4. Details first train stage-I GAN, fix stage-II GAN then train stage-II GAN, fix stage-I GAN 0.0002 Adam decay 0.5, mini-batch 64 nearest-neighbour upsample dimension of z 100 3. Experiments 3.1. Dataset MSCOCO CUB 3.2. Metric Inception Score x. generated sample y. label predicted by Inception Model (fine-tune on Experiment dataset) Human Evaluation 3.3. Comparison GAN-INT-CLS. only reflect the general shape and color of the birds GAWWN. fail to generate plausible images stage-II GAN can correct the defects of stage-I even when stage-I fails to draw a plausible shape, shape-II can generate reasonable object 3.4. Ablation Study CA helps stabilize training and improve diversity of generated samples, because of its ability to encourage robustness to small perturbation along the latent manifold 3.5. Interpolation]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Cross-Modality</category>
        <category>Image-Text</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>Cross-Modality</tag>
        <tag>Image Generation</tag>
        <tag>GAN</tag>
        <tag>Text to Image</tag>
        <tag>StackGAN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(ICML 2017) Generative adversarial text to image synthesis]]></title>
    <url>%2F2018%2F05%2F30%2FGenerative%20adversarial%20text%20to%20image%20synthesis%2F</url>
    <content type="text"><![CDATA[Reed S, Akata Z, Yan X, et al. Generative adversarial text to image synthesis[J]. arXiv preprint arXiv:1605.05396, 2016. 1. Overview 1.1. Motivation RNNs are developed to learn discriminative text feature representations GANs two problems learn text feature representation that captures the important visual detailss features to synthesize a compelling image In this paper, it translated visual concepts from character to pixels introduce manifold interpolation regularizer 1.2. Contribution first end-to-end text-to-image based on GAN zero-shot text-to-image synthesis 1.3. Related Work Multimodal Learning audio-video Boltzmann machine DeConv recurrent convolutional encoder-decoder image caption 1.4. Joint Embedding Δ. 0-1 loss v_n. images t_n. corresponding text description y_n. class labels f_v and f_t are parametrized by φ. text encoder Φ. image encoder T(y). text description of class y V(y). image description of class y 2. Methods 2.1. Architecture2.1.1. G Z. dimension of noise from N(0, 1) T. dimension of text embedding, encoded by text encoder φ D. dimension of image φ(t)-FC(128)-leakyReLU-concat with z 2.1.2. D 2.2. Matching-aware Discriminator (GAN-CLS) real + right text real + wrong text fake + right text 2.3. Learning with Manifold Interpolation (GAN-INT) generate a large amount of additional text embedding by interpolation between embeddings of training set captions found β=0.5 works well 2.4. Inverting the Generator for Style Transfer noise sample z should capture style factors such as background color and pose transfer style of a query image onto the content of a particular text description S. trained style encoder network 3. Experiments 3.1. Details pre-train char-CNN-RNN (text encoder) to increase the speed of training, produce 1024 dimension vector and project to 128 dimension image size 64 x 64 x 3 LR 0.0002, Adam with 0.5 momentum minibatch size 64 3.2. Quantitative Results 3.3. Disentangling Style and Content content. visual attributes (shape, size, color of birds) style. background color, pose oriation text embedding mainly covers content information and typically nothing about style GAN must learn to use noise z for style variation 3.4. Pose and Background Style Transfer 3.5. Sentence Interpolation]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Cross-Modality</category>
        <category>Image-Text</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>Cross-Modality</tag>
        <tag>Image Generation</tag>
        <tag>GAN</tag>
        <tag>Text to Image</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(CVPR 2018) Photographic Text-to-Image Synthesis with a Hierarchically-nested Adversarial Network]]></title>
    <url>%2F2018%2F05%2F30%2FPhotographic%20Text-to-Image%20Synthesis%20with%20a%20Hierarchically-nested%20Adversarial%20Network%2F</url>
    <content type="text"><![CDATA[Zhang Z, Xie Y, Yang L. Photographic text-to-image synthesis with a hierarchically-nested adversarial network[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018: 6199-6208. 1. Overview 1.1. Motivation fully end-to-end map from low-dimension text space to a high-resolution image space still remains unsolved two difficulty balance the convergence between G and D stably model the huge pixel space in high-resolution images and guaranteeing semantic consistency In this paper, it proposed an extensille single-stream generator architecture (HDGAN) jointed discriminator. regularize mid-level representation and assist generator training to capture the complex image statistics multi-purpose adversarial loss new visual-semantic similarity measure single stage no multiple text condition no additional class label supervision 1.2. Dataset CUB birds Oxford-102 flowers MSCOCO 1.3. Related Work1.3.1. Generative Models GAN VAE 1.3.2. Text-to-Image (ICML 2016) GAN (NIPS 2016) GAN what-where network (ICCV 2017) StackGAN (ICCV 2017) joint embedding perceptual loss auxiliary classifier attention-driven 1.3.3. Stability of GAN training techniques regularization using extra knowledge combination of G and D As the targeting image resolution increases, training difficulty increases. 1.3.4. Decompose into Multiple Subtasks LAP-GAN symmetric G and D stage-by-stage 2. Methods 2.1. Hierarchical-nested Adversarial Objective G. hierarchical generator z. noise t. sentence embedding by pre-trained char-RNN text encoder s. number of scales X_n. gradually growing resolution lower-resolution. learn semantic consistent image structure higher-resolution. render fine-grained details 2.2. Multi-purpose Adversarial Loss pair loss (1). guarantee the global semantic consistency Image loss (R_i x R_i). low-resolution D focus on global structures, high focus on local image details output. two type of errors real image + mismatched text fake image + conditioned text 2.2.1. D 2.2.2. G 2.2.3. Conditioning AugmentationInstead of directly using the deterministic text embedding, sample a stochastic vector from a Gaussian distribution And add Kullback-Leiblere divergence regularization term to prevent over-fitting and force smooth 2.3. Architecture2.3.1. G three modules K-repeat ResBlock. 2 Conv + BN-ReLU stretching layers. x2 nearest upsample + Conv-BN-ReLU linear compression layers. Conv-Tanh structure:1-2-1-2-…. text embedding of CA. 1024 x 4 x 4 2.3.2. D stride-2 conv + BN-LeakyReLU two branch following FCN produce R_i x R_i probability map concat 512 x 4 x 4 feature map and 128 x 4 x 4 text embedding (reduced), 1x1 Conv + 4x4 Co 3. Experiments 3.1. Metrics Inception Score. need pre-trained Inception model (fine-tuned on dataset of this paper) Multi-scale Structural Similarity (MS-SSIM). pairwise similarity, lower score indicates higher diversity of generated image VIsual-semantic Similarity. train a visual-semantic embedding model to measure the distance between text and image δ. margin set to 0.2 3.2. Comparison better preserve semantically consistent information in all resolution 3.3. Style Transfer 3.4. Ablation Study local image loss for higher performantic and offer the pair loss more focus on learning sementic consistency 3.5. share top layers of Ds not observe benifits]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Cross-Modality</category>
        <category>Image-Text</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>Cross-Modality</tag>
        <tag>GAN</tag>
        <tag>Text to Image</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(CVPR 2018) Diversity Regularized Spatiotemporal Attention for Video-based Person Re-identification]]></title>
    <url>%2F2018%2F05%2F30%2FDiversity%20Regularized%20Spatiotemporal%20Attention%20for%20Video-based%20Person%20Re-identification%2F</url>
    <content type="text"><![CDATA[Li S, Bak S, Carr P, et al. Diversity regularized spatiotemporal attention for video-based person re-identification[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018: 369-378. 1. Overview 1.1. Motivation most existing methods encoding each video frame in its entirely and computing an aggregate representation across all frame the remaining visible portions of the person may provide strong cues for re-identification features directly generated from entire images can easily miss fine-grained visual cues In this paper, it proposed spatialtemporal attention model multiple spatail attention model (alignment) + diversity regularization term (Hellinger Distance) to not discover the same body align corresponding image patches across frames determining whether a particular part of the body is occluded or not temporal attention automatically discovers a diverse set of distinctive body parts extract useful information from all frames without succumbing to occlusions and misalignment 1.2. Related Work1.2.1. Image-Based Person Re-id extracting discriminative features learning robust metrics Online Instance Matching Loss 1.2.2. Video-Based Person Re-id(extension of image-based) top-push distance RNN space-time 1.2.3. Attention Models for Person Re-id avoid focus on similar region 2. Methods 2.1. Restricted Random Sampling divide video into N chunks of equal duration random sample an image of each chunk 2.2. Multiple Spatial Attention Modelsfoucs on body part, hats, bags, … ResNet-50 (1 Conv + 4 ResBlock). 8x4 grids L. 32 D. 2048 dimension the weight of n-th frame, k-th attention part, l-th grid weighted attention region Enhance (appendix) 2.3. Diversity Regularization collection of each region weight for n-th frame&gt; K. the number of attention model L. grids Hellinger Distance. maximize the distance Regularization Term. will be multipled by a coefficient and added to original OIM loss variant 2.4. Temporal AttentionPooling features arocss time using a per-frame weight is not sufficiently robust, since some frames could contain valuable partial information about an individual. (apply same temporal attention weight to all frames) weight across all frames for one attention region 2.5. Overview entire video is represented by vector x ∈ (K x D) 2.6. Re-id Loss OIM 3. Experiments 3.1. Details N = 6 pretrain ResNet-50 on image-based re-identification datasets fixed CNN, train multiple spatial attention model (Diversity Regularization) fixed CNN, jointly train the whole network SGD, 0.1 drop to 0.01 128 dimension L2-normalized 3.2. Ablation Study 3.2.1. different number of spatial attention models treating a person as single region instead of two distinct body parts is better 3.3. Comparison]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Body</category>
        <category>Person Re-ID</category>
      </categories>
      <tags>
        <tag>Video</tag>
        <tag>Person Re-ID</tag>
        <tag>Body</tag>
        <tag>Spatiotemporal Attention</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(CVPR 2017) Joint detection and identification feature learning for person search]]></title>
    <url>%2F2018%2F05%2F30%2FJoint%20detection%20and%20identification%20feature%20learning%20for%20person%20search%2F</url>
    <content type="text"><![CDATA[Xiao T, Li S, Wang B, et al. Joint detection and identification feature learning for person search[C]//2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE, 2017: 3376-3385. 1. Overview 1.1. Motivation existing methods mainly focus on matching cropped pedestrian images between queries and candidates (assum perfect detection) In this paper, it proposed a framework for person search jointly handle pedestrian detection and person re-identification proposal net. focus more on the recall rather than the precision misalignments of proposal can be furtheradjusted by the identification net Online Instance Matching (OIM) loss function collect and annotate a large-scale benchmark dataset 1.2. Comparison of Loss Function pairwise or triplet loss. O(N^2) need efficient strategy, difficult to find softmax. compare all samples at the same time, as the number of classes increases, training the big softmax classifier matrix become much slower or even can not converge OIM. compare samples of mini-batch with all registered entries unlabeled identities can be served as negatives for labeled identities 1.3. Contribution jointly optimization OIM loss function dataset 1.4. Related Work1.4.1. Person Re-identification manually design discriminative features learn feature transforms across camera views learning distance metrics CNN triplet samples classify on abnormal images. low-resolution and partially occlude images 1.4.2. Pedestrian Detection hand-crafted features. DPM, ACF and Checkerboard 1.5. Dataset CUHK03 Market501 Duke 2. Method 2.1. Structure output. 2048 dimension→ L2-normalized 256 dimension→ cosine similarities 2048 dimension to proposal alignment 2.2. Online Instance Matching Loss only consider the labeled abd unlabeled identities while leave the other proposals untouched the look up table (LUT). L: the size of the table; D: vector dimension forward. compute cosine similarities between the mini-batch sample and all the labeled identities.x. the features of a labeled identity inside a mini-batch backward. if targe class id is t, update t-th column of the LUT, and then scale to unit L2-norm many unlabeled identities can be safely used as negative classes for all the labeled identities, and store in circular queue. Q: size of the queue cosine similarities The probability of x being recognized as the identity with class-id i L. the number of different target people Q. the size of circular queue to store unlabeled prople τ. higher temperature leads to softer probability distribution The probability of x being recognized as the i-th unlabeled identity Maximization Degradation ) 2.3. Drawback of Softmax classifier matrix suffers from large variance of gradients and can not by learned effectively large number of identities, which only has several instances; each image contains a few identities learn more than 5,000 discriminant functions simultaneously, but during each SGD iteration we only have positive samples from tens of classes can not exploit the unlabeled identities with softmax loss OIM is non-parametric. potential drawback. overfit more easily, it find that projecting features into a L2-normalized low-dimensional subspace helps reduce overfitting 2.4. Scalability when the number of identities increases, OIM could be time-consuming approximate by sub-sampling the labeled and unlabeled identities 3. Dataset 3.1. Come From hand-held camera to shoot movie snapshots 3.2. Processing ignore smaller heights than 50 pixels 3.3. Evaluation no overlapped images or labeled identities between training and test set 3.4. Metrics cumulative matching characteristics (CMC top-K) mean averaged precision (mAP) 4. Experiments 4.1. Details τ. 0.1 size of circular queue. 5,000 mini-batch. 2 images learning rate. 0.001 to 0.0001 after 40k 4.2. Detection 4.3. Search 4.4. OIM converge faster consistently improves the test performance 4.5. Sub-sample of OIM small number converge faster 4.6. Low-dimensional Subspace project features into a proper low-rank subspace is very important to regularize the network training 4.7. Detection Recall higher recall does not necessarily lead to higher person search performance, re-id method could still get confused on some false alarms should not only focus on training re-id methods with manually cropped pedestrians, but should consider the detections jointly under the person search problem setting 4.8. Gallery Size larger gallery, more difficult all methods may suffer from some common hard samples]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Body</category>
        <category>Person Re-ID</category>
      </categories>
      <tags>
        <tag>Person Re-ID</tag>
        <tag>Body</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(2017) Dual-Path Convolutional Image-Text Embedding]]></title>
    <url>%2F2018%2F05%2F30%2FDual-Path%20Convolutional%20Image-Text%20Embedding%2F</url>
    <content type="text"><![CDATA[Zheng Z, Zheng L, Garrett M, et al. Dual-path convolutional image-text embedding with instance loss[J]. arXiv preprint arXiv:1711.05535, 2017. 1. Overview 1.1. Motivation existing methods uses RNN for text feature learning and employs off-the-shelf CNN for image extraction ImageNet pre-trained models do not preserve rich image details that are critical for matching languages In this paper, it proposed CNN for fine-tuning the visual and textual representation (network only contains Conv, Pooling, ReLU, BN) instance loss according to viewing each multimodal data pair as a class 1.2. Contribution dual-path CNN model instance loss outperform on FLickr30K, MSCOCO, CUHK-PEDES 1.3. Related Work1.3.1. Model for Image Recognition fixed CNN feature as input 1.3.2. Model for Natural Language Understanding word2vec RNN, directional LSTM CNN to conduct machine translation, 9.3x speed up 1.3.3. Multi-modal Learning class-level retrieval. leverage the class labels in training set instance-level retrieval. match image-text pairs, does not use any class label In this paper, it focus on instance-level retrieval and propsoed instance loss. 1.4. Dataset Flickr30k. 5 sentences, avg 10.5 words per sentence after remove rare word MSCOCO. avg 5 sentences, avg 8.7 words after rare word removal CUHK-PEDES. 2 sentences, avg 19.6 words after removal 2. Method 2.1. Deep Image CNN pre-trained models can still provide for good CNN initialization input. 224x224 output. 2048 dimension vector 2.2. Deep Text CNN2.2.1. Text Processing convert sentence to code T [n x d]. (ont-hot) n. length of the sentence (set to fixed length) d. size of the dictionary use word2vec to filter out rare words pad zeros to T, if less than fixed length words reshape T to 1 x 32 x d (h, w, c) position shift (more robust). pad random number of zeros at the beginning and the end of sentence left alignment. only padding at the end of the sentence. 2.2.2. Deep Text CNN input. T (1 x 32 x d) output. 2048 dimension vector the filter size of the first conv is 1 x 1 x d x 300, two method to init random initialization using d x 300 matrix from word2vec for initialization (better) kernel of Conv. 1x2, every two neighbour components may form a phrase containing content information 2.3. Loss Function2.3.1. Ranking Loss I. visual input T. text input I_a/T_a. the same image/text group I_n/T_n. negative sample α. margin the convergence of ranking loss requires both image and text branches converge may be prone to getting stuck in a local minimum 2.3.2. Instance Loss assumption. each image/text group is distinct instance loss is a softmax loss classifies an image/text group into one of a large number of classes L. loss P. probability P(c). predicted possibility of the right class c enforce shared weight in the fully connected layer for the two modalities, otherwise the learned image and text features mat exist in totally different subspace. 2.3.3. Total Loss stage-1. only instance loss, λ_1 = 0 so the ranking loss can fing a better optimisation for both modalities in the second stage using the instance loss alone can lead to a competitive result instance loss encourages the model to find the fine-grained differences, such as ball, stick,.. stage-2. ranking loss + instance loss 2.3.4. Training Stage stage-1. fixed pre-trained image CNN, use instance loss to tune the remaining part if we train the image and text CNNs simultaneously, the text CNN may comprise the pre-trained image CNN stage-2. instance loss + ranking loss to fine-tune the entire network 3. Experiments 3.1. Metric Recall@K. possibility that true match appears in the top K of the rank list Median Rank. median rank of the closest ground truth result in the rank list 3.2. Details SGD + fixed 0.9 momentum Matconvnet Framework 224x224 random crop from shorter side 256 horizontal flipping for image position shift for text 0.75 dropout max text length 32 for Flickr30K and MSCOCO, 56 for CUHK-PEDES LR. 0.001 α=1 3.3. Comparison 3.4. Ablation Study3.4.1. Loss stage-1. ranking loss focuses on inter-modal distance, may be hard to tune the visual and textual features simultaneous at the beginning stage-1. instance loss performs better, which focusses more on learning intra-modal discriminative descriptors instace loss help to regularise the model 3.4.2. Fine-tune fine-tune in stage-2 helps 3.4.3. Initialization word2vec initialization helps 3.4.4. Position Shift vs Left Alignment 3.5. Training Time image CNN ~119ms per image batch (32) on 1080Ti text CNN ~117ms per sentence batch (32) image feature and text feature can be simultaneously calculated, the model can run in a parallel style efficiently.]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Cross-Modality</category>
        <category>Image-Text</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>Cross-Modality</tag>
        <tag>Image-Text</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(ICCV 2017) Identity-aware textual-visual matching with latent co-attention]]></title>
    <url>%2F2018%2F05%2F30%2FIdentity-aware%20textual-visual%20matching%20with%20latent%20co-attention%2F</url>
    <content type="text"><![CDATA[Li S, Xiao T, Li H, et al. Identity-aware textual-visual matching with latent co-attention[C]//Proceedings of the IEEE International Conference on Computer Vision. 2017: 1890-1899. 1. Overview 1.1. Motivation most existing methods tackle textual-visual matching problem without effectively utilizing identity-level annotations RNNs have difficulty in remembering the complete sequential information of very long sentences RNNs are variant to different sentence structures In this paper, it proposed an identity-aware two-stage framework stage-1. learn to embed cross-modal feature with Cross-Modal Cross-Entropy (CMCE) loss; provide initial training point for stage-2; screen easy incorrect matchings stage-2. refine the matching results with a latent co-attention mechanism spatial attention. relates each word with corresponding image region latent semantic attention. aligns different sentence structures to make the matching results more robust to sentence structure variation; at each step of the LSTM, it learns how to weight different words’ features to be more invariant to sentence structure variations 1.2. Contribution identity-aware two-stage framework CMCE loss latent co-attention mechanism 1.3. Related Work1.3.1. Visual Matching with Identity-Level Annotation person re-identification face recognition classify all identities simultaneously. face challenge when number of classes is too large pair-wise or triplet distance loss function. hard negative training samples might be difficult to sample as the number of training sample increases 1.3.2. Textual-Visual Matching image caption VQA text-image embedding 2. Methods 2.1. Stage-1 with CMCE Loss map image and description into a joint feature embedding space Cross-Modal Cross-Entropy (CMCE) to minimize intra-identity and maximize inter-identity feature distances 2.1.1. Cross-Modal Cross-Entropy Loss pair-wise or triplet loss. N identities contains O(N^2) training samples, difficult to sample hard negative samples CMCE. compare each identity in mini-batch from one modality to all N identites in another modality, can cover all hard negative samples cross-modal affinity. inner products of features from the two modalities textual and visual feature buffers. enable efficient calculation of textual-visual affinities before the first iteration. if an identity has multiple descriptions or images, its stored features in the buffers are the average of the multiple samples in each iteration. (calc loss)-(BP)-(update corresponding rows in buffer for sampled identity), if identity t has multiple images or descriptions update by affinity between one image feature v and ith textual feature S_i. σ. temperature hyper-parameter to control how peaky the probability distribution affinity between one textual feature s and kth image feature V_k. maximize the probability of corresponding identity pairs 2.2. Stage-2 with Latent Co-attention stage-1. visual and text feature embeddings might not be optimal, compress the whole sentence into a single vector stage-1. sensitive to sentence structure variation input. a pair of text description and image output. matching confidence trained stage-1 network serves as the initial point for the stage-2 network only hard negative matching samples from stage-1 results are utilized for training stage-2 2.2.1. Encoder Word-LSTM with Spatial Attention generate the weight between word and L regions sum of all weighted region concate the word and sum word feature of LSTM. H={h_1, …, h_T}, H∈(D_H x T) image feature. I={i_1, …, i_L}, I∈(D_I, L) D_H. dimension of hidden state D_I. dimension of image region feature T. number of words L. number of region W_I, W_H. transform feature to K-dimension space W_P. conver feature to affinity score sum of weighted L regions according to a word concate the word and its weighted region 2.2.2. Decoder LSTM with Latent Semantic Attention generate weights between last hidden state and all x_t sum of weighted x_t process and fed into next LSTM step LSTM not robust to sentence structure variations decoder LSTM with latent semantic attention automatically align sentence structure M-step decoder LSTM processes the encoded feature step by step while searches through the entire input sentence to align the image-word features x_t, at mth step f. two-layer CNN weight the importance of jth word for mth decoding step c_{m-1}. hidden state by decoder LSTM for step m-1 x_m. transform by two-FC layers LSTM is able to focus more on relevant information by re-weighting the source image-word features to enhance the network’s robust to sentence structure variation easier training samples are filtered out by the stage-1 network N’. the number of training samples for training the stage-2 3. Experiments 3.1. Dataset CUHK-PEDES. two descriptions Caltech-UCSD birds (CUB). ten descriptions Oxford-102 Flowers. ten descriptions 3.2. Details σ=0.04 Adam of LSTM, SDG of CNN training and testing samples are screened by the mathcing results of stage-1 for each visual or textual sample, we take its 20 most similar samples from the other modality by stage-1 and construct textual-visual pair samples for stage-2 training and testing 3.3. Comparison 3.4. Ablation Study CMCE loss vs Triplet loss. 3 times more training time than CMCE identity number vs counting number. latent semantic attention vs remove it. align visual and semantic concepts, mitigate sensitivity to sentence structure spatial attention vs concate visual and textual feature. stage-1 vs w/o stage-1]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Cross-Modality</category>
        <category>Person Search</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>Cross-Modality</tag>
        <tag>Body</tag>
        <tag>Person Search</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(CVPR 2017) Person search with natural language description]]></title>
    <url>%2F2018%2F05%2F30%2FPerson%20search%20with%20natural%20language%20description%2F</url>
    <content type="text"><![CDATA[Li S, Xiao T, Li H, et al. Person search with natural language description[C]//Proc. CVPR. 2017. 1. Overview 1.1. Motivation existing methods mainly focus on searching persons with image-based or attribute-based queries (limitations for a practical usage) no person dataset or benchmark with textual description available In this paper, it studied person search with natural language description proposed Recurrent Neural Network with Gated Neural Attention mechanism (GNA-RNN) collect CUHK Person Description Dataset (CUHK-PEDES) 1.2. Image-Based Query person re-identification which requires at least one photo of the queried person being given. 1.3. Attribute-Based Query pre-defined semantic attributes which have limited capability of describing persons’s appearance. And labeling exhausted set of attributes is expensive. 1.4. Contribution person search with language is more practical for real-world investigate different solution. image caption, VQA, visual-semantic embedding proposed GNA-RNN 1.5. Related Work1.5.1. Language Dataset for Vision Flickr8K, Flickr30K MS-COCO Caption Visual Genome Caltech-UCSD Oxford-102 flowers 1.5.2. Deep Language Models for Vision Image Caption. NeuralTalk VQA. Stacked Attention Network Visual-Semantic Embedding 1.6. CUHK-PEDES Dataset 40,206 images of 13,003 persons from five existing person re-identification datasets (CUHK03, Market-1501, SSM, VIPER, CUHK01) 80,412 sentences for 40,206 images (2 sentences/img). details about appearance, actions, poses and interaction with other objects high-frequency word 1.6.1. User Study Language vs Attribute.language description are much precise and effective in describing persons than attributes. (top-1: 58.7% vs 33.3%; top-5: 92.0% vs 74.7%). Sentence Number and Length3 sentences achieve highest retrieval accuracy. the longer the sentences are, the easier for users to retrieve the correct images. Word Types nouns provide most information followed by the adjectives, while the verbs carry least information. 2. GNA-RNN key to build word-image relations. given each word, search related regions to determine whether the word with its context fit the image confidences of all relations should be weighted and then aggregate to generate the final sentence-image affinity 2.1. Visual Units Input. resize to 256x256 Output. 512 visual units pre-trained on our dataset for person classification based on person IDs during jointlt training, only update cls-fc1 and cls-fc2 2.2. Attension over Visual Units word are encoded into K-length one-hot vectors. K is the vocabulary size embedded one-hot vector and concat with image features through LSTM-FCs-Softmax, generate unit-level attention at each word summation of weighted attention image features summation of all T words 2.2.1. LSTM h. tanh 2.3. Word-Level Gates for Visual Units different words carry significantly different amount of information for obtaining language-image affinity. (“white” should be more important than the word “this”) unit-level attention can not reflect such difference learn word-level scalar gates at each word 2.4. Loss Function 2.5. Details SGD positive:negative=1:3 batch size 128 all FC are 512 units except gate-fc1 3. Experiments 3.1. Dataset training set. 11,003 persons; 34,054 images; 68,108 sentence descriptions testing set. 3,074 images of 1,000 persons validation set. 3,078 images of 1,000 persons 3.2. Comparison LSTM might have difficulty encoding complex sentences into a single feature vector word-by-word processing and comparison might be more suitable for the person search problems RNN is more suitable in processing natural language data 3.3. Ablation Study initial training affects the final performance a lot 3.4. The Number of Visual Unit more units might over-fit the dataset]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Cross-Modality</category>
        <category>Person Search</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>Cross-Modality</tag>
        <tag>Body</tag>
        <tag>Person Search</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(ECCV 2018) Exploring the Limits of Weakly Supervised Pretraining]]></title>
    <url>%2F2018%2F05%2F11%2FExploring%20the%20Limits%20of%20Weakly%20Supervised%20Pretraining%2F</url>
    <content type="text"><![CDATA[Mahajan D, Girshick R, Ramanathan V, et al. Exploring the limits of weakly supervised pretraining[C]//Proceedings of the European Conference on Computer Vision (ECCV). 2018: 181-196. 1. Overview 1.1. Motivation the large pre-training datasets are difficult to collect and annotate In this paper present a study transfer training with large network trained to predict hashtags on billions of social media images (Weak Supervised) training for large-scale hashtag prediction leads to excellent result 1.2. Hashtags Dataset billions of images “labeled” in the wild with social media hashtags (no manual labeling) hashtags are noisy and image distribution might be biased multi-label samples. if has k hashtags, then each hashtag is 1/k probability pre-processing utilize WordNet synsets to merge some hashtags into a single canonical form deduplication between training and test sets. compute R-MAC features use kNN (21) 1.3. Model ResNeXt-101 32xCd (classification). 32 groups, C group width Mask R-CNN (detection). Cross-Entropy. sigmoid and binary logistics get worse results 1.4. Training Methods Full Network Finetuning. view pre-training as sophisticated weight initialization Feature Transfer. pre-trained network as feature extractor, without updated, only trained classifierEnsure the experiments of paper on the standard validation sets are clean. 1.5. Dataset ImageNet CUB2011 Places365 1.6. Related Work JFT-300M. 300 million weakly supervised images, proprietary and not publicly visible Word or n-gram supervision. weaker than hashtag supervision 1.7. Observation maybe important to select a label space for the source task to match the target task current network are underfitting when trained on billions of images. lead to very high robustness to noise in hashtags training for large-scale hashtag prediction improves classification while at the same time possibly harming localization performance 2. Experiments 2.1. Hashtags Vocabulary Size pre-trained with 17k hashtags strongly outperforms 1.5k hashtags 17k span more objects scenes and fine-grained categories 2.2. Training Set Size when training network on billions of training images, current network architecture are prone to underfitting on IN-1k, pre-trained with 1.5k hashtags outperform other larger hashtags as the matching between hashtags and target classes disappear, larger hashtags outperform the effectiveness of the feature representation learned from hashtag prediction.train linear classifier on fixed feature are nearly as good as full network finetuning. For CUB2011. when training data is limited, the 1.5k hashtag dataset is better 2.3. The Noise of HashtagsRandomly replaced p% of the hashtags by hashtags obtained by sampling from the marginal distribution over hashtags. 2.4. The Sampling Strategy of Hashtags using uniform or square-root sampling lead to an accuracy improvement 2.5. Model Capacity with large-scale Instagram hashtag training, transfer-learning performance appears bottlenecked by model capacity 2.6. Most Helpful Class of Pre-training more concrete, more helpful 2.7. Detection when using larger pre-training data, detection is model capacity bound gains from Instagram pre-training mainly due to improved object classification rather than spatial localization]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Learning</category>
        <category>Weakly Supervised</category>
      </categories>
      <tags>
        <tag>Weakly Supervised Learning</tag>
        <tag>Hashtags</tag>
        <tag>Social Media Images</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(CVPR 2018) Two-stream convolutional networks for dynamic texture synthesis]]></title>
    <url>%2F2018%2F05%2F11%2FTwo-stream%20convolutional%20networks%20for%20dynamic%20texture%20synthesis%2F</url>
    <content type="text"><![CDATA[Tesfaldet M, Brubaker M A, Derpanis K G. Two-stream convolutional networks for dynamic texture synthesis[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018: 6703-6712. 1. Overview 1.1. MotivationTwo-stream hypothesis model the human visual cortex in terms of two pathways ventral stream. involved with object recognition dorsal stream. involved with motion processing In this paper, it proposed two-stream model for dynamic texture synthesis object recognition (appearance). encapsulate the per-frame appearance, pre-trained for object recognition optical flow prediction (dynamic). model dynamics, pre-trained for optical flow prediction combine the texture appearance from one texture with the dynamics of another to generate entirely novel dynamic textures first work to demonstrate this form for style transfer 1.2. Related Work Two General Approaches non-parametric sampling statistical parametric model Gram Matrix capture the style information, ignore the spatial location [b, c, h, w]→ [b, c, hw] &amp; [b, hw, c]→ [b, c, c] 1.3. Future Work extent the idea of a factorized representation into feed-forward generative networks 2. Method Synthesizing a dynamic texture is formulated as an optimization problem with the objective of matching the activation statistics. 2.1. Appearance Stream N_l. the number of filter M_l. the number of spatial location t. time t Average over the target frames (as groud-truth). T. the number of target frames k. spatial location i, j. the index of filter each single frame to be synthesised (as prediction). The Loss Function L_{app}. the number of layers used to compute Gram Matrices T_{out}. the number of frames being generated in the output 2.2. Dynamic Stream input. a pair of consecutive greyscale images T-1. T frames group into (T-1) pairs The Loss Function 2.3. Overall memory increases as the frames grows separate the sequence into sub-sequenceinitialize the first frame of a sub-sequence as the last frame from the previous sub-sequence and keep it fixed. 3. Experiments 3.1. w/o Dynamic Stream 3.2. Loss of Flow Decode Layer vs Concat Layer concatenation layer activation is far more effective than the flow decode layer 3.3. Failure Example fail to capture spatially-inconsistent dynamics fail to capture textures with spatially-variant appearance]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Image Processing</category>
      </categories>
      <tags>
        <tag>Image Processing</tag>
        <tag>Image Generation</tag>
        <tag>Dynamic Texture Synthesis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(CVPR 2018) Who Let The Dogs Out Modeling Dog Behavior From Visual Data]]></title>
    <url>%2F2018%2F05%2F11%2FWho%20Let%20The%20Dogs%20Out%20Modeling%20Dog%20Behavior%20From%20Visual%20Data%2F</url>
    <content type="text"><![CDATA[Ehsani K, Bagherinezhad H, Redmon J, et al. Who Let The Dogs Out? Modeling Dog Behavior From Visual Data[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018: 4051-4060. 1. Overview 1.1. Motivation most cv task related to visual intelligence In this paper, it directly model a visullay intelligent agent input visual information, predict actions of the agent DECADE dataset. ego-centric videos from a dog’s perspective how the dog acts how the dog plans learn from a dogthe task of walkable surface estimation and scen classification by using this dog modeling task as representation learning. 1.2. Definition of the Problems understanding visual data to the extent that an agent can take actions and perform tasks in the visual world 1.3. Dataset mount Intertial Measurement Units (IMU) on the joints and body of the dog. record the absolute position and calculate the relative angle of the dog’s main limbs and body (angular displacement represented as a 4 dimensional quaternion vector) mount a camera on dog’s head. (380 video clips; 24500 frames, 21000 for training, 1500 for validation and 2000 for testing; various indoor and out door scenes, more than 50 different location) the differences of the angular displacements between two consecutive frames represents the action of the dog in that timestep connect all IMU to the same embedded system (Raspberry pi 3.0) the rate of the joint movement readings and video frames are different. perform interpolation and averaging to compute the absolute angular orientation for each frame use K-means clustering to quantize the action space. formulate the problems as classification rather than regression 1.4. Related Work Visual Prediction. (activity forecast, people intent) Sequence to Sequence Models Ego-centric Vision Ego-motion estimation Action Inference &amp; Planning Inverse Reinforcement Learning Self-supervision 1.5. Act like a Dong input. a series of frame (1~t) output. a series action (t+1~N) ResNet’s weights are shared. 1.6. Plan like a Dog Input. two frames (1, N) Output. a series action (2, N-1) 1.7. Learn from a DogCompare the pre-trained ResNet-18 (input two frames [t, t+1], predict the action between [t, t+1]) on DECADE and ImageNet. 1.8. Future Work variety input. touch, smell collect data from multiple dogs. evaluate generation across dogs 1.9. Experiments1.9.1. Metric class accuracy perplexity 1.9.2. Learning to Act 1.9.3. Learning to Plan 1.9.4. Learning from Dog]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Prediction</category>
      </categories>
      <tags>
        <tag>Prediction</tag>
        <tag>Behavior</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(ICLR 2015) Explaining and harnessing adversarial exampless]]></title>
    <url>%2F2018%2F05%2F11%2FExplaining%20and%20harnessing%20adversarial%20examples%2F</url>
    <content type="text"><![CDATA[Goodfellow I J, Shlens J, Szegedy C. Explaining and harnessing adversarial examples[J]. arXiv preprint arXiv:1412.6572, 2014. 1. Overview 1.1. Motivation ML and DL model misclassify adversarial examples. Early explaining focused on nonlinearity and overfitting generic regularization strategies (dropout, pretraining, model averaging) do not confer a significant reduction of vulnerability to adversarial examples In this paper explain it by their linear nature fast gradient sign method to generate adversarial examples adversarial training can provide an additional regularization benefit beyond that provided by dropout 1.2. Related Work the same adversarial examples is often misclassified by a variety of classifiers with different architecture or trained on different subsets of the training data trained on adversarial examples can regularize the model 1.3. Summary adversarial examples can be explained as a property of high-dimensional dot products the generation of adversarial examples across different models. different models learn similar function when trained to perform the same task adversarial training can result in regularization, further than dropout models optimized easily are easy to perturb linear models lack the capacity to resist adversarial perturbation. only structures with hidden layer should bt trained to resist it ensembles are not resistant to adversarial examples 1.4. Linear Explanation of Adversarial Examples precision of an individual input feature is limiteddiscard all information (η) below 1/255 classify x and x‘ to the same class when the elements of η is less than precision ε consider the dot product between a weight vector w and an adversarial example x’ the adversarial perturbation (η) causes the activation to grow by maximize the growth, when assume w has n dimension, and the average magnitude of an element in w is m, the growth will be grow linearly with n. when dimension n is large, it will make greatly effects 1.5. Linear Perturbation of Non-linear Models 1.5.1. Fast Gradient Sign Method Θ. parameters of model x. Input y. output J. cost function 1.6. Adversarial Training of Deep Network deep network at least can represent function to resist adversarial perturbation, but shallow linear model can not The adversarial training on maxout network does not work well. After make the model larger it works well.]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Adversarial Attack</category>
      </categories>
      <tags>
        <tag>Adversarial Attack</tag>
        <tag>FGSM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(CVPR 2015) Deep neural networks are easily fooled:High confidence predictions for unrecognizable images]]></title>
    <url>%2F2018%2F05%2F11%2FDeep%20neural%20networks%20are%20easily%20fooled%3A%20High%20confidence%20predictions%20for%20unrecognizable%20images%2F</url>
    <content type="text"><![CDATA[Nguyen A, Yosinski J, Clune J. Deep neural networks are easily fooled: High confidence predictions for unrecognizable images[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2015: 427-436. 1. Overview 1.1. MotivationA recent study revealed that changing an image in a way imperceptible to humans can cause a DNN to label the image as something else entirely. In this paper show that easy to produce images that are urecognizable to humans, but DNN 99.99% believe it is a recognizable obj Evolution Algorithmdirectly (row1 ) and indirectly (row 2) Gradient Ascent 1.2. Procedure 1.3. Dataset MNIST ImageNet 1.4. Network AlexNet LeNet CaffeNet 1.5. Discussion the are a discriminative model allocates to a class may be much larger than the area occupied by training examples for that class Application. security camera (face, voice), search engine rankings (image’s background), driverless car (generate fooling images) 2. Evolution 2.1. Directly Encoding unrecognizable to human. uniform random initialize each pixel within [0, 255] each pixel has 10% chosen to be mutated, rate decay every 1000 generation polynomial mutation operator with a fixed mutation strength of 15 to mutate chosen pixel 2.2. Indirectly Encoding recognizable to human. producing image contain compressible patterns (symmetry and repetition) based on Compositional Pattern-Producing Network (CPPN). take pixel as input, and output a new pixel 2.3. Gradient Ascent maximize the softmax output for classes via gradient ascent to find image employ L2-regularization to produce images with some recognizable features of classes (dog face, fox ears) 3. Experiments 3.1. Directly Encoding on ImageNetLess successful at producing high-confidence images on large dataset compare with MNIST. (larger dataset→ less overfit→ more difficult to fool) 3.2. Indirectly Encoding on ImageNet More successful. Similar images for closely related categories. 3.3. Generalization3.3.1. Same Architecture &amp; Different Initialization many images fooling A also fool B still some images different 3.3.2. Different Architecture many images generalize across DNN architecture 3.4. Train Network to Recognize Fooling Images (MNIST) similar performance to without fooling images (ImageNet) on the contrary]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Adversarial Attack</category>
      </categories>
      <tags>
        <tag>Adversarial Attack</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(ECCV 2018) Group normalization]]></title>
    <url>%2F2018%2F05%2F11%2FGroup%20normalization%2F</url>
    <content type="text"><![CDATA[Wu Y, He K. Group normalization[C]//Proceedings of the European Conference on Computer Vision (ECCV). 2018: 3-19. 1. Overview 1.1. Motivation Normalization along batch dimension introduces problems: when batch size smaller, BN’s error increase rapidly BN helps to converge (stochastic uncertainty of batch statistics acts as a regularizer, benifit generalization), but worse for small batch SIFT, HOG. group-wise feature and group-wise normalization BN’s statistics are computed for each GPU, not broadcast across all GPU In this paper, it proposed Group Normalization (GN) independent of batch divide channels into groups and compute μ, σ of each group 1.2. Related Works1.2.1. Normalization Local Response Normalization (LRN). compute the statistics in a small neighbourhood for each pixel BN Layer Normalization (LN) Instance Normalization (IN) Weight Normalization (WN)LN, IN, WN. independent with batch.LN, IN. successful in RNN and GAN model. 1.2.2. Addressing Small Batch Batch Renormalization (BR). two parameters constraint the μ,σ of BN Synchronized BN. μ,σ computed across multiple GPUs 1.2.3. Group-wise Computation group convolution. AlexNet ResXNet depth-wise. MobileNet, Xception ShuffleNet 1.3. Dataset ImageNet. Classification COCO. obj detection, Segmentation Kinectics. Video Classification 1.4. Group NormalizationRelation in Group horizontal orientation frequency shape illumination texture general formulation BN (along NHW) LN (along HW) GN (along HWC_group) G. group number; C/G. channel per group (G=1)→ LN (assume all channels make similar contribution, more stricted than GN) (G=C)→ IN (not exploit channel dependence) 1.5. Future Works investigate GN in reinforcement learning (RL) 2. Experiments 2.1. Ablation Study2.1.1. Batch Size (Classification) Linear Learning Rate Scaling Rule. LR 0.1 for size 32, LR 0.1N/32 for size N. 2.1.2. Batch Size (Video Classification) 2.1.3. Group &amp; Channel Number 2.1.4. Distribution 2.2. Comparison2.2.1. Classification 2.2.2. Detection &amp; Segmentation replace BN* with GN, when fine-tuneing weight decay of 0 for γ and β is important for good detection results the distribution of RoIs batches sampled from the same image is not i.i.d. degrades BN’s estimation 2.2.3. Video Classification]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Architecture</category>
        <category>Component</category>
      </categories>
      <tags>
        <tag>Component</tag>
        <tag>Group Normalization</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(2017) Paying more attention to attention:Improving the performance of convolutional neural networks via attention transfer]]></title>
    <url>%2F2018%2F05%2F11%2FPaying%20more%20attention%20to%20attention%3A%20Improving%20the%20performance%20of%20convolutional%20neural%20networks%20via%20attention%20transfer%2F</url>
    <content type="text"><![CDATA[Zagoruyko S, Komodakis N. Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer[J]. arXiv preprint arXiv:1612.03928, 2016. 1. Overview 1.1. Motivation different observers with different knowledge, goals lead to different attentional strategies can a teacher network improve the performance of another student network by providing to it information about where it looksIn this paper, it improves the student network by forcing it to mimic the attention maps of a powerful teacher network. activation-based and gradient-based attention map 1.2. Contribution attention mechanism to transfer knowledge activation-based (better and can combine with knowledge distillation) and gradient-based spatial attention maps 1.3. Related Work Attention Mechanism image caption VQA weakly-supervised object localization classification Gradient-Based Knowledge Distillation shallow networks has been shown to be able to approximate deeper ones without loss in accuracy Network after a certain depth, the improvements came mostly from increased capacity of the networks (parameter number) 16 layer wider ResNet can learn as good or better as very thin 1000 layers one 1.4. Dataset ImageNet. classification, localization COCO. obj detection, face recognition amd fine-grained recognition 2. Attention Transfer 2.1. Activation-Based Attention Transfer get the attention map from the feature maps first layer. activate for low-level gradient points middle level. high for discriminative regions (eyes, wheels) top level. reflects full obj 2.1.1. three methods stronger networks hace peaks in attention where weak networks don’t (F_sum)^p put more weight (than F_sum) to spatial locations the correspond to the neurons with the highest activation (F_max)^p only consider one of the max rather than sum of all 2.1.2. Cases of Student and Teacher Networks same depth different depth 2.1.3. Loss Function L(W, x). task loss I. pairs of student-teacher attention maps The normalization of attention maps is important for student training Attention transfer can also be combined with knowledge distillation. 2.2. Gradient-Based Attention TransferIf small changes at a pixel can have a large effect on the network output then it is logical to assume that the network is “paying attention” to that pix flip invariant version 3. Experiments 3.1. Attention-Based trained with all transfer loss better than only one transfer loss F_sum better than F_max 4. Compared with Knowledge Distillation]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Learning</category>
        <category>Transfer Learning</category>
      </categories>
      <tags>
        <tag>Transfer Learning</tag>
        <tag>Attention Map</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(ECCV 2014) Visualizing and understanding convolutional networks Network for Point Cloud Analysis]]></title>
    <url>%2F2018%2F05%2F11%2FVisualizing%20and%20understanding%20convolutional%20networks%2F</url>
    <content type="text"><![CDATA[Zeiler M D, Fergus R. Visualizing and understanding convolutional networks[C]//European conference on computer vision. Springer, Cham, 2014: 818-833 1. Overview 1.1. Factors of DL big data GPU model regularization strategy (dropout, …)In this paper, it proposed a visualization technique to explore the network unpooling the horizontal and vertical of Conv kernel 1.2. Dataset ImageNet 2012 1.3. Visualization Method Unpooling→Rectification→Filtering (horizontal and vertical corresponding Conv kernels) 1.4. Network Change based on AlexNet. 1.5. Feature Visualization Layer2. corners, edge/color conjunctions Layer3. texture (more complex invariances) Layer4. significant variation (more class-specific) Layer5. entire obj with pose variation 1.6. Feature Evolution during Training lower layers. converge within a few epochs upper layers. converge after a considerable epoch 1.7. Feature Invariance small transformation dramatic effect in the first layer, less impact at the top layer not invariant to rotation, except for the obj with rotational symmetry 1.8. Architecture Selection (b). mix of extremely high and low frequency information, with little converage of the mid frequency, and some dead featuresreduce 11x11 kernel size to 7x7 (d). aliasing artifacts caused by large stride 4change stride 4 to stride 2 1.9. Occlusion Sensitivity 1.10. w/o Pre-trained]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Visualization</category>
      </categories>
      <tags>
        <tag>Visualization</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(ICCV 2017) SO-Net:Self-Organizing Network for Point Cloud Analysis]]></title>
    <url>%2F2018%2F04%2F29%2FSO-Net%3A%20Self-Organizing%20Network%20for%20Point%20Cloud%20Analysis%2F</url>
    <content type="text"><![CDATA[Li J, Chen B M, Hee Lee G. So-net: Self-organizing network for point cloud analysis[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2018: 9397-9406. 1. Overview 1.1. Motivation The computation of rasterizing 3D data into voxel Point cloud can be easily acquired with popular sensor (RGB-D, LiDAR, camera with the help with Structure-from-Motion) PointNet. can not handle local feature extraction PointNet++. not reveal the spatial distribution of the input cloud Kd-Net. lack of overlapped receptive fields In this paper, it proposde So-Net model the spatial distribution of point cloud by Self-Organizing Map (SOM) overlapped receptive field. conducting by point-to-node KNN search hierarchical feature extraction permutation invariant Experiments. reconstruction, classification, part segmentation, shape retrieval 1.2. Contribution explicity utilize the spatial distribution of point cloud overlapped receptive field pre-trained point cloud autoencoder faster speed 1.3. Related Work voxel grid orientation pooling project 3D into 2D VAE sparse method sepctral ConvNet render 3D into multi-view 3D and view-pooling Kd-Net PointNet (++) 1.4. Dataset MNIST ModelNet10, ModelNet40 ShapeNetPart 2. So-Net 2.1. Permutation Invariant SOM produce 2D representation of input point cloud. m x m nodes, m∈[5, 11] unsupervised trained 2.1.1. The reason of not permutation invariant training result highly related to the initial nodes per-sample update rule depends on the order of the input points 2.1.2. Solution fixed initial nodes. disperse the node uniformly inside a unit ball batch update (matrix operation highly efficient on GPU). based on all points, instead of once per point 2.2. Encoder given the ouput of SOM, for each point p, seach KNN SOM nodes s N input points M nodes kN points (k control the overlapped) normalize each point based on it’s node kN normalized points processed by shared FCs maxpool each M mini cloudsM points 2.3. Feature Aggregation point feature→ node feature node feature→ global feature 2.4. Isolated Node outside the Point Cloud set node features to zero 2.5. Segmentation combine the point, node and global features found middle fusion with avg pooling is effective 2.6. AutoEncoder stacking series of FC will heavy memory and computation, if point is large contains FC branch (details) and Conv branch (main body) upconv. 3x3 Conv + NN upsample (more effective than DeConv) conv2pc. 1x1 Conv + 1x1 Conv coarse-to-fine Chamfer loss. point number between input and output need not equal P_s, P_t: input and recovered cloud 3. Experiments 3.1. Details 8x8 SOM; k=3 batch size 8 each layer followed by (BN + ReLu) 3.2. Dataset Augmentation Gaussian Noise (0, 0.01) to point coordinate and surface normal vectors Gaussian Noise (0, 0.04) to SOM node scaling point clouds, surface normal vectors and SOM node by a factor from an uniform distribution (0.8, 1.2) 3.3. Reconstruction 3.4. Classification pre-trained can improve accuracy overfitting when too many layers (SOM grouping + PointNet) 3.5. Segmentation]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Point Cloud</category>
      </categories>
      <tags>
        <tag>Segmentation</tag>
        <tag>Classification</tag>
        <tag>Point Cloud</tag>
        <tag>SO-Net</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(ICCV 2017) Escape from cells:Deep kd-networks for the recognition of 3d point cloud models]]></title>
    <url>%2F2018%2F04%2F29%2FEscape%20from%20cells%3A%20Deep%20kd-networks%20for%20the%20recognition%20of%203d%20point%20cloud%20models%2F</url>
    <content type="text"><![CDATA[Klokov R, Lempitsky V. Escape from cells: Deep kd-networks for the recognition of 3d point cloud models[C]//2017 IEEE International Conference on Computer Vision (ICCV). IEEE, 2017: 863-872. 1. Overview 1.1. Motivation rasterize 3D models onto uniform voxel grids lead to large memory footprint and slow process time there exist a large number of indexing strucutres (kd-tree, oc-trees, binary spatial partition tree, R-trees and constructive solid geometry) In this paper, it proposed Kd-Networks divide the point cloud to construct the kd-tree perform multiplicative transformation and share parameters of these transformation (mimic ConvNet) not rely on grids and avoid poor scaling behavior 1.2. Related Works 3D Conv (+ GAN) 2D Conv (2D projection of 3D obj) spectral Conv PointNet RNN OctNet (Oct-Trees) Graph-based ConvNet 1.3. Dataset classification. ModelNet10, ModelNet40 shape retrieval. SHREC’16 shape part segmentation. ShapeNet part dataset 2. Network 2.1. InputRecur to divide the point clouds into two equally-sized subsets. get N - 1 nodes and each divide direction d_i (along x, y or z). N. the fixed size of point cloud (sub-sample or over-sample) d_i. divide direction of each level l_i. the level of tree c_1(i) = 2i, c_2(i) = 2i + 1. children of ith node 2.2. Processing Data with Kd-NetGiven a kd-tree, compute the representation v_i of each node. In the ith level, apply the sharing layer to the same divide direction node. v_i. the representation of ith node φ. Relu []. concate W, b. parameters of the layer in ith level, d_i direction (dimension: 2m_{l+1} x m_l, m_l) 2.3. Classification 2.4. Shape Retrieval output a descriptor vector (remove trained classifier of Classification) histogram loss. also can use Siamese loss or triplet loss 2.5. Segmentation mimic encoder-decoder (Hourglass) skip connection 2.6. Properties Layerwise Parameter Sharing CNN. share kernels for each localized multiplication Kd-Net. share kernel (1x1) for points with same split direction in same level Hierarchical Representation Partial Invariance to Jitter split direction Non-invariance to Rotation Role of kd-tree Structure Kd-tree determine the the combination order of leaf representation Kd-tree can be regarded as a shape descriptor 2.7. Details normalize 3D coordinates. [-1, 1]^3 and put the origin at centroid data augmentation. perturbing geometric transformation, inject randomness into kd-tree construction (direction probability) γ=10. 3. Experiments 3.1. Details MNIST→2D Point Cloud. point of the pixel center 3D Point Cloud. sample faces→ sample point from face Self-ensemble in test time Augmentation TR. translation long axis ±0.1 AS. anisotropic rescaling DT. deterministic tree RT. randomized tree 3.2. Classification 3.3. Ablation 3.4. Shape Retrieval 20 rotations→pooling→FC 3.5. Part Segmentation duplicated random sample with an addition of a small noise. help with rare calss during test, predict on upsampled cloud and then obtain the mapping of original points low memory footprint &lt; 120 MB]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Point Cloud</category>
      </categories>
      <tags>
        <tag>Segmentation</tag>
        <tag>Detection</tag>
        <tag>Point Cloud</tag>
        <tag>Kd-Net</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(CVPR 2018) Frustum pointnets for 3d object detection from rgb-d data]]></title>
    <url>%2F2018%2F04%2F29%2FFrustum%20PointNets%20for%203D%20Object%20Detection%20from%20RGB-D%20Data%2F</url>
    <content type="text"><![CDATA[Qi C R, Liu W, Wu C, et al. Frustum pointnets for 3d object detection from rgb-d data[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018: 918-927. 1. Overview Previous method focus on images or 3D voxels Treat RGB-D data as 2D maps for CNN Learning in 3D space can better exploit the geometric and topological structure of 3D space and apply transformation In this paper, it proposed Frustum PointNet operate on raw point clouds by RGB-D scans leverage both 2D detection and 3D object localization key challenge. efficientlypropose possible localtions of 3D obj in 3D space 2D proposal→frustum proposal→segmentation→3D box estimation coordinate normalization 1.1. Related Works Front View Image Based Methodsrepresent depth data as 3D maps Bird’s Eye View Based MethodsMV3D. project LiDAR point cloud to bird’s eye view and train RPN for 3D bounding box proposal 3D Based Methods Deep Learning on Point Clouds 1.2. Problem Definitions depth data. obtained from LiDAR or indoor depth sensors and represented as a point cloud the projection matrix is known. can get a 3D frustum from a 2D image region 3D box is parameterized by size (h, w, l), center (c_x, c_y, c_z) and orientation (Θ, φ, ψ).only consider heading angle Θ in this paper. 1.3. Dataset KITTI (outdoor). RGB + LiDAR point cloud (sparse due to distence) SUN-RGBD (indoor). RGB-D (dense)general framework to sparse cloud and dense cloud. 2. Frustum PointNets 2.1. Frustum ProposalThe resolution of data produced by moist 3D sensors (especially real-time depth sensors) is still lower than RGB image from commodity camera. 2D RGB detector. Fast R-CNN, FPN, focal loss with known camera projection matrix, 2D box can be lifted to frustum rotate. center axis of frustum if orthogonal to the image plane 2.2. 3D Instance Segmentation 2.2.1. V1 PointNet 2.2.2. V2 PointNet++ directly regress 3D object location from a depth map using 2D CNN is not easy, as occluding objects and background clutter segmentation (binary classification of pixel level) in 3D point cloud is much more natural leverage the semantics from 2D detector (one-hot class vector)segmentation network can use this prior to find geometries of that category. coordinate normalization. transform the point cloud by subtracting XYZ values of centroid mask the input frustum 2.3. Amodal 3D Box Estimation2.3.1. T-Net the origin of the mask coordinate frame may be far from the amodal box center STN (no direct supervision) vs T-Net (explicitly supervise) 2.3.2. Box Estimation PointNetV1 PointNet V2 PointNet++ box center residual prediction. combined with the previous center residual from the T-Net and the masked points’ centroid to recover an absolute center pre-defined NS size templates (3:height, width, length) and NH equally split angle (Θ) bins (NS scores for size, NH socres for heading) output dimension. 3(center point) + 4xNS + 2xNH 2.4. Multi-task Loss L_{c1-reg}. center of T-Net L_{c2-reg}. center of box estimation net L_{h-cls}, L_{h-reg}. heading angle prediction L_{s-cls}, L_{s-reg}. size prediction L_{corner}. corner loss for joint optimization pf box parameters Optimized for final 3D box accuracy (center, size and heading) have separate loss terms. And they should be jointly optimized→corner positions are jointly determined by center, size and hearding. for each of NS x NH box only foucs on the gt size/heading class sum of the distance between the eight corners of prediction and gt box To avoid large penalty from flipped heading estimation, further compute p from the flipped gt box and use minimum** of them. 3. Experiments 3.1. Comparison 3.2. Ablation Study3.2.1. 2D vs 3D contains clutter and background 3.2.2. contains clutter and background Frustum rotation and mask centroid subtraction are critical 3.2.3. Loss Function 3.2.4. PointNet Version 3.3. Failure Case inaccurate pose and sparse cloud (less than 5 points) multiple instances from the same category in a frustum 2D detector misses objects due to dark light or strong occlusion]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Point Cloud</category>
      </categories>
      <tags>
        <tag>Localization</tag>
        <tag>Segmentation</tag>
        <tag>Detection</tag>
        <tag>Point Cloud</tag>
        <tag>Frustum PointNets</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(NIPS 2017) Pointnet++:Deep hierarchical feature learning on point sets in a metric space]]></title>
    <url>%2F2018%2F04%2F29%2FPointnet%2B%2B%3A%20Deep%20hierarchical%20feature%20learning%20on%20point%20sets%20in%20a%20metric%20space%2F</url>
    <content type="text"><![CDATA[Qi C R, Yi L, Su H, et al. Pointnet++: Deep hierarchical feature learning on point sets in a metric space[C]//Advances in Neural Information Processing Systems. 2017: 5105-5114. 1. Overview PointNet aggregates all individual point features to a global point cloud and does not capture local structures Point sets are usually sampled with varying densities In this paper, it proposed PointNet++ Hierarchical. learn local features with increasing contextual scales Adaptively combine features from multiple scales 1.1. Two Issues how to generate the partitioning of the point set (neighbourhood ball with centroid, farthest point sampling,FPS)small neighbourhood may consist of too few points due to sampling deficiency, which might be insufficient to allow PointNet to capture patterns robustly. how to abstract sets of points or local features through the local feature learner (recursively PointNet) 1.2. Related Work Hierarchical Feature Learning Deep Learning of Unordered Sets Point Sampling 3D representation. (volumetric grids and geometric graphs) 1.3. Datasets MNIST (2D objects) ModelNet40 (3D) SHREC15 (3D) ScanNet (real 3D scenes) 2. Network 2.1. Sampling LayerUsing farthest point sampling (FPS) to get the the coordinate of N’ centroids’. Input: [N x d], the coordinates of point set Output: [N’ x d], the coordinates of centroid points 2.2. Grouping LayerUsing ball query (or KNN) to group the point set based on centroid point set. Input: [N’ x d], centroid point set; [N x (d+C)], point set Output: [N’ x K x (d+C)], grouped point set Ball query guarantees a fixed region scale, but KNN can’t The coordinates of points in a local region are firstly translated into a local frame relative to the centroid point (to capture point-to-point relations in the local region) 2.3. PointNet Layer Input: [N’ x K x (d+C)] Output: [N’ x (d+C’)] 2.4. Robust under Non-Uniform Sampling DensityFeatures learned in dense data may not generalize to sparsely sampled region. In contrast, the same. 2.4.1. Sample Method (random input dropout)[during training] for each training point set, draw a dropout rate Θ uniformly sampled from [0, p], p≥1 for each point, randomly drop a point with probability Θ In practise, set p=0.95 to avoid empty point set. various sparsity (induced by Θ) varying uniformity (induced by randomly dropout) [during testing] keep all available points. 2.4.2. Solutionseach abstraction level extracts multiple scales of local patterns and combine them intelligently according to local point densitie Multi-scale Grouping (MSG) Features at different scales (processed by PointNet) are concat to form a multi-scale feature Drawbacks. computationally expensive (the number of centroid points is quite large at the lowest level) Multi-resolution Grouping (MRG) Left vector (a). processed by the set abstraction (sampling-grouping-PointNet, L-1 level-group-PointNet) Right vector (b). directly processed by the PointNet (L-1 level-PointNet) low densities. (a) may be less reliable than (b) the subregion in computing (a) contains even sparser points and suffers more from sampling deficiency. high densities. (a) better (a) provides information of finer details sincepossesses the ability to inspect at higher resolutions recursively in lower levels. 2.5. Point Feature Propagation for SegmentationSet abstraction layer will subsampled the original point set, but the segmentation task need to obtain point features for all the original points. Using hierarchical propagation strategy with distance based interpolation (inverse distance weighted average based on KNN) 3. Experiments 3.1. Classification Comparison Robust to Sampling Density Variation 3.2. Segmentation Comparison]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Point Cloud</category>
      </categories>
      <tags>
        <tag>Segmentation</tag>
        <tag>Detection</tag>
        <tag>Point Cloud</tag>
        <tag>Pointnet++</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(CVPR 2017) Pointnet:Deep learning on point sets for 3d classification and segmentation]]></title>
    <url>%2F2018%2F04%2F29%2FPointnet%3A%20Deep%20learning%20on%20point%20sets%20for%203d%20classification%20and%20segmentation%2F</url>
    <content type="text"><![CDATA[Qi C R, Su H, Mo K, et al. Pointnet: Deep learning on point sets for 3d classification and segmentation[J]. Proc. Computer Vision and Pattern Recognition (CVPR), IEEE, 2017, 1(2): 4. 1. Overview Most method based on 3D voxel grids or collections of images, unnecessary and computation cost Point clounds are simple and unified structures, invariant to permutations. In this paper, it proposed PointNet directly consumes unordered point clouds (xyz coordinate plus color etc) using symmetric function (maxpooling) using STN to aligned points learn critical points set (contribute to the results of maxpooling) and upper-bounded shapes (all point has nothing to do with maxpooling) 1.1. Contribution design PointNet for 3D point set exploit to classification, segmentation empirical and theoretical analysis on stability and efficiency illustrate 3D features computed by the selected neurons 1.2. Related Work1.2.1. Point Cloud Feature handcrafted 1.2.2. Deep Learning on 3D Data Volumetric CNN FPNN Vote3D Multiview CNN Spectral CNN Feature-based DNN 1.2.3. DL on Unordered Set1.3. Properties of Point Sets Unorderd. Invariant to permutation Interaction among points. neighbouring points form a meaningful subset Invariant under transformation. not modify the global point cloud category and segmentation of the points. 1.4. Network 1.4.1. Three key modules maxpooling –&gt; unordered a local and global information combination structureSegmentation requires a combination of local and global knowledge. two joint alignment networksTransformation matrix in the feature space has much higher dimension (64*64) which greatly increase the difficulty of optimization. So constrain it to be close to orthogonal matrix 1.5. Formulation using g (maxpooling + single variable function) and h (MLP) to approximate f, so For two sets S and S’. the their distance is small, the mapping f of them is also similar And f can be approximated by PointNet If T (input corruption) contains the critical point set of S, it is unchanged. Based on this, if T contains some noise (not beyond upper-bounded shape), it is also unchanged critical point set only contains a bounded number of points (at most each K points contribute to one dimension of K dimensions global feature) 1.6. Dataset Classification. ModelNet40 Part Segmentation. ShapeNet part dataset Semantic Segmentation. Stanford 3D semantic parsing dataset 2. Experiments 2.1. Classification uniformly sample 1024 points on mesh faces according to face area and normalize them into a unit sphere 2.2. Part Segmentation 2.3. Semantic Segmentation &amp; Detection baseline. handcrafted point features 2.4. Order-Invariant Methods 2.5. Alignment 2.6. Robust 2.7. Time and Space]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Point Cloud</category>
      </categories>
      <tags>
        <tag>Segmentation</tag>
        <tag>Classification</tag>
        <tag>Point Cloud</tag>
        <tag>Pointnet</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(CVPR 2018) Cascaded Pyramid Network for Multi-Person Pose Estimation]]></title>
    <url>%2F2018%2F04%2F29%2FCascaded%20Pyramid%20Network%20for%20Multi-Person%20Pose%20Estimation%2F</url>
    <content type="text"><![CDATA[Chen Y, Wang Z, Peng Y, et al. Cascaded pyramid network for multi-person pose estimation[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018: 7103-7112. 1. Overview Challenge cases of multi-person pose estimation, such as occluded keypoints invisible ketpoints complex background In this paper, it proposed Cascaded Pyramid Network (CPN) GlobalNet. localize simple keypoint RefineNet. explicitly handle hard keypoint (online hard keypoints mining) Top-down pipeline. generate human box based on detector first 1.1. Contribution CPN Explore the effects of various factors in top-down pipeline 1.2. Related Works Classical. pictorial structure, graphical model, tree structure and hand-crafted feature Multi-Person. top-down and bottom-up Single-Person. regressors, heatmap and score map Human Detection. one stage and two stages 1.3. Dataset&amp;Metrics MS COCO. trainval (57k images and 150k person instances), minival (5k images), test-dev (20k) and test-challenge (20k). OKS-based mAP. (object keypoints similarity) 2. Architecture 2.1. GlobalNet Top-Down: C2, C3, C4, C5. C2,C3. High spatial resolution for localization but low semantic information for recognition C4,C5. More semantic information but low spatial resolution Drawbacks: the hard keypoint requires more context rather than the appearance feature nearby 2.2. RefineNet Stack more bottleneck blocks in deeper layers (small spatial) explicitly select the hard keypoint online (top M) based on training loss and BP the loss from the them. 3. Experiments 3.1. Data Process box 256:192 and resize to 256x192 flip, rotation (-40~+40), scale (0.7~1.3) 3.2. Test ensemble mechanism 3.3. Ablation Study3.3.1. NMS strategysoft-NMS surpasses hard-NMS. 3.3.2. Detector PerformanceAP less important for pose estimation. 3.3.3. Hard Keypoints NumberM = 8 works well. 3.3.4. With\Without 3.3.5. Concatenation 3.3.6. DilationDilation increase AP and FLOPs. 3.3.7. Image Size 3.4. Comparison]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Body</category>
        <category>Pose</category>
      </categories>
      <tags>
        <tag>Pose</tag>
        <tag>Human Body</tag>
        <tag>CPN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(CVPR 2017) Annotating object instances with a polygon-rnn]]></title>
    <url>%2F2018%2F04%2F29%2FAnnotating%20object%20instances%20with%20a%20polygon-rnn%2F</url>
    <content type="text"><![CDATA[Castrejon L, Kundu K, Urtasun R, et al. Annotating object instances with a polygon-rnn[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2017: 5230-5238. 1. Overview 1.1. Motivation Most current method treat object segmentation as a pixel-labeling problem In this paper, it cast this segmentation task as polygon prediction proposed Polygon-RNN architecture for semi-automatic annotation speed up the annotation process by 4.7 times in Cityscapes dataset 1.2. Polygon-RNN 1.2.1. Input image crop vertices sequence 1.2.2. Feature Extractor modified VGG (boundary) low-level about the edge and corner (see object) high-level about the semantic information exploit bilinear interpolation or max-pooling before concat 1.2.3. RNN ConvLSTM. preserve spatial information; reduce parameters compared to FC-RNN 1.3. Related Work1.3.1. Semi-automatic Annotation GrabCut. exploit annotation GrabCut + CNNMost define a graphical model at the pixel-level which are hard to incorporate shape prior.1.3.2. Annotation Tool1.3.3. Instance Segmentation pixel-level explicit box or patch produce polygon 1.4. Training Detail cross-entry at each time step of RNN feed t-1, t-2 gt to prediction t step for the first vertex prediction. train another CNN using multi-task loss 250 ms/img about inference time set chessboard distance threshhold T. If distance large than T, simulated human correction 1.5. Dataset Cityscapes KITTI 1.6. Data Process perform polygon simplification with zero error in the quantized grid. eliminate vertices which are in a line or fall into same grid 1.7. Data Augmentation random flip enlarged box 10%~20% random select starting vertex 2. Experiments 2.1. Metrics IoU number of clicks 2.2. Step Limitation set max step to 70 instance-wise. treat the entire instance as an example component-wise. treat each component as a single example 2.3. Result]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Annotation</category>
      </categories>
      <tags>
        <tag>Annotation Algorithm</tag>
        <tag>Semi-Automatic</tag>
        <tag>Polygon-RNN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(CVPR 2018) Shufflenet:An extremely efficient convolutional neural network for mobile devices]]></title>
    <url>%2F2018%2F04%2F29%2FShufflenet%3A%20An%20extremely%20efficient%20convolutional%20neural%20network%20for%20mobile%20devices%2F</url>
    <content type="text"><![CDATA[Zhang X, Zhou X, Lin M, et al. Shufflenet: An extremely efficient convolutional neural network for mobile devices[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018: 6848-6856. 1. Overview 1.1. Motivation the limitation of computing power on mobiledevices costy dense 1x1 convolutions In this paper, it proposed ShuffleNet pointwise group convolution Depthwise convolution channel shuffle. (a) output from a certain channel are only derived from a small fraction of input channel. block information flow between channel group and weakens representation 1.2. Channel Shufflefor a gn (group, number of each group) feature map reshape to gxn transpose nxg reshape to ng 1.3. ShuffleNet Unit 1.4. Architecture 1.5. Unit Comparisonfor a point of c channels feature map, m channels of bottleneck ResNet. 2cm + 9mm ResNeXt. 2cm + 9mm/g ShuffleNet. 2cm/g + 9mShuffleNet apply group convolution to two 1x1 pointwise convolution. 1.6. Related Work1.6.1. Model GoogleNet SqueezeNet SENet NASNet 1.6.2. Group Convolution AlexNet. 50% kernel on first GPU, 50% on second GPU ResNeXt Xception. depthwise MobileNet. depthwise 1.6.3. Channel Shuffle Operation cuda-convnet. random sparse convolution layer, equivalent to random channel shuffle + group Conv 1.6.4. Model Acceleration Pruning connection Channel reduction Quantization Factorization Implement convolution by FFT Distillation PVANET 2. Experiments 2.1. Hyperparameter 2.2. Shuffle Channel sx. means the scale of channel, sxs times complexity of 1x. 2.3. Comparison 18 times faster than AlexNet 2.4. Inference Time on Mobile Devices Empirically g=3 has a proper trade-off between accuracy and actual inference time]]></content>
      <categories>
        <category>Paper Note</category>
        <category>LightWeight</category>
      </categories>
      <tags>
        <tag>Network</tag>
        <tag>LightWeight</tag>
        <tag>Shufflenet</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(IJCAI 2018) Multi-scale and Discriminative Part Detectors Based Features for Multi-label Image Classification]]></title>
    <url>%2F2018%2F03%2F12%2FMulti-scale%20and%20Discriminative%20Part%20Detectors%20Based%20Features%20for%20Multi-label%20Image%20Classification%2F</url>
    <content type="text"><![CDATA[Cheng G, Gao D, Liu Y, et al. Multi-scale and Discriminative Part Detectors Based Features for Multi-label Image Classification[C]//IJCAI. 2018: 649-655. 1. Overview 1.1. Motivation global CNN feature lack geometric invariance for addressing the problem of intra-class variation In this paper, it proposes Multi-scale and Discriminative Part Detectors (MsDPD) task-driven feature pooling 1.2. Architecture 1.3. Formulation S = softmax(); sigma = sigmoid() xij. a patch of origin image phi. pooled DPD-based feature O(xij). a point of DPD-based feature map (1x1xK) P. prediction 1.4. Loss Function classification loss. BCE generalized max pooling regularization term enforce the dot product similarity between O(xij) and the pooled feature phi(Xi) to be a constant one object part-level classification loss term y_l∈R^K. stands for object part label vector of object part instance xl]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Multi-label Classification</category>
      </categories>
      <tags>
        <tag>Multi-label Classification</tag>
        <tag>Multi-scale</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(CVPR 2017) Improving pairwise ranking for multi-label image classification]]></title>
    <url>%2F2018%2F03%2F12%2FImproving%20pairwise%20ranking%20for%20multi-label%20image%20classification%2F</url>
    <content type="text"><![CDATA[Li Y, Song Y, Luo J. Improving pairwise ranking for multi-label image classification[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2017: 3617-3625.8. 1. Overview 1.1. Motivation hinge loss is non-smooth and difficult to optimize simple heuristics (top-k, threshhold) limits the use in the real world In this paper, propose smooth pairwise ranking loss incorporate label decision into model 1.2. Related Work1.2.1. Objective exact match Y^. predicted labels of i-th sample Y. GT Hamming distance Ranking objective 2. Algorithm f. label prediction, d–&gt;K (K. all labels) g. label decision, K–&gt;k (k＜K) 2.1. Label Prediction2.1.1. Pairwise Loss (PWE) 2.1.2. Log-Sum-Exp sample at most t pairs 2.2. Label Decision estimate label count estimate optimal thresholds for each class g. MLP on top of f’(x) (final CNN layer) (FC-ReLU, FC-ReLU) + two different branch 2.2.1. Label Count Estimation n-way classification Cross Entropy 2.2.2. Threshold Estimation 2.3. Details label prediction model VGG16 replace Softmax loss with LSEP loss finetune 10 epoches label decision model maximum number count = 4 first FC-100, second FC-10 count FC-4, threshold FC-14 3. Experiments]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Multi-label Classification</category>
      </categories>
      <tags>
        <tag>Multi-label Classification</tag>
        <tag>Pairwise Ranking</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(CVPR 2018) Tienet:Text-image embedding network for common thorax disease classification and reporting in chest x-rays]]></title>
    <url>%2F2018%2F03%2F02%2FTienet%3A%20Text-image%20embedding%20network%20for%20common%20thorax%20disease%20classification%20and%20reporting%20in%20chest%20x-rays%2F</url>
    <content type="text"><![CDATA[Wang X, Peng Y, Lu L, et al. Tienet: Text-image embedding network for common thorax disease classification and reporting in chest x-rays[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2018: 9049-9058. 1. Overview In this paper, it proposes TieNet (Text-Image Embedding Network) CNN-RNN Multi-level attention highlight the meaning full text words and image regions generate reporting paired text-image representation from training two enhancement: AETE. attention-encoded text embedding SW-GAP. saliency weighted global average pooling 1.1. Related Work image caption 1.2. Task Type1.2.1. Medical Image Auto-Annotation ommit the generation of sequential words BP only for classification loss 1.2.2. Automatic Classification and Reporting of Thorax Disease training. image + report testing. only image 1.3. Architecture 1.3.1. CNN word embedding. (T, d_w) output of transition layer. X (D, D, C), D=16, C=1024 1.3.2. RNN phi(X) map X toget h_0. d_x to d_h Input. w. previous generated word a. previous generated weight 1.3.3. Attention Text Enhancement G. weights (r, T) H. (d_h, T) W_s1 (s, d_h) W_s2 (r, s) r. the number of global attention M. embedding matrix (r, d_h) execute max-over-r pooling across M to highlight word 1.3.4. Saliency Weighted Global Average Pooling reuse G to highlight region 1.3.5. Joint Training concate then use FC to predict classification 1.3.6. Details classification. 15 length word2vec. 200 dimension 15,427 words appear at least twice. out-of-vocabulary token, start token, end token LSTM 256 cell, 350 unit, s=2000, r=5 α=1 0.5 dropout, 1e-4 for L2 regularization 1e-3 LR, Adam balanced loss. β. image with at least one disease and no disease λ. image with and without certain disease Loss L_R. RNN loss L_C. classification loss 2. Experiments 2.1. Dataset ChestX-ray14 Hand-labeled OpenI 2.2. Comparision]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Medical</category>
        <category>Lung</category>
      </categories>
      <tags>
        <tag>Lung</tag>
        <tag>Multi-label Classification</tag>
        <tag>Medical</tag>
        <tag>DenseNet</tag>
        <tag>ChestX-ray14</tag>
        <tag>Localization</tag>
        <tag>NLP</tag>
        <tag>Cross-Modality</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(2018) Deep multiscale convolutional feature learning for weakly supervised localization of chest pathologies in X-ray images]]></title>
    <url>%2F2018%2F03%2F02%2FDeep%20multiscale%20convolutional%20feature%20learning%20for%20weakly%20supervised%20localization%20of%20chest%20pathologies%20in%20X-ray%20images%2F</url>
    <content type="text"><![CDATA[Sedai S, Mahapatra D, Ge Z, et al. Deep multiscale convolutional feature learning for weakly supervised localization of chest pathologies in X-ray images[C]//International Workshop on Machine Learning in Medical Imaging. Springer, Cham, 2018: 26 1. Overview In this paper, it proposes weakly supervised method to localize chest pathologies intermediate feature maps from different stages leared layer relevant weight weighted CAM improves the location performance of small size pathologies (nodule, mass) 1.1. Model Train C-CNN learned weight. (b, c), each (1, c) initialize to 1/B B. number of scales 1.2. Loss Function β. percentage of 0 1.3. Details Adam. LR 1e-3, decay by 0.1 when valid loss plateau randomly split 256x256 others. Xavier init]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Medical</category>
        <category>Lung</category>
      </categories>
      <tags>
        <tag>Lung</tag>
        <tag>Multi-label Classification</tag>
        <tag>Medical</tag>
        <tag>DenseNet</tag>
        <tag>ChestX-ray14</tag>
        <tag>Localization</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(2018) Large scale automated reading of frontal and lateral chest X-rays using dual convolutional neural networks]]></title>
    <url>%2F2018%2F03%2F02%2FLarge%20scale%20automated%20reading%20of%20frontal%20and%20lateral%20chest%20X-rays%20using%20dual%20convolutional%20neural%20networks%2F</url>
    <content type="text"><![CDATA[Rubin J, Sanghavi D, Zhao C, et al. Large scale automated reading of frontal and lateral chest x-rays using dual convolutional neural networks[J]. arXiv preprint arXiv:1804.07839, 2018. 1. Overview In this paper, it proposes DualNet PA-lateral pair DualNet AP-lateral pair DualNet experiments on MIMIC-CXR 1.1. Related Work ChestX-ray14 dataset JSRT dataset BSE-JSRT dataset Indiana chest X-ray Shenzhen dataset 1.2. Limitation medical image. 12-bit or greater make no distinction between PA and AP cardiomegaly can only be accurately assessed in PA image AP view will exaggerate the heart silhouette due to magnificention lateral view reveals lung areas that are hidden in the frontal view lateral view can be useful in detecting lower-lobe lung disease, pleural effusions and anterior mediastinal masses 1.3. Network &amp; Details replace 3-channel to 1-channel four denseblock (32 growth rate) per layer no data augmentation Adam with 0.001~0.02 (Triangular2 policy) 1.4. Dataset 80-20-10 nearest interpolation (ratio mantained). 512x512 normalize from [0, 2^12-1] to [0, 1] 1.5. Results PA results in larger AUC for atelectasis, cardiomegaly, fibrosis, infiltration and pleural thickening lateral benefit for consolidation, edema, effusion, hernia, mass, pneumonia and pneumothorax 1.6. Future Work improvement. data augmentation, pixel normalization patient’s history and current clinical record]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Medical</category>
        <category>Lung</category>
      </categories>
      <tags>
        <tag>Lung</tag>
        <tag>Multi-label Classification</tag>
        <tag>Medical</tag>
        <tag>DenseNet</tag>
        <tag>MIMIC-CXR</tag>
        <tag>Lateral X-ray</tag>
        <tag>PA View &amp; AP View</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(2017) Boosted cascaded convnets for multilabel classification of thoracic diseases in chest radiographs]]></title>
    <url>%2F2018%2F03%2F02%2FBoosted%20cascaded%20convnets%20for%20multilabel%20classification%20of%20thoracic%20diseases%20in%20chest%20radiographs%2F</url>
    <content type="text"><![CDATA[Kumar P, Grewal M, Srivastava M M. Boosted cascaded convnets for multilabel classification of thoracic diseases in chest radiographs[C]//International Conference Image Analysis and Recognition. Springer, Cham, 2018: 546-552. 1. Overview In this paper propose a cascaded DNN exploit smooth PWE loss 1.1. Multi-Label Classification1.1.1. Binary Relevance (BR) not account for the interdependence of different class labels BCE loss 1.1.2. Pairwise Error (PWE) Loss maximzie the margin between positive and negative labels within an example 1.2. Architecture weighted sampling 1.3. Details randomly separate 20% for test 6-level cascading both for cross-entropy and PWE loss ReLU, 0.5 dropout between FC SGD 0.1 1.4. Results]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Medical</category>
        <category>Lung</category>
      </categories>
      <tags>
        <tag>Lung</tag>
        <tag>Multi-label Classification</tag>
        <tag>Medical</tag>
        <tag>ChestX-ray14</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(2018) A Weakly Supervised Adaptive DenseNet for Classifying Thoracic Diseases and Identifying Abnormalities]]></title>
    <url>%2F2018%2F03%2F02%2FA%20Weakly%20Supervised%20Adaptive%20DenseNet%20for%20Classifying%20Thoracic%20Diseases%20and%20Identifying%20Abnormalities%2F</url>
    <content type="text"><![CDATA[Zhou B, Li Y, Wang J. A weakly supervised adaptive densenet for classifying thoracic diseases and identifying abnormalities[J]. arXiv preprint arXiv:1807.01257, 2018. 1. Overview In this paper, adaptive DenseNet bridging layer WSL pooling strucuture 2. Architecture 2.1. Adaptive DenseNet remove avg pooling from the third transition layer dilated all kernels in the fourth dense block output 14x14 2.2. Bridging Layer transform by 1x1 Conv (b, 1664, h, w) – (b, MxC, h, w) 2.3. WSL Pooling 2.3.1. Class-Wise Pooling C (b, M, h, w) – C (b, 1, h, w) – (b, C, h, w) 2.3.2. Spatial-Wise Pooling (b, C, h, w) – (b, C, 1, 1) during training, randomly select from top-k m duting testing, both top-k and bottom-k 3. Experiments 3.1. Dataset same as ChestX-ray14 split 256x256, randomly crop 224x224 normalize by ImageNet test. center crop 3.2. Details weighted BCE in each DenseBlock, add BN and dropout 0.1 M = 14 training. k=10 testing. k+ = k- = 15 α = 1 heatmap threshold. 0.8 for Cardiomegaly, 0.9 for others 3.3. Comparison]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Medical</category>
        <category>Lung</category>
      </categories>
      <tags>
        <tag>Lung</tag>
        <tag>Multi-label Classification</tag>
        <tag>Medical</tag>
        <tag>DenseNet</tag>
        <tag>ChestX-ray14</tag>
        <tag>Localization</tag>
        <tag>Weakly Supervised Learning Pooling Strucuture</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(ACM ICBCB 2018) Diagnose like a Radiologist:Attention Guided Convolutional Neural Network for Thorax Disease Classification]]></title>
    <url>%2F2018%2F03%2F02%2FDiagnose%20like%20a%20Radiologist%3A%20Attention%20Guided%20Convolutional%20Neural%20Network%20for%20Thorax%20Disease%20Classification%2F</url>
    <content type="text"><![CDATA[Guan Q, Huang Y, Zhong Z, et al. Diagnose like a radiologist: Attention guided convolutional neural network for thorax disease classification[J]. arXiv preprint arXiv:1801.09927, 2018. 1. Overview 1.1. Motivation existing methods use global image as input data limitation small localized areas poor alignment In this paper, it proposes AG-CNN (Attention Guided CNN) three-branch. global, local and fusion learn a global CNN branch using global images crop local region based on global branch fuse global and local 1.2. Related Work JSRT dataset Shenzhen chest X-ray set Montgomery County chest X-ray set Indiana University Chest X-ray Colletion dataset 2. Architecture 2.1. AG-CNN output. [l1, l2, …, l15]. 14 disease + 1 No Finding 2.2. Algorithm 2.3. Attention Guided Mask Inference absolute value (b, k, h, w) – (b, 1, h, w) binary mask. threshhold=0.7 crop based on [x_min, y_min, x_max, y_max] 2.4. Training Strategy fine-tune global branch fine-tune local branch, fix global branch fine-tune fusion branch, fix local and global branch 3. Experiments 3.1. Dataset randomly shuffle 70-10-20 3.2. Details 256x256, randomly crop 224x224 random horizontal flipping normalize by ImageNet mean value test. center crop 3.3. Comparison 3.4. Others3.4.1. Threshold 3.4.2. Training Strategy 3.4.3. Heat Map Analysis]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Medical</category>
        <category>Lung</category>
      </categories>
      <tags>
        <tag>Lung</tag>
        <tag>Multi-label Classification</tag>
        <tag>Medical</tag>
        <tag>ChestX-ray14</tag>
        <tag>Localization</tag>
        <tag>Attention</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(ACM ICBCB 2018) Weakly Supervised Deep Learning for Thoracic Disease Classification and Localization on Chest X-rays]]></title>
    <url>%2F2018%2F03%2F01%2FWeakly%20Supervised%20Deep%20Learning%20for%20Thoracic%20Disease%20Classification%20and%20Localization%20on%20Chest%20X-rays%2F</url>
    <content type="text"><![CDATA[Yan C, Yao J, Li R, et al. Weakly supervised deep learning for thoracic disease classification and localization on chest x-rays[C]//Proceedings of the 2018 ACM International Conference on Bioinformatics, Computational Biology, and Health Informatics. ACM, 2018: 103-110. 1. Overview 1.1. Motivation existing methods do not treat different diseases separately In this paper, it exploits DenseNet-121 as backbone, and equip with SE block multi-map transfer max-min pooling 1.2. Related Work JSRT dataset BSE-JSRT dataset Indiana chast X-ray MIMIC-CXR dataset. lateral views are available AGCNN 2. Architecture 2.1. SE Block model the interdependency between channels (b, c, h, w) –avg– (b, c, 1, 1) (b, c, 1, 1) –FC+FC– (b, c, 1, 1) 1. r. reduction rate- (b, c, 1, 1) * (b, c, h, w) 2.2. Multi-Map Layer (b, c, h, w) -M 1x1_Conv- (b, Mc, h, w). (M: the number of class) (b, Mc, h, w) –Classwise Pooling– (b, c, h, w) 2.3. Max-Min Pooling (b, c, h, w) – (b, c) z^c. c-th pooled feature map k+ = k- = 1 α = 0.7 3. Experiments 3.1. Dataset ChestX-ray14. official 70-10-20 512 x 512, 3RGB, normalize 3.2. Details BCE loss Adam with 0.0001, *0.1 when 5 time batch size 16 random crop 448x448 (4 corner + 1 center) horizontal flipping reimplement CheXNet not using BBox 3.3. Comparison 3.4. Ablation Study]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Medical</category>
        <category>Lung</category>
      </categories>
      <tags>
        <tag>Lung</tag>
        <tag>Multi-label Classification</tag>
        <tag>Medical</tag>
        <tag>DenseNet</tag>
        <tag>ChestX-ray14</tag>
        <tag>SE Block</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(2017) Learning to diagnose from scratch by exploiting dependencies among labels]]></title>
    <url>%2F2018%2F03%2F01%2FLearning%20to%20diagnose%20from%20scratch%20by%20exploiting%20dependencies%20among%20labels%2F</url>
    <content type="text"><![CDATA[Yao L, Poblenz E, Dagunts D, et al. Learning to diagnose from scratch by exploiting dependencies among labels[J]. arXiv preprint arXiv:1710.10501, 2017. 1. Overview 1.1. Motivation pre-trained ImageNet models may introduce unintended biases which are undesirable in a clinical setting there exists dependencies among labels the need for pre-training may be safely removed when there are sufficient medical data available In this paper DenseNet Image Encoder LSTM Decoder. using LSTM to leverage interdependencies among target lables without pre-training 1.2. Dependency complex interactions between abnormal patterns frequently have significant clinical meaning that provides radiologist with additional context cardiomegaly (心脏扩大) is more like to additionally have pulmonary edema (肺水肿). edema futher predicate the possible presence of both consolidation (实变) and a pleural effusion (积液) 1.3. Related Work1.3.1. Model Inter-label Dependencies loss function implicitly represent dependencies receive a subset of the previous prediction RNN 1.3.2. OpenI dataset with 7,000 images, smaller and less representative than ChestX-ray8 2. Algorithm 2.1. Model 2.1.1. Encoder 2.1.2. Decoder initial state based on x_enc f_h0, f_c0 with one hidden layer LSTM y. GT with fixed ordering 2.2. Loss Function 3. Experiments 3.1. Details randomly split. 70%, 10%, 20% ChestX-ray8 noticed insignificant performance difference with different random split 512x512 randomly translate 4 directions by 25 pixel randomly rotate [-15, 15] randomly scale [80%, 120%] early stop based on valiadation weighted cross-entropy loss without pre-trained 3.2. Metric NLL AUC DICE Per-example sensitivity and specificity (PESS) Per-class sensitivity and specificity (PCSS) 3.3. Comparison]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Medical</category>
        <category>Lung</category>
      </categories>
      <tags>
        <tag>Lung</tag>
        <tag>Multi-label Classification</tag>
        <tag>Medical</tag>
        <tag>DenseNet</tag>
        <tag>ChestX-ray14</tag>
        <tag>LSTM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(2018) CheXNet:Radiologist-Level Pneumonia Detection on Chest X-Rays with Deep Learning]]></title>
    <url>%2F2018%2F03%2F01%2FCheXNet%3A%20Radiologist-Level%20Pneumonia%20Detection%20on%20Chest%20X-Rays%20with%20Deep%20Learning%2F</url>
    <content type="text"><![CDATA[Rajpurkar P, Irvin J, Zhu K, et al. Chexnet: Radiologist-level pneumonia detection on chest x-rays with deep learning[J]. arXiv preprint arXiv:1711.05225, 2017. 1. Overview 1.1. MotivationIn this paper, it proposes CheXNet 121-layer DenseNet on ChestX-ray14 1.2. Details random split dataset. 70% training, 10% validation, 20% test 224x224, normalize based on the mean and standard deviation of ImageNet data augmentation. horizontal flipping unweighted loss CAM 1.3. Comparison]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Medical</category>
        <category>Lung</category>
      </categories>
      <tags>
        <tag>Lung</tag>
        <tag>Multi-label Classification</tag>
        <tag>Medical</tag>
        <tag>DenseNet</tag>
        <tag>ChestX-ray14</tag>
        <tag>Localization</tag>
        <tag>CheXNet</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(CVPR 2018) Weakly Supervised Medical Diagnosis and Localization from Multiple Resolutions]]></title>
    <url>%2F2018%2F03%2F01%2FWeakly%20Supervised%20Medical%20Diagnosis%20and%20Localization%20from%20Multiple%20Resolutions%2F</url>
    <content type="text"><![CDATA[Li Z, Wang C, Han M, et al. Thoracic disease identification and localization with limited supervision[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018: 8290-8299. 1. Overview 1.1. Motivation Medical training data rarely includes more than global image-level labels as segmentations are time-consuming and expensive to collect In this paper, it proposes a architecture learn at multiple resolutions while generating saliency maps with weak supervision parameterize the Log-Sum-Exp pooling function with a learnable lower-bounded adaptation (LSE-LBA) 1.2. Disease Enlargement. width of the heart is measured to be 50% or greater than the width of the thoracic cage Nodules (肺结节). subtle findings as small as a few millimeters in size and are frequently missed by practitioners even when viewed closely on a high resolution monitor Diffuse infiltrative opacification (浸润). in the periphery of the lung is often more easily noted from a global view. But closer inspection of the anomalous region is often required to narrow the differential diagnosis and determine followup 1.3. Contribution multi-resolution with MIL LSE-LBA 1.4. Related Work Wang. ChestX-ray14 (one resolution, non-adaptive pooling) Li. upsample or downsample to PxP grids Chexnet Yao. leverage interdependencies among 14 diseases Guan. attention guided CNN Kumar. cascaded multiple predictions Tienet. additional radiology reports 2. Methods 2.1. Network ResNet line. f identity connection enseNet line coarse-to-fine. U denote upsample 2.2. Pooling Function Noisy-OR (NOR) Generalized-mean (GM) Log-Sum-Exp Log-Sum-Exp Pooling with Lower-bound Adaptation (LSE-LBA) larger r_0 encourages the learned saliency map to have less diffuse modes. 3. Experiments 3.1. dataset 1024x1024 to 512x512, [0, 1] data augmentation resize [0.25, 0.75] translate in for direction [-50, 50] rotate [-25, 25] not use other clinical information, such as age and gender official split 3.2. Comparison when r_0 is large, the performance of disffused abnormalities degrades, such as atelectasis(扩张不全), cardiomegaly (心脏扩大), effusion (积液) and pneumonia (肺炎) increasing r_0 results in overall sharper saliency maps]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Medical</category>
        <category>Lung</category>
      </categories>
      <tags>
        <tag>Lung</tag>
        <tag>Multi-label Classification</tag>
        <tag>Medical</tag>
        <tag>DenseNet</tag>
        <tag>ChestX-ray14</tag>
        <tag>Localization</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(ICPR 2018) Learning to recognize Abnormalities in Chest X-Rays with Location-Aware Dense Networks]]></title>
    <url>%2F2018%2F03%2F01%2FLearning%20to%20recognize%20Abnormalities%20in%20Chest%20X-Rays%20with%20Location-Aware%20Dense%20Networks%2F</url>
    <content type="text"><![CDATA[Guendel S, Grbic S, Georgescu B, et al. Learning to recognize abnormalities in chest x-rays with location-aware dense networks[C]//Iberoamerican Congress on Pattern Recognition. Springer, Cham, 2018: 757-765. 1. Overview 1.1. Motivation most methods report performance based on random image splitting, ignore the high probability of the same patient appearing in both training and test test most methods fails to explicitly incorporate the spatial information of abnormalities or utilize the high resolution images In this paper, it proposes location aware Dense Networks (DNetLoc) incorporate spatial information and high resolution for classification provide new reference patient-wise splits for ChestX-ray14 and PLCO 1.2. Related Work Wang. proposes ChestX-ray14 dataset Chexnet. slightlymodify DenseNet Yao. DenseNet + LSTM (randomly split dataset) Guan. attention guided CNN (randomly split dataset) 1.3. Dataset1.3.1. ChestX-Ray14 1024x1024, 8 bits gray-scale 112,120 images 1.3.2. PLCO 2500x2100, 16 bits gray-scale choose 12 most prevalent pathology labels, among which 5 labels contains spatial information Across both data sets, there are 6 labels which share the same name, but not combing them 1.4. Method loss function leverage high-resolution images (2 Conv with 3 kernel, 3x3 size, stride 2). 1024x1024 as input 35 lable where 21 from PLCO 1.5. Experiments]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Medical</category>
        <category>Lung</category>
      </categories>
      <tags>
        <tag>Lung</tag>
        <tag>Multi-label Classification</tag>
        <tag>Medical</tag>
        <tag>DenseNet</tag>
        <tag>ChestX-ray14</tag>
        <tag>PLCO</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(2018) ChestNet:A Deep Neural Network for Classification of Thoracic Diseases on Chest Radiography]]></title>
    <url>%2F2018%2F03%2F01%2FChestNet%3A%20A%20Deep%20Neural%20Network%20for%20Classification%20of%20Thoracic%20Diseases%20on%20Chest%20Radiography%2F</url>
    <content type="text"><![CDATA[Wang H, Xia Y. Chestnet: A deep neural network for classification of thoracic diseases on chest radiography[J]. arXiv preprint arXiv:1807.03058, 2018. 1. Overview In this paper, it incorporates the attention mechanism into DNN classification branch attention branch. Grad-CAM experiments on Chest X-ray 14 dataset 1.1. Related Work different pooling strategy statistical label dependencies CheXNet. dense connection + BN 1.2. Dataset Chest X-ray 14 dataset with official patient-wise split (80%/20%) 10% among 80% as validation 1.3. Model ResNet-152. remove softmax layer; replace last FC; Sigmoid 1.3.1. Attention Branch choose output of penultimate residual module first 3 Conv: 1x1, 3x3, 1x1 A~. output of the third CNN A-. map for each class c α_ck. computed by using the gradient propagation of Grad-CAM Then normalize each element in A-. use normalized map A as input to last 3 CNN (14, 1x1; 512, 1x1; 1, 14x14) 1.3.2. Training 224x224 image no data augmentation threshold 0.5 1.4. Experiments 1.4.1. With or Without Attention]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Medical</category>
        <category>Lung</category>
      </categories>
      <tags>
        <tag>Lung</tag>
        <tag>Multi-label Classification</tag>
        <tag>Medical</tag>
        <tag>ChestX-ray14</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(CVPR 2018) Thoracic Disease Identification and Localization with Limited Supervision]]></title>
    <url>%2F2018%2F03%2F01%2FThoracic%20Disease%20Identification%20and%20Localization%20with%20Limited%20Supervision%2F</url>
    <content type="text"><![CDATA[Li Z, Wang C, Han M, et al. Thoracic disease identification and localization with limited supervision[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018: 8290-8299. 1. Overview 1.1. Motivation building a highly accurate prediction model requires a large number of annotation and finding the site of abnormalities In this paper, it proposes a method that can work well with a small amount of location annotations effectively leverage both class information and limited location annotation perform disease identificaion and localization at the same time slice the image into patch grids to capture the local information of the disease 1.2. Disease large object. Cardiomegaly (心脏扩大), Emphysema (肺气肿), Pneumothorax (气胸) small object. Mass (肺部块), Nodule (肺结节) Fibrosis (肺纤维化) Edema (肺水肿) Consolidation (肺实变) Atelectasis (肺扩张不全) Effusion (肺积液) Infiltration (肺部浸润) Pneumonia (肺炎) Hernia (肺氙) Pleural Thickening (肺膜增厚) 1.3. Model feature from ResNet-v2 before global pooling layer upsample (bilinear interpolation) or downsample (max-pooling) (PxPxc*) FCN (PxPxK) 1.4. Loss Function1.4.1. With Annotated Bounding Box i-th image, j-th grid, k-th channel (class) 1.4.2. Without Annotated Bounding Box 1.4.3. Loss for k-th Class *. GT 1.4.4. Simplify to 1.4.5. For All Class 2. Experiments 2.1. Details patch slice. {12, 16, 20} λ_bbox. 5 normalize the patch scores p and 1-p from [0, 1] to [0.98, 1] 2.2. Dataset 112,120 images with 14 disease labels 984 bounding boxes for 880 image about 8 disease annotated 880 images vs unannotated 111,240 images Image. 512x512, [-1, 1] no data augmentation 2.3. Classification train. 70% annotated + 70% unannotated Metric. AUC]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Medical</category>
        <category>Lung</category>
      </categories>
      <tags>
        <tag>Lung</tag>
        <tag>Multi-label Classification</tag>
        <tag>Medical</tag>
        <tag>ChestX-ray14</tag>
        <tag>Localization</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(CVPR 2017) ChestX-ray8:Hospital-scale Chest X-ray Database and Benchmarks on Weakly-Supervised Classification and Localization of Common Thorax Diseases]]></title>
    <url>%2F2018%2F03%2F01%2FChestX-ray8%3A%20Hospital-scale%20Chest%20X-ray%20Database%20and%20Benchmarks%20on%20Weakly-Supervised%20Classification%20and%20Localization%20of%20Common%20Thorax%20Diseases%2F</url>
    <content type="text"><![CDATA[Wang X, Peng Y, Lu L, et al. Chestx-ray8: Hospital-scale chest x-ray database and benchmarks on weakly-supervised classification and localization of common thorax diseases[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2017: 2097-2106. 1. Overview 1.1. Motivation there is still significant room for performance improvement when underlying challenges become greater (0.413 on COCO vs 0.884 on VOC) In this paper, it present a new chest X-ray database (ChestX-ray8) comprise 108,948 frontal-view X-ray images of 32,717 unique patients with the text-mined 8 disease image labels 24,636 images contain one or more pathologies 84,312 images are normal cases, label [0, 0, 0, 0, 0, 0, 0, 0] resize image dimension from 3000x2000 (typical for X-ray image) to 1024x1024 bitmap some connection between different pathologies, which agree with radiologists’s domain knowledge, such as Infiltration is often associated with Atelectasis and Effusion 1.2. DCNN Framework exploit Global Average Pooling to visualize the region 1.2.1. Log-Sum-Exp (LSE) Pooling serve as an adjustable option between max pooling (r→ ∞) and average pooling (r→ 0) LSE suffers from overflow/underflow problems, so 1.3. Loss Functionone of 3 standard loss function Hinge Loss (HL) Euclidean Loss (EL) Cross Entropy Loss (CEL) 1.3.1. Sample-Balance W-CLE 2. Experiments 2.1. Different Pooling Strategy r=10 is best 2.2. Comparison W-CEL is better than CEL]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Medical</category>
        <category>Lung</category>
      </categories>
      <tags>
        <tag>Lung</tag>
        <tag>Multi-label Classification</tag>
        <tag>Medical</tag>
        <tag>ChestX-ray14</tag>
        <tag>Localization</tag>
        <tag>Dataset</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(PAMI 2017) Segnet:A deep convolutional encoder-decoder architecture for image segmentation]]></title>
    <url>%2F2018%2F02%2F25%2FSegnet%3A%20A%20deep%20convolutional%20encoder-decoder%20architecture%20for%20image%20segmentation%2F</url>
    <content type="text"><![CDATA[Badrinarayanan V, Kendall A, Cipolla R. Segnet: A deep convolutional encoder-decoder architecture for image segmentation[J]. IEEE transactions on pattern analysis and machine intelligence, 2017, 39(12): 2481-2495. 1. Overview 1.1. Motivation Due to the lack the good encoding techniques (poor ability to delineate boundaries), existing method use CRF to increase accuracy In this paper. it proposed SegNet In decoder, using upsampling according to the pooling indices of the encoder’s corresponding max-pooling layer. Upsample. get sparse feature map Conv. get dense feature map Eliminate the need for learning to upsample Less memory in the inference time 1.2. Comparison Traditional method DeepLab-LargeFOV DeconvNet 1.3. Dataset CamVid road scenes SUN RGB-D indoor scenes 1.4. Related Work Hand Engineered Feature DNN Random Forest Boosting Smooth classifier by CRF Region Proposal. do not exploit co-occurrence of object or other spatial-context RGBD segmentation CRF-RNN. using RNN mimic the charp boundary delineation capabilities of CRF; can be appended to any deep segmentation architecture Multi-scale. image or feature DeconvNet DecoupledNet SegNet is inspired by the unsupervised feature learning architecture proposed by Ranzato. 1.5. Experiments Best performance is achieved when encoder feature maps are stored in full When memory during inference is constrained, the compressed form (dimensionality reduction, max-pooling indices) can improve performance Larger decoder increase performance for a given encoder]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Architecture</category>
        <category>Network</category>
      </categories>
      <tags>
        <tag>Segmentation</tag>
        <tag>Network</tag>
        <tag>Pooling Indices</tag>
        <tag>SegNet</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(2017) Mobilenets:Efficient convolutional neural networks for mobile vision applications]]></title>
    <url>%2F2018%2F02%2F25%2FMobilenets%3A%20Efficient%20convolutional%20neural%20networks%20for%20mobile%20vision%20applications%2F</url>
    <content type="text"><![CDATA[Howard A G, Zhu M, Chen B, et al. Mobilenets: Efficient convolutional neural networks for mobile vision applications[J]. arXiv preprint arXiv:1704.04861, 2017. 1. Overview 1.1. Motivation DNN become deeper and more complicated to achieve higher accuracy computational limited platform in reality In this paper, it proposed an efficient model called MobileNet split standard Conv into depthwise Conv and pointwise Conv introduce two hyperparameter to trade off between latency and accuracy experiment on object detection, finegrain classification, face attributes and large scale geo-localization 1.2. Dataset Stanford Dogs YFCC100M (Yahoo Flickr Creative Commons 100 Million) 1.3. Related Workmost work compress pretrained networks train small networks directly depthwise separable Conv Flattened Networks Factorized Network Xception Network SqueezeNet Structured Transform Networks Deep Fried Convnets hasing pruning, vector quantization and Huffman coding distillation. using large model train small model instead of gt low bit networks Convolution operations are Implemented by GEMM (general matrix multiply) which makes the kernel and input feature into two matrix and do matrix multiply. Link PlaNet. divide earth into a grid of geography cells, then do geolocation classification on images 2. Architecture 2.1. Depthwise Separable Conv2.1.1. Standard Conv There exists the interaction between N and K. K. kernel size F. feature size M. input channel N. output channel 2.1.2. Depthwise Separable Depthwise Conv (KxKxM -&gt; FxFxM). one kernel for one feature map. Pointwise Conv (1x1xMxN * FxFxM). the whole pointwise kernel for the whole output feature map of depthwise conv. It breaks the interaction between the number of output channel and the size of the kernel. 2.1.3. Comparison for 3x3 kernel, 8~9 time less. 2.2. Structure Mose computation time are in 1x1 Conv Small model has less trouble with overfitting. It is important to put very little or no weight decay on depthwise filter. 2.3. Hyperparameter Width Multiplier (α). Thinner Model (channel number) Resolution Multiplier (ρ). Reduced Representation (input size) 3. Experiments 3.1. Thinner or ShallowThinner is better than Shallow. 3.2. Comparison 3.3. Stanford Dogs 3.4. Geolocalization 3.5. Face Attribute3.6. Detection]]></content>
      <categories>
        <category>Paper Note</category>
        <category>LightWeight</category>
      </categories>
      <tags>
        <tag>Network</tag>
        <tag>Mobilenets</tag>
        <tag>LightWeight</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(2017) MentorNet:Regularizing Very Deep Neural Networks on Corrupted Labels]]></title>
    <url>%2F2018%2F02%2F25%2FMentorNet%3A%20Regularizing%20Very%20Deep%20Neural%20Networks%20on%20Corrupted%20Labels%2F</url>
    <content type="text"><![CDATA[Jiang L, Zhou Z, Leung T, et al. MentorNet: Regularizing Very Deep Neural Networks on Corrupted Labels[J]. arXiv preprint arXiv:1712.05055, 2017. 1. Overview 1.1. Motivation DNN can remember entire data which are labeled randomly DNN has more parameters than the number of training example Poor performance when overfit noise Curriculum Learning. gradually learn samples in a meaningful sequence In this paper, it proposed MentorNet and SPADE (SG-partial-D) algorithm first study to learn a curriculum (weighting schema) from data by neural network supervise the training of StudentNet, improve the generalization on corrupted training data learn time-varying weights for each example to train StudentNet, result in a curriculum that decide the timing and attention to learning each example 1.2. Step pretrain MentorNet to approximate predefined weighting specified in labeled data finetune MentorNet on the third clean label dataset train StudenNet using fixed MentorNet StudenNet make prediction without MentorNet 1.3. Related Work1.3.1. Model Regularizer less effective on corrupted label. weight decay data augmentation dropout 1.3.2. Data Regularizertackle in the data dimension. MentorNet. foucs on weighting example in corrupted labels. can understand and further analyzed existing weighting schemes (self-paced weighting, hard negative mining, focal loss). 1.3.3. Weight Schemes Curriculum Learning Mine hard-negative example Training a network using clean data, coupled with a knowledge graph, to distill soft logits to the noisy data 1.4. ModelGoal overcome overfitting by introducing a regularizer to weight example alternately minimize w and v (fix one, update another) Weighted Loss (WL) v (n samples x m classes). weight of examples L. loss g_{s}. StudentNet Explicit Data Regularizer (G). which contains two forms, both lead to same solution. explicit. analytic form of G(v) implicit. closed-form solution. v^{*} = argmin_{v} F(w, v) (F can be MentorNet) Weight Decay 1.5. Algorithm Problems fix v update w. wasteful when v is far away from optimal point fix w update v. matrix v is too large for memory SPADEminimize w and v stochastically overmini-batches. (5) moving average on the p-th percentile (8) weight decay (9) SGD or other optim 2. MentorNet 2.1. Goal learn optimal Θ to compute the weight of example step. pretrain - finetune - fix and plug in Algorithm 2.2. Architecture 2.2.1. Input z label epoch absolute loss moving average 2.2.2. Sampling Layer sample the weights v, without replacement, according to the normalized weight distribution only perform on trained MentorNet sampling rate. hyperparameter 2.3. Pretraining2.3.1. Dataset enumerate the input space of z, and annotate a weight for each data point weight can derived from any weight schemes 2.3.2. Objective Function (5). explicit (6). converge fast converge to the same solution 2.4. FinetuningThe weighting schemes may change along with the learning process of StudenNet. Dataset sample from dataset D binary label for whether learn this example 3. Experiments 3.1. Other network label weight according to different weighting schemes 3.2. Other regularizer (b). Weighted loss converge to zero 3.3. Representation of MentorNet similar images have less distance.]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Architecture</category>
        <category>Network</category>
      </categories>
      <tags>
        <tag>Network</tag>
        <tag>MentorNet</tag>
        <tag>StudentNet</tag>
        <tag>SPADE</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(CVPR 2018) Data Distillation:Towards Omni-Supervised Learning]]></title>
    <url>%2F2018%2F02%2F25%2FData%20Distillation%3A%20Towards%20Omni-Supervised%20Learning%2F</url>
    <content type="text"><![CDATA[Radosavovic I, Dollár P, Girshick R, et al. Data distillation: Towards omni-supervised learning[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018: 4119-4128. 1. Overview 1.1. Motivation Semi-supervised simulates labeled/unlabeled data (upper-bounded on full annotated data) Omni-supervised exploits extra unlabeled data (lower-bounded on full annotated data) Model distillation distills knowledge from the prediction of multiple model In this paper it proposed data distillation which ensembles prediction from multiple transformations of unlabeled data, using single model Do experiments on keypoint detection and object detection 1.2. Knowledge Distillation Train teacher model on large amount of labeled data (A) Generate annotation on unlabeled data (B) based on teacher model Retrain student model on data (A+B) Problem. training model on its own prediction can not provide meaningful information Solution. ensembling the prediction of the different transformation of unlabeled data (flipping, scaling) 1.3. Related Work Ensemble multiple model, Model Compression FitNet Cross modal distillation Multi-view geometry Auto-encoder (multiple capsule) Self-training can be used for training object detection Multiple views or perturbations of the data can provide useful signal for semi-supervised learningThe method of this paper are also based on multiple geometric ransformations. 2. Data Distillation Step Train teacher model on labeled data (A) Apply teacher model to multiple transformation of unlabeled data (B) Ensemble the multiple prediction to get annotation Retrain student model on data (A+B) 2.1. Multi-transform multi-crop multi-scaleIn the experiment of this paper, it used multi-scale and flipping. 2.2. Ensemble Aggregated prediction generate new knowledge Aggregated prediction outperform any single prediction 2.3. Ensemble way2.3.1. Soft Label average class probability generate probability vector, not category label not suitablefor structure output space (pose, detection) 2.3.2. Hard Label need task specific logic (NMS for merging multiple box) 3. Detail of Pose Estimation 3.1. Selecting PredictionGenerate annotation only from the prediction that are above a certain score threshold. And found that the average number of annotated instances per unlabeled image equal to labeled image’s (similar distribution) still work robust and well when not equal 3.2. Retraining retraining is better than fine-tuning (which is in a poor optimum). 4. Experiments 4.1. Data split co-80. labeled co-35. labeled co-115. co-80 + co-35 un-120. unlabeled s1m-180 (sports-1M static frame). dissimilar distribution 4.2. Amount of Annotated Data 1:ρ in minibach. 4.3. Accuracy of Teacher Model 4.4. Result]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Data Distillation</category>
      </categories>
      <tags>
        <tag>Detection</tag>
        <tag>Pose</tag>
        <tag>Data Distillation</tag>
        <tag>Omni-supervised Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(CVPR 2017) Feature pyramid networks for object detection]]></title>
    <url>%2F2018%2F02%2F25%2FFeature%20pyramid%20networks%20for%20object%20detection%2F</url>
    <content type="text"><![CDATA[Lin T Y, Dollár P, Girshick R, et al. Feature pyramid networks for object detection[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2017: 2117-2125. 1. Overview 1.1. MotivationFeature pyramid are basic in detection, but expensive. In this paper, it proposed Feature Pyramid Network (FPN). fater-rcnn based on FPN run at 6 fps on GPU 1.2. Forms 1.2.1. Pyramidal Feature Hierarchy High-resolution maps have low-level feature. And it will harm representational capacity for detection. SDD build pyramid from high up in the network 1.2.2. Feature Pyramid Network combine low-resolution (strong semantic) high-resolution (weak semantic) 1.3. Related Work1.3.1. Hand-Engineered SIFT HOG 1.3.2. Deep ConvNet OverFeat R-CNN SPPnet 1.3.3. Using Multiple Layers FCN. sum partial scores over multi-scale Hypercolumns, HyperNet, ParseNet, ION. concat feature of multi-layers SSD, MS-CNN. predict at multi-layers without combining U-Net, Sharp-Mask. Recombinator, Hourglass, Laplacian pyramid. lateral/skip connection 1.4. FPN Two Pathway bottom-up pathway top-down pathway and lateral connectionThe bottom-up map is lower-level semantics, more accuracy for localizing. 2. Application 2.1. RPN Head binary classification bounding box regression Attach head (shared) to each (P2, P3, P4, P5, P6) level of FPN. Each level single scale anchor 3 aspect ratios {1:2, 1:1, 2:1}Head shared mechanism is analogous to image pyramid mechanism with common head classifier. 2.2. Fast R-CNNNo RPN, only RoI pooling. view feature pyramid as produced from image pyramid assign RoI (w,h) to level Pk of FPN 224 is the canonical ImageNet pre-training size k0 is the level of 224x224 RoI Shared headAnalogous to ResNet-based Faster R-CNN, set k0 = 4. The k of 112x112 RoI is 3 (k0 - log[112/224]). 3. Experiments 3.1. Ablation StudyUsing P2 have more proposal. 3.2. Comparison 3.3. Extension]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Architecture</category>
        <category>Network</category>
      </categories>
      <tags>
        <tag>Detection</tag>
        <tag>Network</tag>
        <tag>FPN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(ICCV 2017) Focal Loss for Dense Object Detection]]></title>
    <url>%2F2018%2F02%2F25%2FFocal%20Loss%20for%20Dense%20Object%20Detection%2F</url>
    <content type="text"><![CDATA[Lin T Y, Goyal P, Girshick R, et al. Focal loss for dense object detection[C]//Proceedings of the IEEE international conference on computer vision. 2017: 2980-2988. 1. Overview 1.1. Motivation two-stage approach contains sparse set of candidate object location one-stage approach suffered from foreground-background class imbalance In this paper, it proposed focal loss function to deal with class imbalance of one-stage approach (foucs on hard example [large error]). 1.2. Comparison Two-stage Proposal mechanism (1-2k proposals) Biased minibatchsampling (foreground:background=1:3) One-stage 100k proposals (densely) Most are easy negative which contribute no useful learning signal Easy negative overwhelm 1.3. Related Work Two-Stage R-CNN series One-Stage OverFeat SSD YOLO Class Imbalance Hard Negative Mining. completely discard easy example Sampling Schemes Robust Estimation Huber Loss. reduce the contribution of outliers by down-weighting Focal Loss. Down-weighting inliers 1.4. Focal Loss α. balance factor (inverse class frequency or hyperparameter setted by cross validation) improved accuracy over non-α form γ. focusing parameter, equal to 2 better for experiments When summed over a large number of easy examples, small loss values can overwhelm the rare class. 1.4.1. InitializationIntroduce prior concept for the model estimation probability in the beginning. set final layer’s means that p = π when get through sigmoid function. (π = 0.01 in experiments) 1.5. RainNet Detector 2. Experiments 2.1. Effect 2.2. Ablation Study 2.3. Experiments]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Architecture</category>
        <category>Loss Function</category>
      </categories>
      <tags>
        <tag>Detection</tag>
        <tag>Focal Loss</tag>
        <tag>Loss Function</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(NIPS 2016) Coupled generative adversarial networks]]></title>
    <url>%2F2018%2F02%2F25%2FCoupled%20generative%20adversarial%20networks%2F</url>
    <content type="text"><![CDATA[Liu M Y, Tuzel O. Coupled generative adversarial networks[C]//Advances in neural information processing systems. 2016: 469-477. 1. Overview In this paper, it proposed coupled GAN based on existence of shared high-level representation in the domains learn a joint distribution of multi-domain images unsupervised used in image transformation, domain adaption… 1.1. Model Generator share the same high-level conceptDiscriminator sharing constraint can reduce parameters 1.2. Loss Function 1.3. Related Work VAE Attention Model Moment Matching Diffusion Process Cross-domian Image Generation GAN Laplacian Pyramid Conditional GAN 2. Experiments 2.1. Metric ratios of agreed pixels 2.2. Digit [digit-edge], [digit-negative] without weight-sharing constraint, GAN generate unrelated image correlated to the weight sharing of G uncorrelated to D 2.3. Face2.4. Color and Depth Image 3. Application 3.1. Unsupervised Domain Adaption (UDA) [MNIST(labeled)-UDA(unlabeled)] attached a softmax layer c to last hidden layer of D, train on MNIST, predict on UDA 3.2. Cross-Domain Image TransformationGiven x1 in domain 1, find corresponding image x2 in domain 2. As for CoGAN get the most suitable z* for x1 use z_* generate x2]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Image Processing</category>
      </categories>
      <tags>
        <tag>Image Enhancement</tag>
        <tag>Image Generation</tag>
        <tag>coupleGAN</tag>
        <tag>GAN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(2017) Multi-scale dense convolutional networks for efficient prediction]]></title>
    <url>%2F2018%2F02%2F25%2FMulti-scale%20dense%20convolutional%20networks%20for%20efficient%20prediction%2F</url>
    <content type="text"><![CDATA[Huang G, Chen D, Li T, et al. Multi-scale dense convolutional networks for efficient prediction[J]. arXiv preprint arXiv:1703.09844, 2017, 2. 1. Overview 1.1. Motivation Small model can deal with easy example, but make mistake for hard Hard example need to be deal with by large model, but waste for easy last layer’s features for classification, early layer’s are not first layer for fine-scale, later layer for coarse-scale In this paper, Multi-Scale DenseNet (MSDNet) is proposed which automatically small for easy large for hardcontains two setting anytime classification budgeted batch classification (probability threshhold) has two feature multi-scale feature map + multi-classifier dense connectivity 1.2. Contribution First deep learning architecture of its kind that allows dynamic resource adaptation with a single model First discover that dense connectivity is crucial to early-exit classifier 1.3. Setting1.3.1. Anytime prediction stop at any time point (budget exhausted) return most recent predication nondeterministic budget, varies per test instance L. suitable loss function B. budget f. model x. input image 1.3.2. Budget batch classification stop when sufficient confidence Less than B/M computation for easy example More than B/M computation for hard example 1.4. Related Work Computation-efficient prune weights quantize weights compact model knowledge-distillation Resource-efficient FractalNet Adaptive computation time approach 1.5. Visualization 1.6. Future Work Extend to other task. segmentation Combine MSDNet with model compression, spatially adaptive computation, more efficient convolution operation 2. Multi-Scale DenseNet 2.1. Problem Lack of coarse-level feature Accuracy of the classifier is correlated with its position within the network Solution. multi-scale feature map Early classifier interfere with later classifier Solution. Dense connectivity 2.2. Model 2.3. Lazy EvaluationFiner feature map do not influence the prediction of classifier. 2.4. Loss Function Empirically, $w_k$ = 1.]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Architecture</category>
        <category>Network</category>
      </categories>
      <tags>
        <tag>Network</tag>
        <tag>Multi-Scale DenseNet</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(AAAI 2017) Volumetric ConvNets with Mixed Residual Connections for Automated Prostate Segmentation from 3D MR Images]]></title>
    <url>%2F2018%2F02%2F02%2FVolumetric%20ConvNets%20with%20Mixed%20Residual%20Connections%20for%20Automated%20Prostate%20Segmentation%20from%203D%20MR%20Images%2F</url>
    <content type="text"><![CDATA[Yu L, Yang X, Chen H, et al. Volumetric ConvNets with Mixed Residual Connections for Automated Prostate Segmentation from 3D MR Images[C]//AAAI. 2017: 66-72 1. Overview 目前prostate MRI segmentation存在一些挑战 不同MR扫描协议导致MRI存在差别 prostate MRI缺少清晰边界（prostate与周围组织很相似） 不同病变或分辨率导致prostate形状大小变化很大 3D MRI segmentation 耗时 因此，论文提出一种end-to-end train, volume-to-volume predict, 适用于limited training data的模型，并在MICCAI PROMISE12数据集上达到state-of-art结果。 1.1. 模型特点 3D CNN 存在down sampling和up sampling两个阶段 residual block和connection. block内部存在short connection, block之间存在long connection（恢复down sampling造成的空间信息丢失）. 能够增强局部信息和全局信息在整个网格中的传递，以及加快收敛，避免梯度问题 加权(0.3, 0.6, 1) auxiliary cross-entropy loss. 加快收敛; Function as a strong regularization which is important for limited training data 1.2. Dig the Potential of limited data Data augmentation. 但增加的信息量有限 使用skip connection. 提升网络内信息的propagation 1.3. Related WorkMulti-altas based method Deformable method Graph cut based mathod Feature based machine learning method（Deep Learning） 使用2D CNN aggregate 3D contextual feature. adjacent slices, orthogonal planes, multi-view planes. 但没有充分利用3D空间信息 3D CNN. with residual block 1.4. 数据集MICCAI PROMISE12 训练集. 50 transversal T2-weighted MRI and segmentation gt 测试集. 30 MRI, 未提供gt图像来自不同医院、设备、协议以及存在maximum variation in clinical setting (voxel size, dynamic range,position, field of view and anatomic appearance.). 2. Experiments 2.1. 数据预处理 resize MR volumes into 0.625x0.625x1.5 mm 归一化 rotation (90°, 180°, 270°) , x-flip 2.2. 训练细节 batch size 8 lr 0.001 divided by 10 every 3000 iteration weight decay 0.0005 SGD, 0.9 momentum weighted loss (0.3, 0.6, 1) 训练（4小时）. random crop 64x64x16 sub-volume 测试（12 s/MRI, 320x320x60）. overlap sliding window (64x64x16), stride (50x50x12), average probability map 2.3. 评价指标 Dice coefficient (DSC). 链接 aRVD. absolute difference between volumes ABD. average over shortest distance between the boundary points of volumes 95HD (95% Hausdorff distance). 链接 2.4. Ablation 2.5. 实验结果]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Medical</category>
        <category>Prostate</category>
      </categories>
      <tags>
        <tag>Medical</tag>
        <tag>Segmentation</tag>
        <tag>Prostate</tag>
        <tag>3D</tag>
        <tag>MRI</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(2017) A survey on deep learning in medical image analysis]]></title>
    <url>%2F2018%2F02%2F02%2FA%20Survey%20On%20Deep%20Learning%20in%20Medical%20Image%20Analysis%2F</url>
    <content type="text"><![CDATA[Litjens G, Kooi T, Bejnordi B E, et al. A survey on deep learning in medical image analysis[J]. arXiv preprint arXiv:1702.05747, 2017. 1. Overview 论文检索并分析了308篇与深度学习和医学图像有关的论文。 1.1. 检索方式 对PubMed数据库（标题摘要）进行关键词（convolutional or deep learning）检索 ArXiv检索medical imaging MICCAI, SPIE, ISBI, EMBC conference proceeding 1.2. 相关任务 image classification object detection segmentation registration retrieval generation enhancement 1.3. 应用领域 neuro retinal（视网膜） pulmonary（肺） digital pathology（病理学） breast（乳腺） cardiac（心脏） abdominal（腹部） musculoskeletal（肌肉骨骼） 1.4. CNN Architecture Classification Architecture Multi-stream Architecture (multi-scale; 2.5D, multiple angled patched from 3D space) Segmentation Architecture (U-Net) 1.5. RNN pixelRNN for segmentation 1.6. Unsupervised Model AE, SAE (AE+AE+…+AE). for denoising RBM, DBN (RBM+RBM+…+RBM). 无监督训练，再加上分类器fine tuing VAE, GAN 1.7. 数据集 OASIS. brain MRI registration BRATS, LSLES, and MRBrains challenges. brain lesion segmentation Kaggle Diabetic Retinopathy challenge 2015. disbetic retinopathy classification PROMISE12 challenge. prostate segmentation LUNA16 challenge. nodule classification CAMELYON16, TUPAC, DREAM. breast cancer metastases detection in lymph nodes ICPR 2012, AMIDA 2013, GLAS. gland segmentation SLIVER07. liver segmentation LIDC-IDRI. detect nodules in lung CT 2. 医学图像中的深度学习应用 2.1. Classification2.1.1. Image/Exam Classification （图像-&gt;是否病变）使用预训练权重(迁移学习) 预训练网络提取特征，作为后续模型输入 使用medical数据fine tuning预训练网络2015~2017年，47篇exam classification paper， 36篇使用CNN， 5篇使用AE， 6篇使用RBM. 且涉及不同的医学领域：retinal, digital pathology, lung computed tomography. 2.1.2. Object or Lesion Classification对图像中已经识别出来的某个小部分进行多分类（病变类型, nodule classification in chest CT）。通常需要考虑（Multi-stream） 病变外观的局部信息 病变位置的全局上下文信息到达高分类准确度。 2.2. Detection2.2.1. Organ, Region and Landmark Localization通常需要处理3D图像，一些方法将3D空间看做由一些2D正交平面构成（2D MRI slices）。图像localization方法分为 RoI 直接regress此外，还涉及到temporal数据的scan plane or key frame localization (standardized scan planes in mid-pregnancy fetal US). 2.2.2. Object or Lesion Detection定位并识别lesion (multi-stream for CT and PET data; micro-bleed in brain MRI) 2.3. Segmentation2.3.1. Organ and Substructure SegmentationContour or interior of objects (pectoral muscle in breast MRI, coronary arteries in cardiac CT angiography, vertebral body in MRI). 相关技术 2D U-Net 3D V-Net, Dice coefficient 基于RNN，融合双向信息(left/top, right/bottom) 结合双向LSTM和2D U-Net 结合FCN和graphical model (MRF, CRF) 2.3.2. Lesion Segmentation包含 Object detection Organ and substructure segmentation需要考虑global and loval context [Multi-stream] (white matter lesions in brain MRI). 2.4. Registrationspatial alignment. 包含两种方法 估计两张图像的相似度 预测transformation parameters 2.5. Other Task2.5.1. Retrieval使用Hashing forest进行压缩 2.5.2. Generation and Enhancement超分辨率 2.5.3. Combine Image with ReportCaption, LDAMammography中的3个描述 shape margin density 3. 应用领域 3.1. BrainAlzheimer classification, brain tissue segmentation, anatomical structure (hippocampus). 相关技术 multi-scale for contextual 2D slice-by-slice处理3D数据 3.2. Eyecolor fundus imaging (CFI). 3.3. Chest基于CT扫描估计lung cancer概率 3.4. Digital Pathology and Microscopywhole-slide images （WSI） 检测、分割、分类nuclei 分割organ 检测分类 the disease of interest at the lesion or WSI-level 3.5. Breast考虑三个子任务 检测分类mass-like lesion 检测分类micro-calcfication breast cancer评估 3.6. Cardiac使用2D CNN slice by slice处理3D或4D数据。 3.7. Abdomen3.8. Musculoskeletal3.9. Other 4. 总结 4.1. Overview prefer end-to-end CNN 4.2. Key Aspect data preprocessing or augmentation（相同模型结构，不同结果） 针对特定任务的模型结构（multi-view, multi-scale） 增加patch size观察更多context 超参数（LR, Dropout rate） 4.3. Challenge 医学图像数据已有很多，挑战在于图像的标注（需要领域专家进行标注、3D图像标注成本大）。因此，从有限数据进行高效学习是重要的研究方向。 一些研究利用sparse 2D segmentation训练3D segmentation、一些考虑 non-expert labels via crowd-sourcing Label noise. 多专家分别进行标注，对比结果是否一致 Fail in rare category Class imbalance. 研究方向（通常对较少样本进行scale, rotate操作） 结合patient history, age, demographics进行诊断 Balance image feature (thousands) and clinical feature (handful) patch不具有全局位置信息，而entire image占用较大内存 4.4. Outlook Unsupervised, VAE, GAN are attractive Combine Bayesian statistic with deep learning Image reconstruction (unexplored area)]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Medical</category>
        <category>Overview</category>
      </categories>
      <tags>
        <tag>Medical</tag>
        <tag>Overview</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(2017) Deep Learning in Medical Imaging:General Overview]]></title>
    <url>%2F2018%2F02%2F02%2FDeep%20Learning%20in%20Medical%20Imaging%3A%20General%20Overview%2F</url>
    <content type="text"><![CDATA[Lee J G, Jun S, Cho Y W, et al. Deep Learning in Medical Imaging: General Overview[J]. Korean Journal of Radiology, 2017, 18(4): 570-584. 1. Overview 论文综述了深度学习技术的历史、发展和应用（涉及到医学图像）。 1.1. AI三种方法 Symbolism Connectionism Bayesian 1.2. 算法 分类. Bayesian模型、SVM、集成学习(结合不同分类算法) 回归. SVR ANN. DNN Unsupervised restricted Boltzmann machine. Hinton, 解决DNN的局部最优、过拟合问题。以无监督方式生成数据特征 CNN. RNN 1.3. Trick Data Augmentation 预训练权重能够在100 cases per class得到较好地结果。 1.4. 突破 ReLu Dropout Data augmentation 2. 放射学应用 2.1. Segmentation lungs（肺） tumor（肿瘤） structure in brain biological cells（生物细胞） membranes（薄膜） tibial cartilage（胫骨软骨） bone tissue（骨组织） cell mitosis（细胞有丝分裂） 2.2. Registration医学图像配准. 同一患者几幅图像放在一起分析时，首先要做图像严格对齐。医学图像配准是指对于一幅医学图像寻求一种 (或一系列 )空间变换，使它与另一幅医学图像上的对应点达到空间上的一致。 方法 Patch Based Entire Image 3. Automatic Labeling and Captioning 3.1. 数据集 Caption. Flickr8k, Flickr30k, MS COCO Publicly available radiology dataset of chest radiographs 4. Reading Assistant and Automatic Dictation Automatic radiological dictation system. 5. 深度学习应用到放射学的局限性 要求高质量数据，数据量太小容易过拟合 无法得到通用方法. 世界各地成像设备、协议以及疾病流行率的差异 法律伦理问题 事故责任承担问题]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Medical</category>
        <category>Overview</category>
      </categories>
      <tags>
        <tag>Medical</tag>
        <tag>Overview</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(CVPR 2016) Deeply-recursive convolutional network for image super-resolution]]></title>
    <url>%2F2018%2F01%2F24%2FDeeply-recursive%20convolutional%20network%20for%20image%20super-resolution%2F</url>
    <content type="text"><![CDATA[Kim J, Kwon Lee J, Mu Lee K. Deeply-recursive convolutional network for image super-resolution[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2016: 1637-1645. 1. Overview 1.1. Motivation stacking Conv to increase receptive filed lead to more parameter pooling discard pixel-infomation In this paper, it proposed DRCN (deeply-recursive convolutional network) 16 recursion recursive-supervision and skip-connections to solve gradient problem 2. Methods 2.1. Baseline 2.2. Advance Model D predictions (shared net to predict) are simultaneously supervised use all D predictions to compute the final output skip connections. LR and HR are similar weighted average of all intermediate predictions (learned parameter) 2.3. Loss Function intermediate output final output final loss function 3. Experiments 3.1. Comparison 3.2. Ablation Study]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Image Processing</category>
        <category>Super Resolution</category>
      </categories>
      <tags>
        <tag>Image Processing</tag>
        <tag>Image Enhancement</tag>
        <tag>Super Resolution</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(CVPR 2018) Residual-Guide Feature Fusion Network for Single Image Deraining]]></title>
    <url>%2F2018%2F01%2F24%2FResidual-Guide%20Feature%20Fusion%20Network%20for%20Single%20Image%20Deraining%2F</url>
    <content type="text"><![CDATA[Fan Z, Wu H, Fu X, et al. Residual-Guide Feature Fusion Network for Single Image Deraining[J]. arXiv preprint arXiv:1804.07493, 2018. 1. Overview 1.1. Motivation wasteful to utilize a resource-hungry model to meet all kinds of demands for rain streak removal task cascaded architecture may lost valuable intermediate reconstruction features In this paper, it proposed ResGuidedNet residual generated from shallower blocks to guide deeper blocks coarse to fine estimation recursive convolutional apply to low-level and high-level 2. Methods recursive block. Conv+LeakyReLU, 5 recursive time ensemble all intermediate results to predict final output 2.1. Loss Function L2 + SSIM for block_k g. SSIM function λ=1 M + 1 loss function 3. Experiments 3.1. Comparison 3.2. Ablation Study 3.3. Time]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Image Processing</category>
        <category>De-raining</category>
      </categories>
      <tags>
        <tag>Image Processing</tag>
        <tag>Image Enhancement</tag>
        <tag>De-raining</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(CVPR 2018) Learning a single convolutional super-resolution network for multiple degradations]]></title>
    <url>%2F2018%2F01%2F24%2FLearning%20a%20single%20convolutional%20super-resolution%20network%20for%20multiple%20degradations%2F</url>
    <content type="text"><![CDATA[Zhang K, Zuo W, Zhang L. Learning a single convolutional super-resolution network for multiple degradations[C]//IEEE Conference on Computer Vision and Pattern Recognition. 2018, 6. 1. Overview 1.1. Motivation existing SR methods assume bicubicly downsample accurate blur kernel increases good performance, mismatch blur kernel decrease performance In this paper take two keys factors as input. blur kernel, noise level proposed dimensionality stretching strategy training. different combinations of blur kernels and noise levels testing. select the best fitted degradation model rather than bicubic 1.2. Related Works SRCNN VDSR LapSRN SRGAN 1.3. Degradation Model 1.3.1. Blur Kernel the influence of an accurate blur kernel is much larger than that of sophisticated image priors smoother kernel→ over smoothed sharper kernel→ ringing artifacts 1.3.2. Noise directly SR with noise removal will amplify the noise denoising pre-processing loses detail information, jointly better 2. Methods 2.1. MAP Problem 2.2. Dimensionality Stretching blur kernel. pxp→ p^2x1→ tx1→ h x w x t noise. σx1→ tx1→ h x w x 1 concat. h x w x (t+1) 2.3. Network pixelShuffle 2.4. Loss Function 3. Experiments 3.1. Comparison 3.2. Inference for real images, use grid search strategy rather than adopting any blur kernel or noise level estimation methods]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Image Processing</category>
        <category>Super Resolution</category>
      </categories>
      <tags>
        <tag>Image Processing</tag>
        <tag>Image Enhancement</tag>
        <tag>Super Resolution</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(CVPRW 2017) Formresnet:Formatted residual learning for image restoration]]></title>
    <url>%2F2018%2F01%2F24%2FFormresnet%3A%20Formatted%20residual%20learning%20for%20image%20restoration%2F</url>
    <content type="text"><![CDATA[Jiao J, Tu W C, He S, et al. Formresnet: Formatted residual learning for image restoration[C]//Computer Vision and Pattern Recognition Workshops (CVPRW), 2017 IEEE Conference on. IEEE, 2017: 1034-1042. 1. Overview 1.1. Motivation directly learn the clean images may suffer gradient problem directly learn the high-frequency residual may harm the structure detials L2 loss suffers from blur In this paper, it proposed residual formating layer to recover the latent clean image learn the structure detial proposed cross-level loss net Experiments on denoising, SR, de-raining, inpainting, enhancement 2. Methods 2.1. Residual Formatting Layer aim to reduce the corrupation on the input image layer can be stacked 2.2. Cross-Level Loss Net2.2.1. Pixel Loss 2.2.2. Feature Loss 2.2.3. Gradient Loss sobel 2.2.4. Loss Function 3. Experiments 3.1. Comparison 3.2. Time]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Image Processing</category>
        <category>Super Resolution</category>
      </categories>
      <tags>
        <tag>Image Processing</tag>
        <tag>Image Enhancement</tag>
        <tag>Super Resolution</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(2018) PAD-Net:A Perception-Aided Single Image Dehazing Network]]></title>
    <url>%2F2018%2F01%2F24%2FPAD-Net%3A%20A%20Perception-Aided%20Single%20Image%20Dehazing%20Network%2F</url>
    <content type="text"><![CDATA[Liu Y, Zhao G. PAD-Net: A Perception-Aided Single Image Dehazing Network[J]. arXiv preprint arXiv:1805.03146, 2018. 1. Overview 1.1. Motivation L2 norm implicitly assumes a white Gaussian noise, which is an oversimplified case that is not valid in general dehazing cases L2 treats the impact of noise independently to the local characteristics In this paper, it replaced the L2 loss with perceptually derived loss function SSIM MS-SSIM prposed Perception-Aided SIngle Image Dehazing Network (PAD-Net). AOD-Net + Loss Function 2. Loss Function 2.1. L2 Loss 2.2. L1 Loss The derivative of L1 loss is not defined at 0. If the error is 0, do not need to update the weight. So use the convention that sign(0) = 0 2.3. SSIM μ_x. view as estimates of the luminance of x σ_x. contrast of x μ_{xy}. structural similarity l(p). measure the comparisons of the luminance cs(p). combination of contrast and structure similarity p. the pixel involved the μ and σ of the Gaussian filter on the pixel p p’. center pixel of patch P 2.3.1. Derivation 2.4. MS-SSIM smaller σ_G loses the ability to preserve the local structure and reintroduce splotchy in flat region larger σ_G tends to keep the noises in the proximity of edges Multi-scale σ_G α = β_j = 1 j = 1, …, M 3. Experiments 3.1. System MS-SSIM + L2 MS-SSIM + L1 3.2. Dataset ITS. indoor training set OTS. outdoor training set random select 10,000 images from ITS + OTS (2,790 IST + 7,210 OST) random select 1,000 non-overlapping set as validation set test on SOTS (500 indoor + 500 outdoor) 3.3. Directly Train 3.4. Fine-tuing on AOD-Net]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Architecture</category>
        <category>Loss Function</category>
      </categories>
      <tags>
        <tag>Dehazing</tag>
        <tag>Image Processing</tag>
        <tag>Image Enhancement</tag>
        <tag>AOD-Net</tag>
        <tag>Loss Function</tag>
        <tag>MS-SSIM</tag>
        <tag>SSIM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(ECCV 2018) Image Inpainting for Irregular Holes Using Partial Convolutions]]></title>
    <url>%2F2018%2F01%2F24%2FImage%20Inpainting%20for%20Irregular%20Holes%20Using%20Partial%20Convolutions%2F</url>
    <content type="text"><![CDATA[Liu G, Reda F A, Shih K J, et al. Image inpainting for irregular holes using partial convolutions[C]//Proceedings of the European Conference on Computer Vision (ECCV). 2018: 85-100. 1. Overview 1.1. Motivation Conv responses conditioned on both valid pixels as well as the substitute values (mean value) in masked holes often lead to artifacts such as color discrepancy and blurriness post-processing used to reduce artifacts, but expensive and may fail previous methods focus on rectanglar regions In this paper, it proposed partial convolutionss convolution is masked and renormalized to be conditioned on only valid pixels generate update mask for the next layer focus on irregular mask 1.2. Contribution partial Conv (PConv) replace Conv with PConv get state-of-art first on irregularly shaped holes proposed a large irregular mask dataset 1.3. Related Work1.3.1. Non-learning Approach neighbouring pixel. can only handle narrow holes, where color and texture variance is small patch based. PatchMatch (faster similar patch searching algorithm) 1.3.2. Deep Learning Based initialize the holes with some constant placeholder values (mean pixel value of ImageNet) post-processing replace post-processing with refinement network ignore the mask placeholder values search closest encoding in a latent space 1.4. Dataset ImageNet Places2 CelebA-HQ 1.4.1. Augmentation dilation, rotation, cropping masks with and without holes close to border 1.5. Do Inpainting on SRextension experiments. offset pixels and insert holes (scaling factor k) 2. Methods 2.1. Partial Convolution (PConv) M. binary mask 1/sum(M). scaling factor to adjust for the varying amount of valid (unmasked) inputs After each PConv, update the mask mask will eventually be all ones 2.2. Network U-Net (8 encoder + 8 decoder)… No padding 2.3. Loss Function2.3.1. Total Loss 2.3.2. Pixel Loss 2.3.3. Perceptual Loss I_out. output image I_comp. I_out of hole + Input of non-hole ψ_n. nth layer of VGG16 2.3.4. Style Loss K_n. 1/chw 2.3.5. Total Variation (TV) Loss P. region of 1-pixel dilation of the hole region smoothing penalty. deal with the checkerboard artifacts of perceptual loss 3. Experiments 3.1. Comparison]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Image Processing</category>
        <category>Inpainting</category>
      </categories>
      <tags>
        <tag>Image Processing</tag>
        <tag>Image Enhancement</tag>
        <tag>Inpainting</tag>
        <tag>Partial Convolution</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(CVPR 2018) Gated fusion network for single image dehazing]]></title>
    <url>%2F2018%2F01%2F24%2FGated%20fusion%20network%20for%20single%20image%20dehazing%2F</url>
    <content type="text"><![CDATA[Ren W, Ma L, Zhang J, et al. Gated fusion network for single image dehazing[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018: 3253-3261. 1. Overview 1.1. MotivationMost existing methods follow the atmosphere scattering model. In this paper, it proposed Gated Fusion Network (GFN) directly restore a clear image from a hazy image encoder-decoder fusion-based strategy + confidence maps. White Balance, Contrast Enhancing and Gamma Correlation multi-scale to avoid halo artifacts GAN loss 1.2. Major Factors of Hazy Image color cast introduced by atmosphere light lack of visibility due to attenuation solution WB. eliminating chromatic casts CE. better global visibility in thick hazy region but too dark in light hazy region GC. recover the light hazy region 1.3. Contribution Network not follow the atmosphere scattering model demonstrate the utility and effectiveness of GFN multi-scale approch to eliminate halo artifacts 1.4. Related Work1.4.1. Multiple-image aggregation1.4.2. Hand-crafted Priors Based Methods maximize the contrast dark channel color-lines non-local fusion luminance, chromatic and saliency maps 1.4.3. Data-driven Methods combine four feature with Random Forest color attenuation prior deep learning 1.5. DatasetA∈(0.8, 1.0), β∈[0.5, 1.5]. random sample 7 groups, add 1% Gaussian noise. Train. 1400 images x7 Test. remained 49 images x7 RESIDE dataset. benchmark 2. Network 2.1. White Balance Input recover the latent color of the scene and eliminate chromatic cast but still present low contrast 2.1.1. Gary World Assumpationthe average value of the R,G,B components should average out to a common gray value. (Link) 2.2. Contrast Enhanced Input subtracting the average luminance value dark image region tend to black 2.3. Gamma Corrected Input overcome the dark limitation of CE 2.4. Network dilation Conv Relu 3 Conv + 3 DeConv, stride 1 first layer 5x5, other 3x3x32 2.5. Multi-Scale Refinement vary the image resolution (x2) to preserve halo artifacts loss function motivationthe human visual system is sensitive to local changes (e.g., edges) over a wide range of scales. As a merit, the multi-scale approach provides a convenient way to incorporate local image details over varying resolutions. 2.6. GAN Lossapply to finest image. 2.7. Total Loss 3. Experiments 3.1. Details 128x128 patches, batch size 10, 240,000 iteration Adam, 0.0001 LR, LR decay weight decay 0.00001 train 35 hours on K80 3.2. Comparison light, medium and heavy (β=0.8, 1.0, 1.2). 3.3. Ablation Study Multi-scale Gated Fusion 3.4. Limitation can not handle very large fog]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Image Processing</category>
        <category>Dehazing</category>
      </categories>
      <tags>
        <tag>Dehazing</tag>
        <tag>Image Processing</tag>
        <tag>Image Enhancement</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(AAAI 2018) Densely connected pyramid dehazing network]]></title>
    <url>%2F2018%2F01%2F24%2FDensely%20Connected%20Pyramid%20Dehazing%20Network%2F</url>
    <content type="text"><![CDATA[Zhang H, Patel V M. Densely connected pyramid dehazing network[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2018: 3194-3203. 1. Overview 1.1. MotivationMost existing method inaccuracies in the estimation of transmission map not leverage end-to-end training, unable to capture the inherent relation among transmission map, atmospheric light and dehazed image In this paper, it proposed Densely Connected Pyramid Dehazing Network (DCPDN) jointly learn transmission map, atmospheric light and dehazed image multi-level pyramid pooling for the estimation of transmission map edge-preserving loss function joint-discriminator stage-wise learning 1.2. Contribution jointly network jointly discriminator edge-preserving loss, edge-preserving pyramid densely connected encoder-decoder network experiments, ablation study 1.3. Related Work1.3.1. Prior-based dark-channel contrast color-lines haze-line1.3.2. Learning-based1.3.3. GAN 1.4. DatasetA∈[0.5, 1]， β∈[0.4, 1.6]. random sample 4 groups. TrainA. 1000 images from NYU-depth2, get 4000 paris TestA. 100 images from NYU-depth2, get 400 paris TestB. 200 paris fromMiddlebury and Sun3D 2. Network 2.1. Pyramid Densely Connected Transmission Map Estimation Net dense: maximize information flow. Encoder. 5 (dense block + down-sample transition block) Decoder. 5 (dense block + up-sample transition block) Multi-level pyramid pooling module. deal with lacking of global structure information 2.2. Atmosphere Light Estimation Net U-net. 4 (Conv-BN-Relu) + 4 (DeConv-BN-Relu) 2.3. Joint Discriminator 2.4. Edge-preserving Loss (set 1) L2 loss of transmission map (set 0.8) two-directional gradient loss edge corresponds to the discontinuities in the image intensities. (0.8) feature edge loss low-level features (edges and contour) can be captured in the shallow layers. relu1-1 and relu2-1 of VGG16. 2.5. Loss Function L^t. edge-preserving loss L^a. L2 of atmospheric loss L^d. L2 of dehazed loss (0.25) L^j. joint Discriminator loss Stage-wise training and then fine-tuned. 3. Experiments 3.1. Details LR. 0.002, Adam batch size 1, 512x512, 400000 iteration 3.2. Ablation Study multi-level pooling. better preserve the global structural for obj with relatively larger scale edge-preserving loss. better refined the edge of transmission map joint Discriminator. enhance the transmission map 3.3. Comparison]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Image Processing</category>
        <category>Dehazing</category>
      </categories>
      <tags>
        <tag>DenseNet</tag>
        <tag>Dehazing</tag>
        <tag>Image Processing</tag>
        <tag>Image Enhancement</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(CVPR 2018) Density-aware Single Image De-raining using a Multi-stream Dense Network]]></title>
    <url>%2F2018%2F01%2F23%2FDensity-aware%20Single%20Image%20De-raining%20using%20a%20Multi-stream%20Dense%20Network%2F</url>
    <content type="text"><![CDATA[Zhang H, Patel V M. Density-aware single image de-raining using a multi-stream dense network[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018: 695-704. 1. Overview 1.1. Motivation Non-uniform rain densities Existing method do not consider the density of rain drops (lead to over de-rain or under de-rain) [6]. Deep Detail Network. CVPR 2017 [33]. Deep Joint Rain Detection and Removal. CVPR 2017 In this paper, it proposed DID-MDN joint (two stage) rain density estimation and de-raining (guided by the estimated rain-density label) multi-stream (different scale feature) use residual to represent rain-density feature create a dataset containing rain-density label (heavy, medium, light) 1.2. Related Work video-based prior-based (over-smooth details) Multi-scale feature (U-Net, FCN, skip-connection) 1.3. Dataset 12,000 for training 1, 200 for testing use PS to get different level rain density (noise level [5%, 35%], [35%, 65%], [65%, 95%]) link 2. Architecture 2.1. Residual-Aware Rain-Density Classifiersingle network may not be sufficient enough to learn different rain-densities occurring in practice. used for guiding the de-raining process residual can better represent the rain feature observe that fine-tune pre-trained model is not an efficient solution. high-level feature focus on localizing the discriminative object (not small rain-streak), so it’s not good for density classify. 2.1.1. step estimate residual (residual feature extraction network) train classifier with residual (classifier) 2.1.2. Loss firstly train residual network then train classifier finally joint optimized 2.1.3. Classifier (low, medium, high) 2.2. Multi-Stream Dense Network Concat multi-stream feature and density label Refinement 2.3. Multi-Stream smaller rain-streak can be capture by small-scale feature longer rain-streak can be capture by larger-scale feature For each stream six dense blocks + six transition layer short path better for convergence Dense1. 3-down + 3-up (7x7) Dense2. 2-down + 2-no + 2-up (5x5) Dense3. 1-down + 4-no + 1-up (3x3) 2.4. Total Loss 3. Experiments 4. Dataset Train1. 12,000 Test1. 1,200 Test2. 1,000 (from Deep Detail Network) 4.1. Detail random crop, horizontal flip batch size 1 λF = 1 4.2. Ablation Study VGG vs Residual classifier Modules Single. single stream without label fusion Yang-Multi. multi-stream (dilated) Multi-no-label. multi stream without label fusion DID-MDN. multi-stream with label fusion Over de-rain with blur. Single and Yang-Multi Leave some rain-streak. Muti-no-label 4.3. Comparison Synthetic [33, 41] leave some rain-streak. [6] remove some details. Real long-thin rain-streak heavy rain small round rain medium reain 4.4. Inference Time]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Image Processing</category>
        <category>De-raining</category>
      </categories>
      <tags>
        <tag>Image Processing</tag>
        <tag>Image Enhancement</tag>
        <tag>De-raining</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(AAAI 2018) End-to-End United Video Dehazing and Detection]]></title>
    <url>%2F2018%2F01%2F23%2FEnd-to-End%20United%20Video%20Dehazing%20and%20Detection%2F</url>
    <content type="text"><![CDATA[Li B, Peng X, Wang Z, et al. End-to-end united video dehazing and detection[C]//Thirty-Second AAAI Conference on Artificial Intelligence. 2018. 1. Overview 1.1. Motivation end-to-end video dehazing has not been explored temporal consistency In this paper, it proposed EVD-Net (End-to-End Video Dehazing Network) EVDD-Net (End-to-End United Video Dehazing and Detection Network) 1.2. Related Work Classical Atmosphere Scattering Model DehazeNet MSCNN AOD-Net. re-formulationVideo Dehazing 1.3. Dataset1.3.1. Dehazing Synthetic Hazy Video Dataset from TUM RGB-D Dataset refined depth infomation TestSet. video from city road when PM2.5 is 223 1.3.2. Dehazing + Detection ILSVRC2015 VID estimated depth by paper 2016 1.4. Start Pointbased on AOD-Net architecture. 1.5. EVD-Net Multi-framethe global atmosphere light A should be hardly or slowly changed over a moderate number of consecutive frame. 1.6. EVDD-Net Multi-Frame Faster RCNN EVD-Net 2. Experiments 2.1. Details MSE loss well aligned with SSIM and visual quality 2.2. Fusion Strategy 2.3. Comparison of Dehaze 2.4. Comparison of Detection naive concatenation of low-level and high-level models often can not sufficiently boost the high-level task performance JAOD-Faster RCNN. flickering and inconsistent detection Only EVDD-Net detection 4 cars in all frames]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Image Processing</category>
        <category>Dehazing</category>
      </categories>
      <tags>
        <tag>Dehazing</tag>
        <tag>Image Processing</tag>
        <tag>Image Enhancement</tag>
        <tag>Video</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(CVPR 2018) Locally Adaptive Learning Loss for Semantic Image Segmentation]]></title>
    <url>%2F2018%2F01%2F23%2FLocally%20Adaptive%20Learning%20Loss%20for%20Semantic%20Image%20Segmentation%2F</url>
    <content type="text"><![CDATA[Guo J, Ren P, Gu A, et al. Locally Adaptive Learning Loss for Semantic Image Segmentation[J]. arXiv preprint arXiv:1802.08290, 2018. 1. Overview 1.1. Motivation Most loss layer focus on pixel-wise, ignore spatial layout and interaction with neighbouring pixel (not sensitive to intra-class connection) In this paper, it proposed Locally Adaptive Learning Loss for segmentation merge predicted distribution over a small group of neighbouring pixels with same category (imporve the capability of discriminating targets from both inter- and intra- class) sliding window + ensemble by Minkowski pooling (focus on high loss, rebalancing) 1.2. Related Work contrastive loss, triplet loss and center loss pixel-wise loss. collapse the spatial dimension Aligned RoI. sigmoid + binary loss (ROI maintain spatial layout) Loss Max-Pooling. handle imbalanced inter-class dataset in segmentation (assign weight to each pixel based on their losses) weighted ensemble entropy estimator. better accuracy and higher converge rate 1.3. Locally Adaptive Loss Selective Ensemble m. number of the pixel which have the same label μ. select same label pixel wd. Gaussian weight based on chessboard distance x. pixel when ε is softmax cross-entropy Batch Pooling Mp. number of batchMinkowski Pooling. when k increases, it will focus on high loss value and increase the impacts of mispredited samples of intra-class 1.4. Dataset &amp; MetricVOC2012, IoU 1.5. Experiments DeepLabV2 (disable multi-scale and CRF) Batch size 2 Crop size 321x321]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Architecture</category>
        <category>Loss Function</category>
      </categories>
      <tags>
        <tag>Segmentation</tag>
        <tag>Loss Function</tag>
        <tag>Locally Adaptive Learning Loss</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(CVPR 2018) Residual Dense Network for Image Super-Resolution]]></title>
    <url>%2F2018%2F01%2F23%2FResidual%20Dense%20Network%20for%20Image%20Super-Resolution%2F</url>
    <content type="text"><![CDATA[Zhang Y, Tian Y, Kong Y, et al. Residual dense network for image super-resolution[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018: 2472-2481. 1. Overview 1.1. Motivation Most SR models do not make full use of the hierarchical feature from the original LR image Hierarchical features give more clues for reconstruction Interpolation of LR image will increase computation and lose details of LR image Higher growth rate of dense block can improve performance but hard to train In this paper, it proposed residual dense network (RDN) consist of residual dense block (RDB) contiguous memory mechanism (CM) local feature fusion (LFF). deal with high growth rate global feature fusion (GFF) local residual learning (LRL) global residual learning (GRL) 1.2. Related Work VDSR, IRCNN. residual learning DRCN, DRRN. recursive learning Memnet. memory block ESPCN. sub-pixel 1.3. Architecture contains four parts SFENet (shallow feature extraction net) RDBs (residual dense blocks) DFF (dense feature fusion)contains GFF (1x1 for fusion, 3x3 for extraction effective in another paper). UPNet (up-sampling net) inspired by CVPRW 2017 1.4. RDB CMpassing the state of preceding RDB to each layer of current RDB. LFFfind that as G grows, very deep dense nerwork without LFF would be hard to train. LRL 1.5. Details remove BN (performance and memory) remove pooling L1 loss self-ensemble method 1.6. Dataset DIV2K (800 train, 100 valid, 100 test) 5 benchmark dataset for testing (Set5, Set14, B100, Urban100, Manga109) 1.7. Degradation Model bicubic Gaussian kernel bicubic + Gaussian noise 2. Experiments 2.1. Study of D, C, G D. number of RDB C. number of conv layers in RDB G. growth rate the larger the better. 2.2. Ablation Study 2.3. Comparison when scaling factor become larger, RDN not better for MDSR (depth, multi-scale, larger patch size). Method using interpolated LR image would produce artifacts and blur.]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Image Processing</category>
        <category>Super Resolution</category>
      </categories>
      <tags>
        <tag>Image Processing</tag>
        <tag>Image Enhancement</tag>
        <tag>Super Resolution</tag>
        <tag>RDN</tag>
        <tag>RDB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(ICPR 2018) Deep joint rain and haze removal from single images]]></title>
    <url>%2F2018%2F01%2F23%2FDeep%20joint%20rain%20and%20haze%20removal%20from%20single%20images%2F</url>
    <content type="text"><![CDATA[Shen L, Yue Z, Chen Q, et al. Deep joint rain and haze removal from a single image[C]//2018 24th International Conference on Pattern Recognition (ICPR). IEEE, 2018: 2821-2826. 1. Overview 1.1. Motivation Rain streak correspond to high-frequency Haze correspond to dark channel Paper proposed Deep Joint Rain and Haze Removal Network (DJRHR-net) Haar wavelet transform Extract dark channel as input 1.2. Wavelet Transformation LL. background HL. vertical LH. horizontal HH. diagonal 1.3. Simple Rain Removal Network (SRR-net)Spectrum of an image loses a lot of great properties such as local receptive field, which makes it difficult to use convolutional neural network Process Loss function 1.4. DJRHR-netIt is more effective to add the artificial feature directly than the features learned by the deep network. Process Loss Function 1.5. Dataset1.5.1. TrainSetA Directly used Deep Detail Network’s Without haze veil 12 type rain streaks 1.5.2. TrainSetB 1449 RGBD from NYU Depth V2 12 type rain streaks 2. Experiments 2.1. Training Setup Dense Block Remove BN and pooling to get better result 2.2. Metric PSNR SSIM NIQE. the lower the better 2.3. Synthetic 2.4. Real 2.5. DenseBlock parameters]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Image Processing</category>
        <category>De-raining</category>
      </categories>
      <tags>
        <tag>Dehazing</tag>
        <tag>Image Processing</tag>
        <tag>Image Enhancement</tag>
        <tag>De-raining</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(CVPR 2018) Zero-Shot Super-Resolution using Deep Internal Learning]]></title>
    <url>%2F2018%2F01%2F23%2FZero-Shot%20Super-Resolution%20using%20Deep%20Internal%20Learning%2F</url>
    <content type="text"><![CDATA[Shocher A, Cohen N, Irani M. “Zero-Shot” Super-Resolution using Deep Internal Learning[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018: 3118-3126. 1. Overview 1.1. Motivation Existing SR supervised method rely on prior training (restricted to training data) Non-ideal acquisition process in reality (not always bicubic, bilinear), old photo, noisy image, biological data, phone image Natural images have strong internal data repetition (at different location and scale). Internal entropy of patches inside a single image is much smaller than the external entropy of patches in a general collection of natural images 论文提出Zero-Shot SR方法 Not rely on prior training Exploit internal recurrence of information inside a single image Perform SR on real image where the acquisition process is unknown and non-ideal Train a small CNN at test time only from the LR image and its downscaled (self-supervision) train + test time is comparable to the test time of SotA 1.2. Contribution First unsupervised CNN-based SR method Handle non-ideal condition Not require pretraining SR any size 1.3. Related WorkSupervised EDSR VDSR Unsupervised (not deep learning) Blind-Deblurring Blind-Dehazing Blind-SR 1.4. Model Downscaling test image to many smaller version of itself (HR-fater vs LR-son). Gradually SR. Several intermediate scale-factors 4 Rotations (0, 90, 180, 270) and 2 flip Learn residual Select rate based on proportional to the size of the HR-father. Size close to test image has high rate 8 output of test image (4R x 2F), take median + back-projection More intermediate scale-factor increase accuracy Time independent of image size and scale-factor 54 s/img on GPU, EDSR+ 20s on 200x200 image, EDSR 5min on 800x800 image 1.5. Parameters Downscaling kernel Scale-factor Number of gradual scale increase Whether Backprojection Whether add noise (learn to ignore uncorrelated cross-scale information) 1.6. Future Work Combine Internal-Learning with External-Learning in a single computational framework 2. Experiments 2.1. Ideal Case (bicubic kernel) 2.2. Non-ideal Case2.2.1. Deviate from bicubic Random Gaussian Kernel Two case apply to ZSSR Blind-SR evaluate kernel, feed to ZSSR True kenel feed to ZSSR Solution Accurate downscaling model is more important than sophisticated image prior Wrong downscaling kernel lead to oversmoothed SR result 2.2.2. Low-quality LR image Gaussian noise Speckle noise JPEG compression]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Image Processing</category>
        <category>Super Resolution</category>
      </categories>
      <tags>
        <tag>Image Processing</tag>
        <tag>Image Enhancement</tag>
        <tag>Super Resolution</tag>
        <tag>Unsupervised</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(CVPR 2018) Deep Image Prior]]></title>
    <url>%2F2018%2F01%2F23%2FDeep%20Image%20Prior%2F</url>
    <content type="text"><![CDATA[Ulyanov D, Vedaldi A, Lempitsky V. Deep image prior[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018: 9446-9454. 1. Overview 目前深度学习都是learning-based方式，论文将深度学习作为learning-free方式 used as handcrafted priors to solve inverse problems, such as denoising, super-resolution, inpainting, restore image based on flash-no flash image generator network is sufficient to capture low-level image statistics proir bridge the gap between learning-based and laerning-free first study to investigates the prior captured by deep convolutional generative networks independently of learning the network parameters from images 1.1. Loss Function input. random noise, usually initialize randomly and keep it fixed gt. noise image 1.2. Converge 图像的收敛比噪声快 1.3. 应用 Denoising Super-Resolution input. random noise gt. H x W output. tH x tW, and then downsampled H x W Inpainting output. x gt. x0, image with missing pixel m. binary mask skip-connection对该类任务的效果较差 Natural Pre-image pre-image loss function Flash-no Flash Reconstruction]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Image Processing</category>
      </categories>
      <tags>
        <tag>Image Processing</tag>
        <tag>Image Enhancement</tag>
        <tag>Super Resolution</tag>
        <tag>Inpainting</tag>
        <tag>Inverting</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(ECCV 2018) Discriminative Region Proposal Adversarial Networks for High-Quality Image-to-Image Translation]]></title>
    <url>%2F2018%2F01%2F23%2FDiscriminative%20Region%20Proposal%20Adversarial%20Networks%20for%20High-Quality%20Image-to-Image%20Translation%2F</url>
    <content type="text"><![CDATA[Wang C, Zheng H, Yu Z, et al. Discriminative region proposal adversarial networks for high-quality image-to-image translation[C]//Proceedings of the European Conference on Computer Vision (ECCV). 2018: 770-785. 1. Overview 论文提出Discriminative Region Proposal Adversarial Network (DRPAN)结构，并用于一些任务 de-raining image-to-image translation 1.1. 模型 1.1.1. Discriminator generate score map find the most artificial patch by sliding window map the patch into the real image, generate fake-mask real w. sliding window size w_s. score map size w_i. input image size w_*. patch size of input image that window map to x_c, y_c. center point of score map patch τ. scale d_r. discriminate region of input image 1.1.2. Reviser discriminate 1.2. Loss Function Discriminator Reviser δ. noise Generator 1.3. Algorithm 1.4. 数据集 Cityscapes Facades SUN6 UT Zappos 50K edge to handbag 1.5. Related Work ID-CGAN Pix2Pix CRN (cascaded refined network) 1.6. Metric PSNR SSIM VIF RECO IOU FCN-score AMT Inception score 1.7. 实验结果]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Image Processing</category>
        <category>Dehazing</category>
      </categories>
      <tags>
        <tag>Image Processing</tag>
        <tag>Image Enhancement</tag>
        <tag>GAN</tag>
        <tag>De-raining</tag>
        <tag>Image-to-Image Translation</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(CVPR 2018) xUnit:Learning a Spatial Activation Function for Efficient Image Restoration]]></title>
    <url>%2F2018%2F01%2F22%2FxUnit%3A%20Learning%20a%20Spatial%20Activation%20Function%20for%20Efficient%20Image%20Restoration%2F</url>
    <content type="text"><![CDATA[Kligvasser I, Rott Shaham T, Michaeli T. xUnit: learning a spatial activation function for efficient image restoration[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018: 2433-2442. 1. Overview 目前 0% parameters in Activation unit (nonlinearities), Conv (spatial processing) Network deeper and deeper 因此，论文提出xUnit结构 learnable nonlinear function with spatial connection reduce layer used in low-level task (denoising, de-raining, super resolution) 1.1. Related Work ELUs SRCNN VDSR SRResNet EDSR ESPCN Binarized Neural Networks Deep Detail Network DehazeNet MobileNet 1.2. 模型 Conv+Activation形式如下： 1.2.1. ReLU o表示点乘，定义0/0=0. 1.2.2. xUnit H_k为depth-wise convolution.对于d-channel输入，d-channel输出而言，计算复杂度为 标准Conv. rxrxdxd depth-wise Conv. rxrxd 2. Experiments 2.1. Compare 2.2. 特征图可视化ReLU丢弃了大部分特征图（黑色），而xUnit大部分特征图都参与后续计算（白色）。 2.3. Denoising 2.4. De-raining (PSNR) 28.94 VS 31.17. 2.5. Super Resolution]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Architecture</category>
        <category>Component</category>
      </categories>
      <tags>
        <tag>Component</tag>
        <tag>Super Resolution</tag>
        <tag>De-raining</tag>
        <tag>xUnit</tag>
        <tag>Activation Unit</tag>
        <tag>Denoising</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(2018) WaterGAN:unsupervised generative network to enable real-time color correction of monocular underwater images]]></title>
    <url>%2F2018%2F01%2F22%2FWaterGAN%3A%20unsupervised%20generative%20network%20to%20enable%20real-time%20color%20correction%20of%20monocular%20underwater%20images%2F</url>
    <content type="text"><![CDATA[Li J, Skinner K A, Eustice R M, et al. WaterGAN: unsupervised generative network to enable real-time color correction of monocular underwater images[J]. IEEE Robotics and Automation Letters, 2018, 3(1): 387-394. 1. Overview 论文 基于SimGAN的思想，提出WaterGAN模型。通过un-supervised方法利用WaterGAN将in-air image渲染成underwater image 基于渲染得到的underwater image，通过supervised方法利用Restoration Network将underwater image恢复成in-air image 1.1. Related Work CNN SimGAN RenderGAN 1.2. 模型1.2.1. WaterGAN 输入大小：48x64 Attenuation η：wavelength-dependent attenuation coefficient estimated by networkr_c：range from camera to sceneλ：color channel约束η≥0：确保颜色衰减而不增强 Scattering 输入：48x64 depth map + 100 noise (project, reshape, concat) 计算：3个并行CNN 输出：48x64x3 mask Camera Modela, b, c, k estimated by network. 约束条件 1.2.2. Restoration Network 使用segNet中的non-parametric upsampling layer (uses the index information from corresponding max-pooling layers). Depth Estimation Network 输入：56x56x3 downsampled 输出：56x56x1 Loss：L2 Color Restoration Network 输入：480x480x1 upsampled and then padded to 512x512x1 下采样：128x128 AvgPooling Core Component 上采样：512x512 DeConv (initialized by bilinear interpolation) Loss：L2 1.3. 数据集 Synthetic4 Kinect dataset (B3DO, UW RGB-D Object, NYU Depth, Microsoft 7-scenes). Total 15000 RGB-D images (12000 training, 3000 validation). MHL (University of Michigan’s Marine Hydrodynamics Laboratory)7000 underwater images. Port Royal6500 underwater images, maximum depth 1.5m. Lizard Island6083 underimages, maximum depth 2.0m. 1.4. Metric Color Accuracy Color Consistenct 1.5. 实验结果 MHL Validation Skip Connection]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Image Processing</category>
        <category>DeWater</category>
      </categories>
      <tags>
        <tag>Image Processing</tag>
        <tag>Image Enhancement</tag>
        <tag>GAN</tag>
        <tag>SimGAN</tag>
        <tag>DeWater</tag>
        <tag>WaterGAN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(AAAI 2018) Towards Perceptual Image Dehazing by Physics-based Disentanglement and Adversarial Training]]></title>
    <url>%2F2018%2F01%2F22%2FTowards%20Perceptual%20Image%20Dehazing%20by%20Physics-based%20Disentanglement%20and%20Adversarial%20Training%2F</url>
    <content type="text"><![CDATA[Yang X, Xu Z, Luo J. Towards perceptual image dehazing by physics-based disentanglement and adversarial training[C]//Thirty-second AAAI conference on artificial intelligence. 2018. 1. Overview 现有的de-hazing方法 on synthetic dataset hand-designed priors supervised training 因此，论文提出Disentangled Dehazing Network weakly-supervised GAN, multi-scale discriminator physical-model based disentanglement reconstruction collect HazyCity dataset 1.1. Related Work DehazeNet MSCNN AOD-Net CycleGAN dualGAN AIGN WaterGAN discoGAN UNIT 1.2. 模型 1.2.1. Generator G_J. 生成clean image G_A.生成atmosphere light G_t. 生成transmission map 1.2.2. Discriminatormulti-scale结构 local discriminator (感知域70x70). Model high-frequency structure (texture/style recognition) global discriminator (感知域256x256). Global information, alleviate artifacts 1.3. Reconstruction Reconstruction Loss Adversarial Loss Regularization Lossthe smoothness of the medium transmission map. 1.4. Recovering Method of Haze-free Image The output of G_J Generate from A and t Combine 1.5. 数据集 D-HAZY (synthetic) β=1, A=255. NYU-Depth (23 images). Middlebury (1449 images). HazyCity (real) natural, outdoor. Build on PM25 dataset. hazy (845), haze-free (1891) crawled from tourist website and photos of various attraction sites and street scenes in Beijing. 三个标注者，选取标注一致的图片。 1.6. Future Work de-raining image matting 2. Experiments 2.1. Baseline2.1.1. prior-based DCP CAP NCP 2.1.2. learning-based DehazeNet MSCNN CycleGAN 2.2. Metric PSNR SSIM CIEDE2000. measure color difference 2.3. 实验结果 2.4. Ablation Study]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Image Processing</category>
        <category>Dehazing</category>
      </categories>
      <tags>
        <tag>Dehazing</tag>
        <tag>Image Processing</tag>
        <tag>Image Enhancement</tag>
        <tag>GAN</tag>
        <tag>Unsupervised</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(2017) Single Image Deraining using Scale-Aware Multi-Stage Recurrent Network]]></title>
    <url>%2F2018%2F01%2F22%2FSingle%20Image%20Deraining%20using%20Scale-Aware%20Multi-Stage%20Recurrent%20Network%2F</url>
    <content type="text"><![CDATA[Li R, Cheong L F, Tan R T. Single Image Deraining using Scale-Aware Multi-Stage Recurrent Network[J]. arXiv preprint arXiv:1712.06830, 2017. 1. Overview 现实生活中rain的两个显著特点 Rain streaks of various sizes and directions can overlap each other Veiling effect 因此，论文提出Scale-aware Multi-stage CNN parallel sub-network to deal with different rain streaks veil module to deal with veiling effect multi-stage to deal with rain streaks accumulation (DenseNet能够提高效果) 1.1. Related Work DetailsNet JORDER Have not been subject to the full force of the tropical heavy rain. Have not been tested where the scenes contain a range of depths. 2. Model 2.1. Rain Model 2.2. FrameworkDenseNet结构去掉transition layer，不使用down-sampling. Veil Module 训练集包含不同的A值。测试阶段将最亮的pixel设为A. 2.3. Loss Function 3. Experiments 3.1. 数据集 BSD300 rain size (area). small (0, 60], middle (60, 300], large (300, 600). 3300 rain images containing 11 rain streak orientation. NYU (depth information) Rain12 12 synthetic rain image with one type streak. Rain12S extension of Rain12. various sizes and densities of streak. Rain100-COCO rander different-sized streak on 100 images from COCO. Rain12-Veil rander streak and atmosphere veils. 12 images from BSD300. 3.2. 实验结果 3.3. Ablation Study]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Image Processing</category>
        <category>De-raining</category>
      </categories>
      <tags>
        <tag>Image Processing</tag>
        <tag>Image Enhancement</tag>
        <tag>De-raining</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(2017) Joint Transmission Map Estimation and Dehazing using Deep Networks]]></title>
    <url>%2F2018%2F01%2F22%2FJoint%20Transmission%20Map%20Estimation%20and%20Dehazing%20using%20Deep%20Networks%2F</url>
    <content type="text"><![CDATA[Zhang H, Sindagi V, Patel V M. Joint transmission map estimation and dehazing using deep networks[J]. arXiv preprint arXiv:1708.00581, 2017. 1. Overview 大多数现有的方法假设constant atmosphere light，包含两个步骤 基于prior-based方法估计transmission map 近似解计算haze-free image 论文提出multi-task结构 Relax constant atmosphere light assumption, joint estimate transmission map and de-hazing Introduce GAN Introduce perceptual loss 1.1. Related Work DehazeNet Multi-scale Net 1.2. Model PReLU Generator使用U-Net结构 1.3. Loss Function Transmission Map Loss Dehazing LossPerceptual loss (VGG-16 relu3_1) 1.4. 速度512x512. 18 fps 1.5. 数据集α ∈ [0.5, 1.2]. β ∈ [0.4, 1.6] 1.5.1. NYU Depth Dataset Training Set. 600 images x 4 Testing Set. 300 images x 4 Real Set. 30 images 2. Experiments 2.1. AblationStudy Adversarial Loss Perceptual Loss Euclidean Loss Transmission Map 2.2. 实验结果]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Image Processing</category>
        <category>Dehazing</category>
      </categories>
      <tags>
        <tag>Dehazing</tag>
        <tag>Image Processing</tag>
        <tag>Image Enhancement</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(2018) CANDY:Conditional Adversarial Networks based Fully End-to-End System for Single Image Haze Removal]]></title>
    <url>%2F2018%2F01%2F22%2FCANDY%3A%20Conditional%20Adversarial%20Networks%20based%20Fully%20End-to-End%20System%20for%20Single%20Image%20Haze%20Removal%2F</url>
    <content type="text"><![CDATA[Swami K, Das S K. Candy: Conditional adversarial networks based fully end-to-end system for single image haze removal[J]. arXiv preprint arXiv:1801.02892, 2018. 1. Overview 现有的去雾方法关注intermediate parameter (transmission map)，并没有将haze-free image quality考虑到optimization framework中。而intermediate parameter的估计误差会进一步影响haze-free image的质量。 因此，论文提出CANDY (Conditional Adversarial Networks based Dehazing of hazY images)结构 First work of end-to-end de-hazing to generate haze-free image First work of introducing GAN for de-hazing 1.1. 速度On GPU. 256x256. 35ms 1024x1024. 53ms Model size 3MB 1.2. Related Work估计intermediate parameter，没有将image quality考虑到optimization framework中。 DehazeNet Multi-scale Net 1.3. Model 1.3.1. Generator 6 Conv + 6 Deconv. 3x3 kernel size, 64 channels, 1 stride, 1 padding Down-Sampling会导致图片特征丢失 PReLU 1.3.2. Discriminator 7 Conv. 3x3, 2s, 1p, double channel Leaky ReLU. λ=0.2 1.4. Loss Function Content Loss Feature Reconstruction Loss 使用VGGNet提取 9(relu2_2) 16(relu3_3) 23(relu4_3) 1.5. 数据集 Make3D Depth BSDS500 MeddleBury NYU Depth只包含indoor images 论文使用CVPR 2015的single image depth estimation方法估计图片的depth α = [k, k, k], k ∈ [0.7, 1], β ∈ [0.5, 1.5] 1.5.1. Training Set 700 x 3 images. 500 from Make3D, 200 from BSDS500 1.5.2. Validation Set 40 images 1.5.3. Testing Set Test-Synthetic-A. 90 from Make3D and BSDS500 Test-Synthetic-B. 23 from Middlebury Test-Real-500 2. Experiments 2.1. Baseline GEN. L2 loss + 9th L2 loss CANDY_L1_9P. 500 iteration GEN initialized … 2.2. 模型选择 Smooth L1比L2更有效，并且能够稳定GAN训练 Lower Layers feature reconstruction的结果更好。可能是因为higher layer preserve spatial structure，而忽略了texture and color 最终选择CANDY_L1_9P。 2.3. 实验结果 2.4. Night Hazy ImageAlthough train on daytime hazy image, it can work on night hazy image.]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Image Processing</category>
        <category>Dehazing</category>
      </categories>
      <tags>
        <tag>Dehazing</tag>
        <tag>Image Processing</tag>
        <tag>Image Enhancement</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(ITIP 2016) Dehazenet:An end-to-end system for single image haze removal]]></title>
    <url>%2F2018%2F01%2F22%2FDehazenet%3A%20An%20end-to-end%20system%20for%20single%20image%20haze%20removal%2F</url>
    <content type="text"><![CDATA[Cai B, Xu X, Jia K, et al. Dehazenet: An end-to-end system for single image haze removal[J]. IEEE Transactions on Image Processing, 2016, 25(11): 5187-5198. 1. Overview 论文提出DehazeNet结构 Maxout unit to generate almost all haze-relevant feature Bilateral rectified linear unit (BReLU) 1.1. 模型结构 Local Extremum假设medium transmission是局部常量。 1.2. BReLU 1.3. Maxout Maxout activation functions can be considered as piece-wise linear approximations to arbitrary convex functions. 1.4. Haze-Relevant Feature Dark Channel Haze-free patches中至少有一个channel中的一些像素值非常低，接近0. 因此，dark channel feature与haze amount高相关，能够用来估计medium transmission. Maximum Contrast Haze transmission会减小对比度。因此，contrast与medium transmission高相关。 Color AttenuationPrior：Hazy导致saturation下降，brightness上升。 Color attenuation feature： 与depth成正比，能够用于transmission estimation. Hue DisparityOriginal image与semi-inverse image之间的hue disparity能够用于检测haze. Semi-inverse Image]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Image Processing</category>
        <category>Dehazing</category>
      </categories>
      <tags>
        <tag>Dehazing</tag>
        <tag>Image Processing</tag>
        <tag>Image Enhancement</tag>
        <tag>Dehazenet</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(ECCV 2016) Single image dehazing via multi-scale convolutional neural networks]]></title>
    <url>%2F2018%2F01%2F22%2FSingle%20image%20dehazing%20via%20multi-scale%20convolutional%20neural%20networks%2F</url>
    <content type="text"><![CDATA[Ren W, Liu S, Zhang H, et al. Single image dehazing via multi-scale convolutional neural networks[C]//European Conference on Computer Vision. Springer International Publishing, 2016: 154-169. 1. Overview 现有的hand-designed feature dark channel color disparity maximum contrast fusion schemes 存在局限性。因此，论文提出multi-scale结构 coarse-scale net. predict holistic transmission map fine-scale net. refine result locally 1.1. 模型 网络使用max-pooling增加模型的非线性能力，而输出的t需要与hazy image尺寸一样，因此使用bilinear interpolation. 网络预测透射率矩阵t. 选择t中0.1%透射率最小的像素点位置，对应到hazy image像素点，其中强度最大的值作为A 当t为0时，对其设置阈值 1.2. Loss Function 同时用在coarse-scale和fine-scale的网络输出上. 1.3. 数据集 A = [k, k, k], k ∈ [0.7, 1.0] β ∈ [0.5, 1.5] β ∈ (0, 0.5)，透射率增大，导致thin haze, boost noise β ∈ (1.5, OO)， 透射率减小 利用数据集提供的depth-meta生成数据. 320 x 240 NYU Depth Dataset Training Set. 6000 images x 3 Middlebury Stereo Dataset Valid Set. 50 images x 3 1.4. Related Work Multiple images Albedo Maximizing local contrast Boundary constraint and contextual regularization Dark channel prior Random Forest 2. Experiments 2.1. 实验结果 He.估计的透射矩阵是均匀分布的，一些区域中的hazy thickness overestimated. 2.2. 运行时间 2.3. 泛化性虽然real image depth通常很大，而训练集中的depth较小。但是可以通过调整介质的消光系数（medium extinction coefficient）来增加雾的浓度。这种方案独立于depth，能够包含real transmission map的范围。 2.4. Fine-Scale的作用 3-scale网络性能并没有提升 single-deep-scale网络性能降低 得出结论：相比于high-level task，deep structure对low-level task并不明显。 2.5. Up-Sampling的作用 (c)不使用pooling，效果较差 虽然(d)与(b)相似，但(b)计算量较大 2.6. 特征提取方法对比 CNN提取的特征类似于dark channel, local max contrast. 2.7. Failure Case 由于缺少相应数据，对夜间hazy image效果并不好 2.8. Future Work 处理上述缺陷 End-to-end同时估计transmission map, atmosphere light]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Image Processing</category>
        <category>Dehazing</category>
      </categories>
      <tags>
        <tag>Dehazing</tag>
        <tag>Image Processing</tag>
        <tag>Image Enhancement</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(ICCV 2017) Aod-net:All-in-one dehazing network]]></title>
    <url>%2F2018%2F01%2F21%2FAod-net%3A%20All-in-one%20dehazing%20network%2F</url>
    <content type="text"><![CDATA[Li B, Peng X, Wang Z, et al. Aod-net: All-in-one dehazing network[C]//Proceedings of the IEEE International Conference on Computer Vision. 2017: 4770-4778. 1. Overview 此前的大多数工作都是独立估计 Transmission matrix Atmosphere light 但是，通过non-joint估计的这两个参数在同时使用时，可能会进一步放大误差。 DehazeNet overestimate A and cause overexposure visual effect 因此论文 基于re-formulated atmosphere scattering model 提出AOD-Net，直接生成clean image AOD-Net可直接嵌入到其他模型中，如Faster R-CNN 0.026s to process 480x640 image with a single GPU 1.1. Re-formulated Atmosphere Scattering Model 基本物理模型 Re-formulated估计K（即联合估计A和t） 1.2. Model 1.3. 相关工作 Maximizing the local contrast Estimate the albedo of the scene Dark channel prior Enforce the boundary constraint and contextual regularization Color attenuation prior and A linear model of scene depth (Deep Learning) MSCNN. Multi-scale, coarse-to-fine transmission matrix DehazeNet. end-to-end for transmission estimation 1.4. 数据集根据基本物理模型，通过设置不同的参数合成haze（数据集提供depth-meta） A. [0.6, 1.0],choosing each channel uniformly β. {0.4, 0.6, 0.8, 1.0, 1.2, 1.4, 1.6} NYU2 Depth Database 训练集. 27,256张图片 TestSetA. 3,170张图片 Middlebury Stereo Database TestSetB. 800张图片 Natural Hazy Image 2. Experiments 2.1. Synthetic实验结果 图片可以分解两个元素的和 Mean. 所有像素点的值设置为同一个均值，即与A有关的global illumination Residual. local structural variations and contrast 两张图片的MSE，可看作是Mean和Residual两部分的MSE 相比于local distortion，人眼对global illumination更敏感 实验证明，AOD-Net的Residual MSE与其他方法相近，但是Mean MSE更低 说明AOD-Net能够更好地recover A. 2.2. 运行时间 2.3. Natural实验结果 2.4. Anti-halation 2.5. 白色背景 2.6. 嵌入到Faster R-CNN中雾霾程度 Heavy. A=1, β=0.1 Medium. A=1, β=0.06 Light. A=1, β=0.04]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Image Processing</category>
        <category>Dehazing</category>
      </categories>
      <tags>
        <tag>Dehazing</tag>
        <tag>Image Processing</tag>
        <tag>Image Enhancement</tag>
        <tag>AOD-Net</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(CVPR 2017) Deep joint rain detection and removal from a single image]]></title>
    <url>%2F2018%2F01%2F21%2FDeep%20joint%20rain%20detection%20and%20removal%20from%20a%20single%20image%2F</url>
    <content type="text"><![CDATA[Yang W, Tan R T, Feng J, et al. Deep joint rain detection and removal from a single image[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2017: 1357-1366. 1. Overview 现实生活中的雨滴具有以下特性 远处的雨滴堆积产生类似于雾的大气遮盖物，散射光线，导致可见度降低 近处的雨滴产生镜面反射，遮挡背景 雨滴具有不同形状和方向. 尤其是大雨，可见度很低 现有的方法存在一些不足之处 雨滴与背景纹理存在重叠，去雨导致over-smoothing the regions 雨滴的退化效果很复杂，现有方法不能充分包含所有重要的因素 基于local patch或有限receptivefield 因此，论文 对雨滴进行建模 创建数据集Rain100H 并在此基础上提出multi-task深度学习框架 1.1. Contribution First method to model rain-streak binary mask, atmosphere veils and various shapes and directions of overlapping rain streaks First method to jointly detect and remove rains First rain method to use contextualized dilated network First method use recurrent to address heavy rain 1.2. 相关工作 Sparse coding. Learned dictionary Low rank Non-local mean filter Discriminative sparse coding GMM Denoising, complement, super-resolution, deblurring, deconvolution, style tansfer, dehazing, light enhancement. 1.3. 数据集数据集中的图片选择BSD200 Rain12, Rain100L. one type of rain streak Rain100H (论文提出). five type of rain streak，虽然现实生活中很少同时存在多种类型的rain streak. 但这样的训练数据能够提升网络性能 d. simulated sharp line streaks along a certain direction with a small variation 2. Rain Image Model 2.1. Widely Used Model此前的一些工作用 表示rain image model. 将rain removal转化为分离两个信号问题. O. Input image or Observe image B. Background layer S. rain streak layer但该模型存在缺陷 不同region中的S具有不同的密度，无法使用统一的S表示 没有区分rain region和no-rain region，造成no-rain region过度平滑 2.2. Region-dependent Model R为binary mask. 1表示rain region. 0表示no-rain region. 公式表明 提供额外gt (rain streak region)给神经网络模型. Multi-task pipeline. 检测rain region, 对rain region和no-rain region进行不同的处理 2.3. Rain Accumulation Model雨滴存在堆积的情况，并形成类似与雾的视觉效果。 Koschmieder模型适用于近似多种浓密的介质，如雾、水下等。因此，可将模型表示为 t. rain-streak layer index s. rain-streak layer number A. global atmosphere light α. atmosphere transmission 公式表明：可独立处理rain removal和rain accumulation 3. Architecture 3.1. Joint Rain Streak Detection and Removal (JORDER) 结构 通过Contextualized Dilated Network提取特征F 基于F预测R 基于[F, R]预测S 基于[F, R, S]预测SR B = O - SR Recurrent也可采用其他的策略：并行预测等。 3.2. Contextualized Dilated Network 分支一：Conv 3x3 1 dilate, Conv 3x3 1 dilate, 感知域5x5 分支二：Conv 3x3 2 dilate, Conv 3x3 2 dilate, 感知域9x9 分支三：Conv 3x3 3 dilate, Conv 3x3 3 dilate, 感知域13x13 3.3. Loss Function 3.4. Recurrent 每次迭代的过程 最终生成图片 τ为迭代次数 Loss Function 3.5. Rain-Accumulation Removal 虽然公式3表明，首先应做rain-accumulation removal. 但会增强图片中sharp, visible rain streak，导致与数据集中的rain streak不同。因此，论文的处理过程为 streak removal rain-accumulation removal streak removal Accumulation removal网络 Create another network based on the structure of contextualized dilated network, with only one recurrence Trained with the synthesized data generated with the random background reliance and transmission value 4. Experiments 4.1. Baseline JORDER-. 只有一条分支，不使用dilated convolutions JORDER. JORDER-R. JORDER-R-DEVEIL. Image Decomposition (ID) CNN DSC Layer Prior (LP) SRCNN 4.2. 实验结果]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Image Processing</category>
        <category>De-raining</category>
      </categories>
      <tags>
        <tag>Dehazing</tag>
        <tag>Image Processing</tag>
        <tag>Image Enhancement</tag>
        <tag>De-raining</tag>
        <tag>Rain Model</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(TPAMI 2015) Image super-resolution using deep convolutional networks]]></title>
    <url>%2F2018%2F01%2F21%2FImage%20super-resolution%20using%20deep%20convolutional%20networks%2F</url>
    <content type="text"><![CDATA[Dong C, Loy C C, He K, et al. Image super-resolution using deep convolutional networks[J]. IEEE transactions on pattern analysis and machine intelligence, 2016, 38(2): 295-307. 1. Overview Kappeler A, Yoo S, Dai Q, et al. Video super-resolution with convolutional neural networks[J]. IEEE Transactions on Computational Imaging, 2016, 2(2): 109-122.]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Image Processing</category>
        <category>Super Resolution</category>
      </categories>
      <tags>
        <tag>Image Processing</tag>
        <tag>Image Enhancement</tag>
        <tag>Super Resolution</tag>
        <tag>SRCNN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(CVPR 2017) Real-Time Video Super-Resolution with Spatio-Temporal Networks and Motion Compensation]]></title>
    <url>%2F2018%2F01%2F21%2FReal-Time%20Video%20Super-Resolution%20with%20Spatio-Temporal%20Networks%20and%20Motion%20Compensation%2F</url>
    <content type="text"><![CDATA[Caballero J, Ledig C, Aitken A, et al. Real-time video super-resolution with spatio-temporal networks and motion compensation[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2017: 4778-4787. 1. Overview 基于ESPCN，论文 扩展到spatial-temporal 使用STN进行motion compensation 1.1. 模型 1.2. Loss Function 1.3. Fusion方案 1.4. 数据集 CDVL. 115 uncompressed full HD videos 1.5. 评价指标 MOVIE. Motion-based Video Integrity EvaluationThe MOVIE index was designed as a metric measuring video quality that correlates with human perception and incorporates a notion of temporal consistency. 1.6. 实验结果 L. Layer number E5. Early fusion with 5 frame MC. Motion compensation]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Image Processing</category>
        <category>Super Resolution</category>
      </categories>
      <tags>
        <tag>Image Processing</tag>
        <tag>Image Enhancement</tag>
        <tag>Super Resolution</tag>
        <tag>Video</tag>
        <tag>Optical Flow</tag>
        <tag>FlowNet</tag>
        <tag>Pixel Shuffle</tag>
        <tag>VESPCN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(CVPR 2018) Frame-Recurrent Video Super-Resolution]]></title>
    <url>%2F2018%2F01%2F21%2FFrame-Recurrent%20Video%20Super-Resolution%2F</url>
    <content type="text"><![CDATA[Sajjadi M S M, Vemulapalli R, Brown M. Frame-recurrent video super-resolution[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018: 6626-6634. 1. Overview 目前的視頻SR任务使用CNN+motion compensation方法，通过多个LR帧生成一个LR帧。当前State-of-art方法使用sliding window实现，但存在缺陷： 计算冗余. 多帧被重复计算 独立估计每帧. 限制了temporally consistent results 因此，论文提出frame-recurrent video super-resolution (FRVSR)框架，previous HR estimate参与到当前帧的预测 (optical flow warping) Temporally consistent result 降低计算量. 相比于sliding window, 每帧只计算一次 Assimilate a large number of previous frame No pre-train, end-to-end 处理任意size, length视频 1.1. 模型 1.2. 数据集1.2.1. 训练集 vimeo.com下载40个HR视频，downsmaple 2倍 Extract 256x256 patch Gaussian blur. 方差1.5 提取相似场景连续帧 1.2.2. 测试集 youtube.com下载3-5s HR视频（YT10） 1.3. Future Work Occluded region Application. video compression Loss term. GAN, EnhenceNet. 1.4. 相关工作 InterpolationBilinear, Bicubic, Lanczos Example-Based Dictionary Learning Self-similarity (Deep Learning) GAN Multi-frame (Expensive) Optical Flow Conv-LSTM Bidirectional Recurrent Architecture 2. Experiments 2.1. Baseline SISR. LR输入SRNet VSR. sliding window, warp t+1, t-1 to t帧, concate输入SRNet 2.2. 实验结果 2.3. Blur Size 2.4. Training Clip Length 2.5. Degraded Input 2.6. Temporal Consistent 2.7. Range of Information Flow 2.8. Network Size]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Image Processing</category>
        <category>Super Resolution</category>
      </categories>
      <tags>
        <tag>Image Processing</tag>
        <tag>Image Enhancement</tag>
        <tag>Super Resolution</tag>
        <tag>Video</tag>
        <tag>Optical Flow</tag>
        <tag>FlowNet</tag>
        <tag>ESPCN</tag>
        <tag>Pixel Shuffle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(CVPR 2016) Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network]]></title>
    <url>%2F2018%2F01%2F21%2FReal-time%20single%20image%20and%20video%20super-resolution%20using%20an%20efficient%20sub-pixel%20convolutional%20neural%20network%2F</url>
    <content type="text"><![CDATA[Shi W, Caballero J, Huszár F, et al. Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2016: 1874-1883. 1. Overview 在此前的工作中，首先将low resolution (LR)图像upscale（bicubic插值）到high resolution (HR)空间。然后，输入到神经网络中。然而，这种方法的计算复杂度很大。 论文提出ESPCN结构(efficient sub-pixel convolutional neural network) 降低计算复杂度与速度.输入为LR空间图像（3 x h x w），网络结构中最后一层Conv层为efficient sub-pixel convolution layer （输出维度为3rr x h x w）.最后进行rearrange（3 x rh x rw）成HR空间 Pipeline不需使用bicubic 提高准确度 1.1. 效果 Real-time SR of 1080p videos on single K2 GPU r*r times faster Perform better (+0.15dB on Images, +0.39dB on Videos) 1.2. SR Problem ill-posed problem multiple solutions (one-to-many mapping). key assumption. Much of the high-frequency data（边缘） is redundant and thus can be accurately reconstructed from low frequency components 1.3. Related Work Edge-based Image statistics-based Patch-based Sparsity-based (sparse coding). dictionary (prior) discover correspondence between LR and HR. Computation expensive Random forest Auto-encoder SRCNN 1.4. Dataset1.4.1. Image Timofte (widely used by SISR paper)91张训练图片，2个测试集（Set5, Set14分别包含5张、14张图片）. Berkeley segmentation dataset (BSD300, BSD500) Super texture dataset136张texture图片. ImageNet机选取5000张. 1.4.2. Video Xiph8 1920x1080 videos, length ≈10s. Ultra Video Group7 1920x1080 videos, length 5s. 1.5. Future Work Neighbouring video frames Spatial-temporal network 2. Experiments 2.1. 网络结构实验中的ESPCN结构： Input (b, 3, h, w) Conv_1 (5x5, 64, 1s) –&gt; (b, 64, h, w) Conv_2 (3x3, 32, 1s) –&gt; (b, 32, h, w) Conv_3 (3x3, rr, 1s) –&gt; (b, 3r*r, h, w) PixelShuffle (r) –&gt; (b, 3, rh, rw)模型使用tanh，实验中与relu进行比较。 2.2. Loss FunctionMSE. 2.3. 评价指标PSNR of luminance in YCbCr. 2.4. 实验结果]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Image Processing</category>
        <category>Super Resolution</category>
      </categories>
      <tags>
        <tag>Image Processing</tag>
        <tag>Image Enhancement</tag>
        <tag>Super Resolution</tag>
        <tag>ESPCN</tag>
        <tag>Pixel Shuffle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(CVPR 2017) Removing rain from single images via a deep detail network]]></title>
    <url>%2F2018%2F01%2F21%2FRemoving%20rain%20from%20single%20images%20via%20a%20deep%20detail%20network%2F</url>
    <content type="text"><![CDATA[Fu X, Huang J, Zeng D, et al. Removing rain from single images via a deep detail network[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2017: 3855-3863. 1. Overview 当object的structure, orientation与rain streak相似时，基于low-level图像特征的de-rain方法效果很差 ResNet通过改变映射形式简化训练过程 因此，论文提出用于de-rain的Deep Detail Network框架 利用lossless negative residual mapping, 缩小映射范围，简化训练过程. (输入)rain image - (预测)rain residual = (输出)clean image 引入prior信息（rain high-frequency）. 去除图片背景，得到包含object and rain streaks structures的图片，将其作为神经网络的输入，神经网络对rain structure建模. 此外，去除背景后的图片像素值范围缩小，且大部分像素点为0，其稀疏性能够提高效果。 Y: gt; X: rain image; X_detail: obj and rain streak structure; |Y - X|: rain structure. 创建数据集. 1.1. 框架特点 可泛化到真实图像 通用性. 能用于去噪，降低JPEG artifacts 基于先验信息缩小映射范围（而ResNet改变映射形式） 1.2. 数据集 从UCID、BSD和Google image search中收集1000张clean image 每张图片生成14种不同streak orientation and magnitude的rain image 1.3. 相关工作分为两类 基于视频 基于单张图片 基于视频的de-rain任务可以使用inter-frame information，相对于基于单张图片而言，较简单。 基于单张图片的工作有： Non-local mean fltering GMM + patch-rank prior (low-rank) （patch-based） GMM + patch-based prior DSC (discriminative sparse coding). Dictionary Learning 1.4. Findingsheavy rain image存在类似于haze的现象，因此，对图片进行de-haze预处理能够提高效果。 2. Deep Detail Network 2.1. Direct Network 神经网络输入(X): rain image 神经网络输出&amp;gt: clean image 出现color shift现象. frobenius范数平方-求和-开方 2.2. Negative Residual Mapping 神经网络输入(X): rain image 神经网络输出: rain structure 模型输出: clean image = rain image - rain structure gt: clean image 通过缩小输出映射范围，减小solution space. 无法完全de-rain. 2.3. Deep Detail Network 神经网络输入(X_detail): obj + rain structure image 神经网络输出: rain structure 模型输出: clean image = rain image - rain structure gt: clean image 输入具有稀疏性 + 输出缩小映射范围. 2.3.1. Decompose使用guided filtering（实验中，半径设为15）作为low-pass filter，将图片分为 Base layer Detail layer Detail layer包含图片中object and rain streak structure信息. 2.4. 网络结构 3. Experiments 实验与两个算法进行对比 [25] DSC [24] GMM + Patch-rank prior评价指标SSIM 3.1. 合成数据 3.2. 真实数据 3.3. 测试时间[24] [25]算法为CPU运行时间。 3.4. 收敛比较 3.5. 网络depth vs width 3.6. 与深度学习算法(ICCV 2013)比较 3.7. 扩展任务（通用性）]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Image Processing</category>
        <category>De-raining</category>
      </categories>
      <tags>
        <tag>Image Processing</tag>
        <tag>Image Enhancement</tag>
        <tag>De-raining</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(2017) Image De-raining Using a Conditional Generative Adversarial Network]]></title>
    <url>%2F2018%2F01%2F21%2FImage%20De-raining%20Using%20a%20Conditional%20Generative%20Adversarial%20Network%2F</url>
    <content type="text"><![CDATA[Zhang H, Sindagi V, Patel V M. Image de-raining using a conditional generative adversarial network[J]. arXiv preprint arXiv:1701.05957, 2017. 1. Overview 图像去雨/去雪任务具有ill-posed性质，此前大多数工作都引入了prior信息，将问题转化为well-posed。本篇论文（cGAN+refined loss函数, ID-CGAN），在不引入prior信息的情况下，达到state-of-art。 1.1. 模型 1.2. Loss Function1.2.1. G loss 包含三部分 去雨图像与gt之间的L2 loss 对抗loss perceptual loss. 去雨图像与gt的特征L2 loss（通过VGG-16提取）实验中λ_a设为0.0066, λ_p设为1。 1.2.2. D loss 1.3. 相关工作 SPM（sparse coding-based clustering method）. 使用双边滤波将图像分为高频和低频两个部分， 接着将高频图像分为有雨和无雨两部分。基于假设：雨的条纹具有相同的边缘方向。 DSC（discriminative sparse coding based method）. Dictionary Learning PRM. GMM + patch-based prior GMM. GMM + patch-rank prior CCR（convolutional coding-based method）. 学习convolutional low-rank filters CNN 相比于相关工作，ID-CGAN 关注优化函数 没有使用额外的处理方法 1.4. Drawbacks对于白色圆形雨点，不但无法去除，而且还会增强。可能的原因： 训练集缺少多样性 VGG-16捕获到白色原形雨点的特征，通过loss function使其增强 1.5. 发现训练好的de-rain模型还具有de-snow和de-haze效果。 2. 数据集 2.1. 合成数据 训练集. 700张图片。其中，500张从UCID数据集前800张图片中随机抽取；剩余200张从BSD-500训练集随机抽取 测试集. 100张图片。其中，50张从UCID后500张中随机抽取；50张从BSD-500测试集随机抽取 使用photoshop加上雨状条纹（不同方向和像素强度）。图像resize到256x256. 2.2. 真实数据从网上下载50张包含雨点的图片，且具有不同方向和像素强度。 3. 实验 3.1. 评价指标论文使用图像的luminance channel进行计算. 3.1.1. PSNR（Peak Signal Noise Ratio） 3.1.2. SSIM（Structure Similarity Index） 3.1.3. UQI（Universal Quality Index） 3.1.4. VIF（Visual Information ） 3.2. Baseline GEN. 图像L2 loss CGAN. 图像L2 loss + 对抗loss CGAN-P. 对抗loss + perceptual loss 3.3. Baseline实验结果 对抗loss能够提高视觉效果，但相关于常规CNN而言引进了artifacts Perceptual loss能够很好地的解决artifacts问题，以及增强实验效果。但不能完全消除artifacts，造成质量下降 3.4. 算法比较]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Image Processing</category>
        <category>De-raining</category>
      </categories>
      <tags>
        <tag>Image Processing</tag>
        <tag>Image Enhancement</tag>
        <tag>GAN</tag>
        <tag>De-raining</tag>
        <tag>Conditional GAN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(CVPR 2016) Deep region and multi-label learning for facial action unit detection]]></title>
    <url>%2F2018%2F01%2F17%2FDeep%20region%20and%20multi-label%20learning%20for%20facial%20action%20unit%20detection%2F</url>
    <content type="text"><![CDATA[Zhao K, Chu W S, Zhang H. Deep region and multi-label learning for facial action unit detection[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2016: 3391-3399. 1. Overview 在人脸AU检测中，存在两种常见的任务 Region Learning (RL) Multi-label Learning (ML)论文将两者进行结合，提出能够同时解决上述两个任务的框架Deep Region and Multi-label Learning (DRML)。 框架中包含一个region laryer结构。 region layer可通过两种方式设计 locally connected layers (LCN). 每个像素点对应一个卷积核（参数量较大） conventional convolution layers. 首先将输入特征图分成n*m个region, 每个region内的像素点共享卷积核权重（即每个region通过一个卷积层） DRML框架特点 end to end训练 non-linear模型论文在BP4D和DISFA数据集上进行实验，对比F1-score和AUC评价指标。 1.1. Region Learning 通过传统方法识别特定的区域来提高检测性能（类似于Attention）。例如patch-based方法，首先将图像划分为patch，然后将patch分类为普通patch和特定patch来描述不同的表情 patch-based 缺点. easily fail on faces with modest or large pose （某些patch中部分相关，但被排除在外，没有用于识别，从而导致性能降低） 1.2. Multi-label Learning 传统的AU检测方法（AdaBoost、SVM等）都是对某个AU进行检测 ML对于每个表情，同时预测多个AU。 能够在一定程度上解决正负样本不平衡的问题 1.3. 评价指标 F1-score. precision和recall的调和均值，常用于AU检测 AUC. 量化true positive和false positive之间的关系 2. DRML结构 大多数表情分析都使用很小的人脸图像（如4848）作为输入，为了避免人脸微小细节的丢失，论文使用**170170**大小的图像作为输入。 2.1. Loss Function 2.2. Region Layer 人脸图像相对于自然图像更加结构化，不同的人脸区域具有不同的局部统计。因此，可使用LCN（参数太多）或者区域卷积进行处理 可看做是对每个区域进行Attention操作 结构包含三部分 patch clipping （论文采用8*8 grid） local convolution identity addition (避免梯度消失；如果patch与AU检测无关，it would be easier to directly forward the patch than learning a filter bank to reduce the patch’s effect) 2.3. Region可视化使用saliency map（通过计算每个像素点对于某个特定AU的梯度级数）进行可视化。 2.4. 与相关工作的比较 DRML受JPML启发，但存在不同之处 JPML通过对数据集统计，定义AU关系 JPML使用manually-crafted feature (SIFT) JPML轮流学习PL和ML JPML线性 3. Experiments 200200图像random crop为170170，并进行horizontally mirrored。 DRML收敛更快，训练loss更低，更接近gt统计 DRML比LCN速度快；ConvNet（去掉Region Layer的DRML）比AlexNet速度慢（由于卷积核为11*11） 3.1. BP4D&amp;DISFA实验结果 learned features are of lower dimension, more than 40% of learned features for AlexNet, LCN, and DRML, are zeros Multi-label训练提高了效果 当训练数据很少时，multi-label学习能够减少imbalance的影响]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Face</category>
      </categories>
      <tags>
        <tag>Multi-label Classification</tag>
        <tag>Face</tag>
        <tag>Facial Action Units</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(CVPR 2017) Fatauva-net:An integrated deep learning framework for facial attribute recognition, action unit (au) detection, and valence-arousal estimation]]></title>
    <url>%2F2018%2F01%2F17%2FFatauva-net%3A%20An%20integrated%20deep%20learning%20framework%20for%20facial%20attribute%20recognition%2C%20action%20unit%20(au)%20detection%2C%20and%20valence-arousal%20estimation%2F</url>
    <content type="text"><![CDATA[Chang W Y, Hsu S H, Chien J H. Fatauva-net: An integrated deep learning framework for facial attribute recognition, action unit (au) detection, and valence-arousal estimation[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshop. 2017. 1. Overview 目前人脸表情识别的两种主流方式为 Action Units (AUs) Valence-Arousal space (V-A space) 结合上述两种方式，论文提出一种能够同时用于 人脸属性识别 AU检测 V-A估计3种任务的集成深度学习框架FATAUVA (Facial Attribute Recognition, Action Unit Detection, Valence-Arousal Estimation)。 在FATAUVA框架中 将Attribute层的输出作为中间特征，用于后续AU检测 将AU层的输出作为中间特征，用于后续V-A估计 1.1. 训练过程 利用CelebA数据集训练Core Layer和Attribute Layer 固定Core Layer和Attribute Layer权重, 利用FERA2015数据集训练AU Layer 固定Core Layer, Attribute Layer和AU Layer权重，利用AFF-Wild Challenge训练V-A Layer 1.2. V-A space分为两个维度 1.3. 相关数据集 cross-age celebrity dataset (CADA) [Attribute] CelebA [AU] FERA2015, [AU] BP4D (Video) [AU] SEMAINE (实验环境Image) [V-A] AFF-Wild Challenge 训练集共253个视频，每帧都有标注；测试集47个视频 2. 网络结构 2.1. Attribute Layer分为四个子层：Face、Eye、Eyebrow、Mouth 论文从CelebA数据集中选出10种人脸属性，并将这10种属性归属到最相关子层代表的区域中（通过在子层后连接相应的2-way FC层进行预测，每种属性对应一个FC层）。 2.2. AU Layer将AUs归属到最相关的Attribute子层代表的区域中（通过在子层后连接相应的AU Conv层，并连接2-way FC层进行预测）。 2.3. V-A Layer将AU分为两组（Valence和Arousal），每组AU concat在一起，输入后续Conv层以及FC层。 2.4. Convolutional Block使用PolyNet中的块结构 Core Layer 8 rPoly-2 blocks Attribute Layer 2 rPoly-2 blocks AU Layer 2 rPoly-3 blocks V-A Layer 2 rPoly-3 blocks 3. Experiments 3.1. 数据预处理 Attribute和AU数据集 使用MTCNN截取人脸区域 V-A数据集 使用数据集给定的bounding box截取人脸区域 对每个AU的预测是一个二分类问题。由于正负样本比例不平衡，实验对较少的AU进行over sampling，对负样本进行down sampling. 将V-A得分量化到[-5,5]范围，进行可视化 由于样本分布不平衡，实验同样进行over sampling和down sampling. 3.2. Loss Layer在Attribute Layer和AU Layer后连接3层FC，最后对2维输出做softmax操作。 在V-A Layer后连接3层FC，并使用了两种loss class-based 将[-5, 5]范围的得分离散化为11种类别。选择top 3得分：（1）如果得分连续（1,2,3或1,3,2），进行加权求和得到最终得分。（2）如果得分不连续，取top 1得分作为最终得分。 regression-based 结合center loss和smooth L1 loss x 倒数第二层FC输出的特征 c 类别y的中心（倒数第二层FC输出对应类别y的特征的均值） y 预测值 y^{~} ground truth t L1与L2之间的转折点 3.3. Attribute Recognition实验结果 3.4. AU Detection实验结果 3.5. V-A Estimation实验结果 使用AU能够提高V-Aestimation结果 使用regression-based loss优于class-based loss CCC: Concordance Correlation Coefficient]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Face</category>
      </categories>
      <tags>
        <tag>Multi-label Classification</tag>
        <tag>Face</tag>
        <tag>Facial Action Units</tag>
        <tag>Valence-Arousal space</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(ICCV 2017) DeepCoder:Semi-Parametric Variational Autoencoders for Automatic Facial Action Coding]]></title>
    <url>%2F2018%2F01%2F17%2FDeepCoder%3A%20Semi-Parametric%20Variational%20Autoencoders%20for%20Automatic%20Facial%20Action%20Coding%2F</url>
    <content type="text"><![CDATA[Linh Tran D, Walecki R, Eleftheriadis S, et al. DeepCoder: Semi-Parametric Variational Autoencoders for Automatic Facial Action Coding[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2017: 3190-3199. 1. Overview 人脸表情可以编码成一系列的面部活动单元(facial action units, AUs)及其对应的活动强度(intensity). 而变分自编码器(VAE)能够通过无监督学习（重构loss+KL loss）提取数据的隐含表达（latent representation）。因此，对于人脸AU强度估计的任务可分为两个步骤 利用VAE提取人脸特征 使用分类器对特征进行AU活动强度估计 另一方面，non-parametric方法（如Gaussian Process）的效果优于parametric，但该方法只适用于小样本数据，无法很好地处理大样本数据。因此，论文将两者进行结合，提出semi-parametric的DeepCoder框架 parametric VC-AE (Variational Convolutional AEs) non-parametric VO-GPAE (Variational Ordinal GP AEs)并在DISFA和FERA2015数据集上进行实验验证。 1.1. FACSFacial Action Coding System 定义30多个面部肌肉活动单元，及其活动强度评分标准。 2. 框架结构 2.1. VC-AE包含两部分loss KL loss (Z0) reconstruction loss (x-&gt;Z0-&gt;x’) 实验中使用warming strategy, 额外加入了AU强度估计loss 2.2. VO-GPAE包含三部分loss KL loss (Z0) reconstruction loss (Z0-&gt;Z1-&gt;Z0) 强度估计loss (Z1-&gt;Y) 2.3. Joint LearningLoss function VO-GPAE中的covariance function计算量会随着数据量的增多而增加，因此论文提出leave-subset-out策略，将训练集X分为不相交的两个子集X_R和X_L. X_R用于训练VC-AE, X_L用于训练VO-GPAE, 且X_R&gt;&gt;X_L. 3. Experiments NLPD negative log-predictive density for reconstruction error ICC intra-class correlation, agreement between annotators 在Z1空间中模型将每个点都fit到一个独立的cluster中，从而使得对Z1空间上的特征进行AU强度估计效果更好。]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Face</category>
      </categories>
      <tags>
        <tag>Multi-label Classification</tag>
        <tag>Face</tag>
        <tag>VAE</tag>
        <tag>Facial Action Units</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(ECCV 2016) Stacked hourglass networks for human pose estimation]]></title>
    <url>%2F2018%2F01%2F03%2FStacked%20hourglass%20networks%20for%20human%20pose%20estimation%2F</url>
    <content type="text"><![CDATA[Newell A, Yang K, Deng J. Stacked hourglass networks for human pose estimation[C]//European Conference on Computer Vision. Springer International Publishing, 2016: 483-499. 1. Overview 论文提出一种用于single person pose estimation的 repeated bottom-up, top-down (Hourglass) intermediate supervision模型结构（Stacked Hourglass Networks），能够captures and consolidates information across all scales of the image. 论文在 FLIC MPII数据集上是进行实验 2. 模型结构 2.1. Hourglass图中每个box都是一个residual结构. 在top-down过程中只使用upsampling，不适用deconv. Bottom-up. Conv + ReLU + BN + Max pooling Top-Down. Upsamping + Add 2.2. Residual Block &amp; Full Network 网络输入256x256，输出64x64 整个网络最开始使用一个Conv(7x7, 2s) 所有residual block输出通道数为256 网络中的Hourglass结构不共享参数 intermediate supervision使用相同gt 3. Experiments 3.1. 数据处理 对于多人情况. MPII训练集和测试集都提供了target person的中心点、scale (相对于200pixel的倍数)，可根据这些信息crop person, resize到256x256，再训练. 另外，可移动target person中心点到图像中心 Data augmentation. 旋转(±30°), 缩放(.75-1.25), 不使用平移 3.2. 训练&amp;测试 使用MSE计算loss 测试时，使用origin image和flip image的平均结果作为最终预测 评价标准. FLIC：normalized by torso size, MPII： normalized by head size 3.3. 实验结果 3.4. Ablation 3.5. Multiple People 3.6. Occlusion]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Body</category>
        <category>Pose</category>
      </categories>
      <tags>
        <tag>Pose</tag>
        <tag>Human Body</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(CVPR 2016) Convolutional pose machines]]></title>
    <url>%2F2018%2F01%2F03%2FConvolutional%20pose%20machines%2F</url>
    <content type="text"><![CDATA[Wei S E, Ramakrishna V, Kanade T, et al. Convolutional pose machines[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2016: 4724-4732. 1. Overview 论文将CNN与Pose Machine相结合，提出Convolutional Pose Machine (CPM) Multi-stage结构. 将stage_n的预测输入到stage_{n+1}中进行refine Intermediate supervised &amp; end-to-end. 每个阶段的预测都计算loss。Forward完成后，各阶段loss一起回传. 解决gradient vanish问题 级联CNN. 实现long-range spatial modeling 1.1. 相关工作 使用CNN直接预测人体关键点. 没有保留空间不确定性，导致accuracy很低 论文中CPM使用heap map. 基于数据集中人体关键点gt构建grid，并在关键点的位置加上Gaussian分布，形成heap map。最后通过MSE计算loss 1.2. 数据集 MPII. 16 key points, 全身（单人/多人） Extended Leeds Sports Dataset (LSP). 14 key points, 全身（单人） FLIC. 9 key points, 全身（单人） 1.3. 评价指标 PCKh. 记头部框对角线长度距离为d，预测点与gt之间的距离小于d表示预测正确 PCKh-0.5. 0.5d距离内表示预测正确 2. CPM stage_1的输入图像可以与后续stage的输入图像不同 每阶段预测结果(batch_size, body_part + 1, h’, w’) 2.1. Stage 1 predicts part beliefs from only local image evidence. 虽然stage_1的预测结果较差，但能为后续stage的预测提供信息 2.2. LossMSE 2.3. Pytorch版本代码 Center Map (中心点的高斯分布). 训练阶段，数据集label直接给出中心点(或根据关键点坐标计算得出). 中心点计算方法（关键点最大最小坐标和，求平均） 根据中心点生成center map方法 3. Experiments 3.1. 模型比较 3.2. 实验结果]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Body</category>
        <category>Pose</category>
      </categories>
      <tags>
        <tag>Pose</tag>
        <tag>Human Body</tag>
        <tag>CPN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(2017) Going deeper into action recognition:A survey]]></title>
    <url>%2F2018%2F01%2F03%2FGoing%20deeper%20into%20action%20recognition%3A%20A%20survey%2F</url>
    <content type="text"><![CDATA[Herath S, Harandi M, Porikli F. Going deeper into action recognition: A survey[J]. Image and Vision Computing, 2017, 60: 4-21. 1. Overview 论文介绍了Action Recognition方面的四种深度学习结构 Spatiotemporal Network Multiple Steam Network Deep Generative Network Temporal Coherency Network以及Action Recognition数据集（KTH、Weizmann、Hollywood2、HMDB-51、UCF-101、Sports 1-M）。 2. Spatiotemporal Network 2.1. 3D CNN 在实际中，加入一些补充信息（optical flow）训练网络能得到更好的性能。将时间信息输入网络的过程称为fusion，有以下3种机制 slow fusion. 同时输入视频中的几个片段。在foveated结构中，输入context stream的同时，还输入了fovea stream（图像中心；假设在拍摄时，会将重要的内容移动到视野中心） early fusion. 输入相邻帧集合 late fusion. 逐帧输入进行处理，最后将所有帧的特征融合在3D CNN结构中，使用更长的帧能够提高效果；将3D卷积核分解为2D卷积核和1D卷积核能够减少参数。 2.2. RNN 先用3D CNN提取特征，再输入到LSTM中 LRCN（Long-term Recurrent Convolutional Network） 3. Multiple Stream Network 两个并行输入 基准帧 连续光流特点 使用ImageNet预训练权重 光流 early fusion multi-task训练 （由于数据集较小，因此使用多数据集进行训练，每个数据集对应一个分类层）在中间层进行fusion，既能提高效果，也能减少参数。 4. Deep Generative Models 时间序列维度的生成预测是一个无监督问题。 4.1. Dynencoder分为三层 输入帧x_t得到h_t 根据h_t预测h_{t+1} 根据h_{t+1}生成帧x_{t+1}先分别预训练每层，再end-to-end fine tuning. 4.2. LSTM Autoencoder分为两层 encoder LSTM decoder LSTM （合成和预测） 4.3. Adversarial方法训练 5. Temporal Coherency Network 一种弱监督形式。模型判断一段视频帧是否时序正确。 5.1. Siamese Network判断给定序列是否Coherency。相比ImageNet预训练权重，give more attention to human poses，并且能够提高准确度。 缺点. 视频段之间可能会出现场景变化（如Sports 1M数据集中的广告插播）。## 5.2. 基于Siamese网络的并行结构将视频帧分为两个集合- prediction set X_p- effect set X_e将X_p特征进行变换，与X_e特征进行对比，从而识别行为。 6. 数据数据 6.1. dataset controlled condition (limited camera motion, almost zero background clutter) limited to basic action (walking, running and jumping) 6.2. HMDB-51&amp;UCF-101 非专业拍摄的Youtube视频(contain camera motion (and shakes), view-point variations and resolution inconsistencies) Actions are well cropped in the temporal domain, not well-suited for measuring the performance of action localization 包含subtle classes (chewing and talking or playing violin and playing cello)，要求网络深层次理解时空线索 6.3. Hollywood2&amp;Sports-1M 视角变换（view-point/editing complexities） 行为只发生在视频中某个很小的clipsSports-1M中还包含观众和广告条 7. 未来发展 knowledge transfer &amp; domain adaptation 算法混合（3D CNN、temporal pooling、 optical flow frames、LSTM） 提升性能（data augmentation、foveated architecture、distinct frame sampling strategis） 8. 实际应用 通常会涉及到joint dection（人体关键点检测） fine-grained行为识别，而非识别所有类别的行为]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Body</category>
        <category>Action Recognition</category>
      </categories>
      <tags>
        <tag>Overview</tag>
        <tag>Action Recognition</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(CVPR 2016) Compact Bilinear Pooling]]></title>
    <url>%2F2017%2F12%2F30%2FCompact%20Bilinear%20Pooling%2F</url>
    <content type="text"><![CDATA[Gao Y, Beijbom O, Zhang N, et al. Compact bilinear pooling[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2016: 317-326. 1. Overview 此前提出的full bilinear模型存在一些问题(channel=512, k=1000时) 输出512512(260000)维度，导致后续的分类器参数很大(1000512*512≈250 million) storage expensive (2TB) 无法扩展到spatial pyramid matching、domain adaptation等 在few-shot learning上存在困难（如参数太多很容易过拟合） 基于上述问题，论文提出两种compact bilinear pooling方法 Random Maclaurin Projection (RM) Tensor Sketch Projection (TS) 具有以下特点 减少两个数量级维度的同时，几乎不降低性能 back-propagation efficient且end-to-end train 实际上，full bilinear方法在只使用2%特征维度情况下，与论文提出方法的性能相似。因此，98%的特征都是冗余的。 1.1. 与其他方法比较 1.2. 数据集 CUB（鸟类） MIT（室内场景） DTD（结构） 2. Pooling方法 2.1. circular convolution（循环卷积）a=[1,2,3,4], b=[5,6,7,8], a*b计算过程 a逆时针固定在圆上，b顺时针固定在圆上。对应相乘求和结果y(0)=66. a顺时针旋转一位，计算y(1)=68 最终y的维度与a,b相同 2.2. Bilinear Sum Pooling(h, w, c)-&gt;(h,w, cc)-&gt;(cc) 3. Compact Bilinear Models 3.1. Full Bilinear Pooling S. set of location, 数量为height*width X. local descriptors, 输入特征图每个点的值(长度为channel数c)维度变化(h, w, c)-&gt;(h, w, cc)-&gt;(cc) 3.2. Compact Bilinear Pooling找到Φ(x)∈R^d使得 $Φ(x) ≈ xx^T$ Random Maclaurin Tensor Sketch pytorch版本代码过程 stream_1, stream_2;(b, 512, h, w), d=8000 初始化h_1, s_1, h_2, s_2;(512)，构造矩阵sparse_sketch_matrix1,sparse_sketch_matrix2;(512, 8000) reshape stream_1,stream_2;(bhw, 512) sketch=stream*sparse_sketch_matrix; (bhw, 8000) FFT(sketch_1)·FFT(sketch_2); (bhw, 8000) IFFT; (bhw, 8000) reshape (b, h, w, 8000) sum pooling; (b, 8000) 3.3. 计算时间 在Caffe K40c GPU下 VGG16 forward backward时间为312ms Bilinear pooling 0.77ms TS(d=4096) 5.03ms，FFT计算量比矩阵运算大 4. Experiments 4.1. Pooling Methods Full Bilinear Pooling Compact Bilinear Pooling. 相对于512*512（≈260000）特征维度，8000维度足够表示特征。fine tuning能够提高效果，但提升很小。 Fully Connected Pooling Improved Fisher Encoding. 对输出进行64 GMM编码 4.2. 维数&amp;方式比较 在维度很小时，fine tune能够显著提升效果 维度在2000～8000合适 4.3. 与PCA比较PCA Bilinear通过在bilinear layer前插入1*1Conv实现，初始化为PCA权重。 4.4. 实验结果 4.5. Few Shot Learning 训练样本数量与分类器假设空间（hypothesis space）大小相关 实验证明低维度表示更适合few-shot learning]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Architecture</category>
        <category>Component</category>
      </categories>
      <tags>
        <tag>Fine-Grained Classification</tag>
        <tag>Component</tag>
        <tag>Compact Bilinear Pooling</tag>
        <tag>Few Shot Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(ICCV 2015) Bilinear CNN Models for Fine-grained]]></title>
    <url>%2F2017%2F12%2F30%2FBilinear%20CNN%20Models%20for%20Fine-grained%2F</url>
    <content type="text"><![CDATA[Lin T Y, RoyChowdhury A, Maji S. Bilinear cnn models for fine-grained visual recognition[C]//Proceedings of the IEEE International Conference on Computer Vision. 2015: 1449-1457. 1. Overview 人类大脑双流假说 (two-streams hypothesis). 假说认为大脑中有两种视觉系统 腹流(ventral stream; what pathway). 参与物体识别 背流(dorsal stream; where pathway). 处理物体相对于viewer的空间位置 基于上述假说，论文提出bilinear模型，该模型可end-to-end训练，有助于fine-grained分类问题。 模型分为两条stream，分别负责 localization (where) [part detector] appearance modeling (what) [feature extractor]但最终实验表明两条stream并没有明显的界限，它们都趋向于激活特定的semantic part. 2. 计算过程 得到两条stream输出的特征图后(h, w, c1), (h, w, c2) 首先，对应空间点进行外积操作，从而实现part-feature interaction. (h, w, c1*c2) 其次，进行sum-pooling操作. (c1*c2) 接着，进行signed square-root和L2归一化 最后，分类 3. 数据集 (bird) CUB-200-2011. 11788张图片，200种鸟类 (aircraft) FGVC-aircraft. 10000张图片，100中飞机类型 (car) Cars. 16185张图片，196种车类型 4. 实验结果]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Architecture</category>
        <category>Component</category>
      </categories>
      <tags>
        <tag>Fine-Grained Classification</tag>
        <tag>Bilinear CNN</tag>
        <tag>Component</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(CVPR 2018) Learning to segment every thing]]></title>
    <url>%2F2017%2F12%2F22%2FLearning%20to%20Segment%20Every%20Thing%2F</url>
    <content type="text"><![CDATA[Hu R, Dollár P, He K, et al. Learning to segment every thing[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018: 4233-4241. 1. Overview 使用监督学习方式训练object instance segmentation任务，要求数据集有segmentation mask的标注。由于mask的标注成本非常高，现有数据集只有较少mask标注，而物体检测box的标注成本相对较低，现有数据集包含大量box标注。 因此论文提出一种叫做partially supervised (transfer learning)的训练方法，在包含所有box标注和少量mask标注的数据集上训练能够分割所有box标注对象的instance segmentation模型。对maks预测和box预测解耦的Mask R-CNN结构非常适合用于这种训练方法，论文称网络结果为Maskx R-CNN。 论文在COCO和Visual Genome数据集上进行实验，训练出能够检测3000种对象类别的instance segmentation模型。 1.1. 思想 在视觉语义空间中，邻近的embedding向量在appearance或semantic上相近。可将box head（最后一层）权重参数和mask head（最后一层）权重参数看作embedding向量。同类别的两个向量在appearance上相似，因此可利用transfer从box head embedding向量学习出mask head embedding向量。 利用weight transfer function基于box权重预测mask权重，weight transfer function通过少量mask标注数据进行学习。由于box权重是针对所有box标注对象而言的，因此transfer后的mask权重也能够针对所有box标注对象， 从而能够预测数据集中mask标注对象以外的对象。换而言之，将category specific信息从box detectors迁移到instance mask predictors. 1.2. 数据集 COCO. 为了模拟partially supervised instance segmentation, 将数据集分为两部分：使用box标注和mask标注、只使用box标注。 Visual Genome. 规模较大，只有box标注信息。 2. 细节 2.1. 训练数据划分 C=A∪B C. 数据集中所有对象类别 A. 含有mask标注的对象类别 B. 只有box标注的对象类别（已知mask标注，可以很容易得到box标注） 2.2. Weight Transfer Function w_{det}. box head最后一层中的权重，可看作appearance-based visual embedding w_{seg}. mask head最后一层中的权重 Θ. 学习的参数，class-agnostic w_{det}的三种类型 w_{cls} w_{box} cat(w_{cls}, w_{box}) 2.3. 训练使用A∪B训练box head，使用A训练mask head和τ. 训练方式可分为两种 State-wise Training 第一阶段只使用A∪B中的box标注训练模型。第二阶段固定conv和box head，使用A中的mask标注训练mask head和τ. End-to-end Joint Training（Mask R-CNN论文中表明multi-stask训练优于分别训练每个任务） Box loss和mask loss都直接回传，但在transfer分支上的mask loss回传至τ后停止（由于只有A的mask loss回传至w_{deg}，不存在B的mask loss，为了保持w_{det}在A和B之间的一致性）。 2.4. BaselineMask R-CNN with class-agnostic FCN mask head. 2.5. 扩展：融合FCN+MLP Mask Head 两种mask head互补 FCN. capture detail MLP. capture gist 对baseline的FCN和论文transfer模型的FCN进行MLP融合来提高效果 3. Experiments on COCO 为了模拟partially supervised训练，将COCO中80个类别分为A(20, 类别包含为VOC数据集中，voc)和B(60, non-voc) Oracle Model. 同时利用A和B中的mask标注进行训练Mask R-CNN 3.1. 实验结果 3.2. Ablation Experiments τ的输入 MLP融合 训练方式 4. Experiments on Visual Genome 使用VG数据集的box标注，COCO数据集的mask标注，由于VG没有mask标注，无法计算AP，因为论文直接可视化结果]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Learning</category>
        <category>Transfer Learning</category>
      </categories>
      <tags>
        <tag>Segmentation</tag>
        <tag>Detection</tag>
        <tag>Transfer Learning</tag>
        <tag>Mask R-CNN</tag>
        <tag>Maskx R-CNN</tag>
        <tag>Weight Transfer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(ICCV 2017 Best Paper) Mask r-cnn]]></title>
    <url>%2F2017%2F12%2F22%2FMask%20r-cnn%2F</url>
    <content type="text"><![CDATA[He K, Gkioxari G, Dollár P, et al. Mask r-cnn[C]//Proceedings of the IEEE international conference on computer vision. 2017: 2961-2969. 1. Overview 论文提出一个用于object instance segmentation的通用框架Mask R-CNN. 该框架在Faster R-CNN的基础上 并行增加了一个预测对象mask的分支（全卷积网络，对每个RoI进行预测） 将RoI Pooling操作改为RoI Align操作（将量化操作改为双线性插值操作） Mask R-CNN 5fps on GPU，论文进行object detection、instance segmentation和human pose estimation实验。 1.1. 分割任务 Semantic segmentation(pixel labeling task).对每个像素点进行类间分类，不区分类内实例（例如图像中两只狗的像素点都分类为狗，不区分两只狗的像素点） Instance segmentation(object detection task). 检测图像中每个对象，并对每个对象实例进行分割 1.2. RoI Pooling的量化操作 Faster R-CNN主要用于物体检测任务，其中的RoI Pooling过程存在两次量化 浮点数RoI缩小feature_stride后，对应到共享特征图上的边界量化 RoI对应特征图进行RoI Pooling操作时，划分H*W区域的边界量化 量化操作会导致共享特征图上的RoI与输入图像之间存在错位问题，对分类和检测任务的影响并不大，但是对像素级别mask预测的影响较大（输入与输出之间并没有pixel-to-pixel alignment）。 1.3. RoI Align操作 将RoI Pooling中的量化操作改为双线性插值操作，RoI Align是提高准确度的关键操作。 考虑 Scale (feature_stride)为16的共享特征图 相对于输入图像的预测RoI x 量化操作 计算RoI相对于共享特征图的点坐标x/16.0 量化取整数点坐标round(x/16.0) 在共享特征图上采样该整数点的值 Align操作 计算RoI相对于共享特征图的点坐标x/16.0 在共享特征图上使用双线性插值采样该小数点的值 2. Mask R-CNN Mask R-CNN分为两个阶段 RPN预测RoIs 并行预测每个RoI的box (class, box offset)和binary mask. 看做两个分支 2.1. 解耦mask与class FCN. class预测包含在mask预测中，存在类间竞争问题（per-pixel softmax, 多类别交叉熵）。即mask中每个像素点是一个k维softmax向量（或理解为对每个RoI预测一张mask，像素点分为k类型） Mask R-CNN. class预测从mask预测中分离出来，不存在类间竞争问题（per-pixel sigmoid, binary loss）。即mask中每个像素点是k个sigmoid值（或理解为对每个RoI预测k张mask，每张mask中的像素点分为background与foreground两种类型） 2.2. Multi-task Loss对于每个RoI，有3个loss L_{cls}. log loss L_{box}. smooth L1 L_{mask}. binary cross entropy loss, 只计算对应类别的mask loss. 2.3. 网络结构 Backbone. ResNet, ResNeXt, FPN Head. cls, box, mask 2.4. 细节 训练 RoI与gt box的IoU大于等于0.5为正例，其余为负例 采样RoI正负比例1:3 Mask target为RoI与gt mask的交集 shorter side 800 测试 (与训练阶段的区别) 对预测的box进行NMS过滤 选择top 100个预测box进行mask预测 3. Instance Segmentation 3.1. 实验结果 3.2. Ablationsclass-agnostic mask (29.7AP，m*m mask)与class-specific mask(30.3AP，m*m mask per class)效果相似。 3.3. 边界框检测使用mask分支的Mask R-CNN比没有使用mask分支的Faster R-CNN&amp;RoI Align效果好 4. Human Pose Estimation 考虑身体部位共k个关键点（肩膀、肘等），则对每个RoI预测k张mask，每张mask中只有一个像素点为foreground，因此对m*m mask做softmax操作. 4.1. 实验结果 5. Cityscape数据集扩展实验]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Object Detection</category>
      </categories>
      <tags>
        <tag>Segmentation</tag>
        <tag>Detection</tag>
        <tag>Mask R-CNN</tag>
        <tag>RoI Align</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(NIPS 2015) Faster R-CNN:Towards real-time object detection with region proposal networks]]></title>
    <url>%2F2017%2F12%2F22%2FFaster%20R-CNN%3A%20Towards%20real-time%20object%20detection%20with%20region%20proposal%20networks%2F</url>
    <content type="text"><![CDATA[Ren S, He K, Girshick R, et al. Faster R-CNN: Towards real-time object detection with region proposal networks[C]//Advances in neural information processing systems. 2015: 91-99. 1. Overview SPPnet和Fast R-CNN通过共享特征图的机制解决了proposal计算冗余的问题，因此网络的瓶颈在于region proposal计算。由于 神经网络通过GPU运算 Region proposal通过CPU运算，Selective Search 2s/image 两者在时间上差别太大，在GPU上重新实现region proposal算法虽然能解决问题，但是并没有利用到计算共享的思想。 因此，论文提出Region Proposal Network (RPN). RPN是全卷积网络，模拟预测每个position的object bound和score, nearly cost free (10ms/image). 1.1. 模型结构 论文将RPN与Fast R-CNN合并，称为Faster R-CNN. RPN与Fast R-CNN共享输入图像的卷积特征图conv_map. RPN对conv_map进行计算输出RoIs，类似于attention机制 Fast R-CNN基于RoIs进行计算输出对象框、对象置信度 论文使用的VGG16网络，5fps on GPU. Faster R-CNN在VOC 2007/2012和MS COCO数据集达到state of the art。 1.2. 多尺寸机制 2. Faster R-CNN 论文中的Faster R-CNN使用了两种网络 ZF net RPN与Faster R-CNN共享5层网络 VGG16 RPN与Faster R-CNN共享13层网络 3. RPN 3.1. RPN结构RPN网络有3个Conv层组成 第一层Conv. 256 kernels for ZF net, 512 kernels for VGG16, n*n size (论文使用n=3) 第二层Conv(cls). 2k_anchors kernels, 11 size 第三层Conv(reg). 4k_anchors kernels,11 size 3.2. Anchors对共享特征图上的每个点模拟生成k个anchors box. 对于一张hw的RPN特征图，共有hw*k个anchor box.上述方法为多尺寸anchors, 因此模型中只使用单尺寸输入图片和单尺寸卷积核。 3.3. 测试阶段 输入一张图片，图片经过共享卷积层生成共享特征图 共享特征图经过RPN网络生成whk个anchor box (reg)和对应的whk个前景/背景分类概率 (cls) 选出topN个前景概率最大anchor box，并使用nms算法对anchor box进行过滤，得到最终输入到Fast R-CNN网络中的RoI (proposal或anchor box) 3.4. 训练阶段与测试阶段的区别在于 通过nms生成RoI的数量不同（例如github代码中测试阶段保留300 RoI，训练阶段保留2000 RoI） 计算loss（分类交叉熵和预测框的误差） 3.5. Loss Function基于gt_box定义anchor box为正例还是负例 与gt_box IoU最高的anchor box为正例 与gt_box IoU大于0.7的anchor box为正例 与gt_box IoU小于0.3的anchor box为负例 计算公式 i表示第i个anchor box 表示ground truth, 正例p_{}为1，负例p_{*}为0 L_{cls}为log loss L_{reg}为L1 loss λ默认为10 x表示预测 x_{a}表示anchor x^{*}表示ground truth 3.6. 训练细节 随机抽取一张图片中的256个anchor计算一个minibatch中的loss. 在这256个anchor中，正负例数量为1:1。如果一张图片中正例少于128个，使用负例填充 模型使用ImageNet预训练权重 3.7. 迭代训练每次迭代分为四个步骤 预训练权重初始化模型，训练RPN 使用RPN产生RoI，训练检测网络Fast R-CNN 使用检测网络权重初始化RPN，固定共享层，微调RPN 固定共享层，微调检测网络剩余部分 3.8. 实现细节 使用单尺寸图片，将最短边resize到s=600. anchor scale采用128128, 256256, 512*512 aspect ratios采用1:1, 1:2, 2:1 对于1000600的图片，共大约20000(6040*9)个anchor。剔除超过边界的anchor后，剩余大约6000 anchor for training。选出前景概率topN的anchor，并使用0.7阈值的NMS算法过滤，2000个用于训练的RoI。 4. 实验 4.1. VOC数据集 4.2. 耗时 4.3. Anchor超参数 4.4. λ超参数 5. 代码流程图 RoI是相对于输入图像而言的，因此在将RoI和共享特征图输入到RoI Pooling层的同时，还会输入一个scale参数，也称为feature_stride（表示共享特征图与输入图像的比例）。]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Object Detection</category>
      </categories>
      <tags>
        <tag>Segmentation</tag>
        <tag>Detection</tag>
        <tag>Faster R-CNN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(ICCV 2015) Fast r-cnn Segmentation]]></title>
    <url>%2F2017%2F12%2F22%2FFast%20r-cnn%2F</url>
    <content type="text"><![CDATA[Girshick R. Fast r-cnn[C]//Proceedings of the IEEE international conference on computer vision. 2015: 1440-1448. 1. Overview 由于R-CNN和SPPnet存在一些缺陷，本篇论文在R-CNN的基础上进行改进，提出速度更快，准确度更高的Fast R-CNN结构(相对于R-CNN增加了一个RoI pooling层)。 对于特定一张图像A，R-CNN模型利用selective search方法对其提取N个proposal (RoIs)，输入特征提取网络中进行后续运算。由于N个proposal中存在大量重叠的内容，特征提取网络的运算存在冗余。 因此，Fast R-CNN利用特征图共享机制，首先将图像A输入到特征提取网络中得到特征图B，再将RoIs定位到B中，选择相应区域特征图输入到RoI pooling层进行计算。 可看作是在输入图像提取的特征图上进行region proposal，而非在输入图像上进行region proposal后再对proposal提取特征。 Fast R-CNN在PASCAL VOC 2012数据集上 相比于R-CNN，训练速度快9倍，测试速度快213倍. mAP到达66% (R-CNN为62%) 相比于SPPnet，训练速度快3倍，测试速度快10倍 1.1. R-CNN缺点 Multi state pipeline. 微调、SVM、bounding box回归 速度慢. 47s/image on GPU Expensive in space and time. 在训练SVM之前，需要将提取特征存入硬盘 1.2. SPPnet缺点 Multi stage pipeline 提取的特征需要存入硬盘 1.3. Fast R-CNN贡献 更高mAP Single stage 能够更新所有网络层 不需硬盘存储 2. 结构与训练 2.1. RoI pooling 输入. 输入图像中的RoI在其特征图上对应的区域（RoI是相对于输入图像而言的，因此将RoI和共享特征图输入到RoI Pooling层的同时，还会输入一个scale参数，也称为feature_stride（表示共享特征图与输入图像的比例））。 指定超参数H*W(论文设置为77)，即RoI pooling层的输出大小。对于hw的输入特征图，RoI pooling层将其划分为H*W的网格，对每个网格进行max pool操作 2.2. 预训练权重初始化网络 使用ImageNet预训练权重，将 最后一个max pooling层换成RoI pooling层 最后一个 (fc, softmax) 换成2个 (fc, softmax)分支 网络的输入为 图像 对应图像中的RoIs 2.3. Fine-tuning 采样N张图片，每张图片采样R/N个RoIs. 论文将N设置为2，R设置为128，即对采样的两张图片，各采样64个RoIs 由于所属某张图片的各RoI能够共享从该图片提取的特征图，因此相比与对128张图片各采样1个RoIs的方法快64倍 2.4. Multi-task loss p由第一个分支输出，包含对k+1个类别的预测置信度，u为ground truth t由第二个分支输出，包含k个类别的bounding box regression offset预测. v为ground truth, 归一化到标准正态分布 [u≥1] 当u≥1时，值为1，否则为0 λ超参数设置为1 对于一张图片（包含2只猫，一只狗），共产生200个region proposal (RoI)，模型分类器包含4类（猫、狗、鸟和背景） cls loss 每个RoI会产生1个置信度向量[猫，狗，鸟，背景]，直接与 class groud truth向量[p1, p2, p3, p4]计算loss loc loss 每个RoI会产生针对3个类别（猫、狗、鸟）的bounding box偏移量BB猫, BB狗, BB鸟，只计算这个RoI的class ground truth对应的bounding box loss. 例如，如果一个RoI的class ground truth为狗，那么计算BB ground truth与BB狗的loss. 上述为训练阶段的操作，在测试阶段，对N个RoI预测结果使用NMS算法进行过滤。 2.5. Mini-batch sampling Mini-batch大小为R=128，每张图片64个RoIs，25%为正样本（IoU≥0.5），其余为负样本（IoU∈[0.1,0.5)，忽略小于0.1的RoI） 0.5概率水平翻转图像，扩大数据集 2.6. Scale invariance Brute force. 直接将图片resize 成固定大小 Image pyramid. 在训练阶段，每次随机采样一个特定的pyramid scale。 论文在试验中证明image pyramid对mAP提升很小，但耗时明显增加。因此论文采用第一种方法。 2.7. 测试阶段 R设置为2000，计算得到每个RoI的预测和bounding box regression 使用R-CNN中的NMS方法减少预测结果RoI的数量（NMS仅在测试阶段使用） 2.8. 使用Truncated SVD提速对于u*v的权重矩阵，将其分解为 U u*t Σ t*t V v*t将计算量从uv降低至t(u+v)，将t远小于u,v时。因此，可将fc层的W分解为两个fc层 ΣV (no bias) U (包含原fc层中的bias) 3. 实验结果 3.1. VOC数据集增大数据集能提高准确度. 3.2. 耗时 3.3. Fine-tune对于较深的网络，只微调fc层的结果明显差于微调卷积层+fc层的结果。 3.4. multi-task训练效果 3.5. Scale invariancesingle scale和multi scale性能相差不大，但multi scale耗时增加明显。 3.6. Softmax与SVM 3.7. More proposal worse]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Object Detection</category>
      </categories>
      <tags>
        <tag>Segmentation</tag>
        <tag>Detection</tag>
        <tag>Fast R-CNN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(CVPR 2014) Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation]]></title>
    <url>%2F2017%2F12%2F22%2FRich%20Feature%20Hierarchies%20for%20Accurate%20Object%20Detection%20and%20Semantic%20Segmentation%2F</url>
    <content type="text"><![CDATA[Girshick R, Donahue J, Darrell T, et al. Rich feature hierarchies for accurate object detection and semantic segmentation[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2014: 580-587. 1. Overview 论文提出R-CNN模型 (combine region proposals with CNN)，在VOC 2012物体检测任务上达到53.3% mAP，相对于当前best result提高了30%。 由于detection标注数量较少，而classification标注数量较多。因此 首先，基于classification标注信息预训练分类网络 接着，针对detection标注信息，选择合适的分类层替换分类网络中最后的分类层，进行微调 最终，利用该网络（去掉一些fc和softmax层）提取的4096维特征，训练SVM分类器 2. Modules R-CNN模型包含3个模块: Region Proposal, CNN, SVM. 2.1. Region Proposals 选用selective search方法： 初始化n个region，组成region集合R 计算每两个region之间的相似度，加入相似度集合S中 选出S中相似度最大的region pair (r_{i}, r_{j})，将r_{i}, r_{j}合并为r_{t} 从S中剔除与r_{i}, r_{j}有关的region pair 计算r_{t}与剩余所有region的相似度，并加入S中 将r_{t}加入到R中 重复[加粗部分]直到S为空 2.2. Feature Extraction 输入为mean-subtracted 227*227 RGB，结构由5层CNN和2层FC组成，输出为4096维特征向量。对proposal进行transform的3种方法 将proposal放入tightest框中，并额外加入context信息 将proposal放入tightest框中，不加入context信息 warp 此外，每种方法都是用context padding p像素 3. 测试阶段 使用selective search方法提取2000个region proposal. 最终用贪心non-maximum suppression算法reject proposal预测结果数量。 3.1. non-maximum suppression (NMS) 对于一个特定的分类 从region预测结果集合P中，选出最高得分的region A 遍历region集合，剔除与A的IoU大于指定阈值的region 将A存入最终目标结果集合Q中 重复上述3个步骤，直至P为空，则Q为最终所求的预测regions 对N个类别重复上述整个过程。 3.2. 测试耗时13s/image on GPU, 53s/image on CPU. 主要耗时在于dot product和NMS。在实际中，将一个图像中的2000个proposal组成一个矩阵，因此Feature extraction输出的特征为2000*4096. 4. 训练阶段 训练分为3个阶段：监督预训练、微调、SVM分类器训练。 4.1. 监督预训练使用auxiliary数据集(ILSVRC2012 classification)预训练分类网络。 4.2. Domain-specific fine-tuning 把分类网络的最后一层(1000 way)替换成随机初始化的(N+1背景 way)层。 正样本 与ground-truth的IoU≥0.5的region 负样本 minibatch 128 (32正样本+96负样本) 4.3. SVM分类器 正样本 ground-truth 负样本 与ground-truth的IoU＜0.3的region. 忽略＞0.3的region 5. Bounding Box Regression 为了提高localization准确度，论文进一步在R-CNN模型中加入了bounding box regression. ground truth和每个proposal的bounding box都由坐标(中心点坐标，宽，高)表示 通过学习一个映射，将proposal box映射到ground truth box. 6. 实验结果]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Object Detection</category>
      </categories>
      <tags>
        <tag>Segmentation</tag>
        <tag>Detection</tag>
        <tag>R-CNN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[条件随机场&非局部均值去噪&双边滤波]]></title>
    <url>%2F2017%2F12%2F19%2F%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA%26%E9%9D%9E%E5%B1%80%E9%83%A8%E5%9D%87%E5%80%BC%E5%8E%BB%E5%99%AA%26%E5%8F%8C%E8%BE%B9%E6%BB%A4%E6%B3%A2%2F</url>
    <content type="text"><![CDATA[如何轻松愉快地理解条件随机场（CRF）非局部均值去噪（NL-means）图像非局部均值滤波的原理Bilateral Filtering(双边滤波) for SSAO 1. 条件随机场 对一系列连续时间段的照片分类时，为了提高分类器的性能，常常将相邻照片的标签信息考虑进来，而非单独考虑某张照片进行分类——条件随机场(CRF). 1.1. 词性标注中的特征函数 考虑对标注序列评分问题。定义CRF中的特征函数，如 s 需要标注词性的句子 i 句子s中第i个单词 l_{i} 第i个单词标注的词性 l_{i-1} 第i-1个单词标注的词性 特征函数输入0或1，0表示标注序列不符合特征函数的特征。该特征函数仅考虑当前单词i的标签与其前一个单词i-1的标签来进行判断，因此称为线性链CRF。 1.2. 特征函数到概率 定义好一组特征函数及其权重λ后，即可对标注序列进行评分。 第一个Σ 所有特征函数对某个标注序列的评分求和 第二个Σ 某个特征函数对某个标注序列中每个单词的评分求和 进行指数化和标准化即可得到标注序列的概率值（属于某类的概率） 1.3. 特征函数例子 l_{i-1}是介词，l_{i}是名词，则为1 l_{i}为副词，且第i个单词以”ly”结尾，则为1 如果i=1，l_{i}为动词，且句子以”?”结尾，则为1 1.4. 与逻辑回归比较CRF可看做逻辑回归的序列化版本。逻辑回归是用于分类的对数线性模型，CRF是用于序列化标注的对数线性模型。 与HMM比较 HMM也能解决词性标注问题，其思路是生成方式。即已知标注序列的情况下，判断生成该标注序列的概率。 p(l_{i} | l_{i-1}) 转移概率，如l_{i-1}=介词，l_{i}=名字，表示第i-1个词为介词，第i个词为名词的概率 p(w_{i} | l_{i}) 发射概率，如l_{i}=名词，w_{i}=”ball”，表示第i个词为名字，则该词为”ball”的概率 CRF比HMM强大，每个HMM模型都等价于某个CRF. 且CRF能解决许多HMM无法解决的问题。 对HMM取log 与CRF进行比较 如果把HMM中的概率看成CRF中的权重，可发现每个HMM转移概率能够定义成一个对应的CRF特征函数，并乘以权重（HMM的概率）。 HMM中的发射概率同理。 CRF优势 CRF可定义大量不同的特征函数 HMM具有局部性，只考虑当前单词i与其前一个单词i-1. CRF可定义具有全局性的特征函数 HMM中的log概率值小于等于0，即转换成CRF后，其权重受到限制。而CRF中每个特征函数的权重可是任意值 2. 非局部均值去噪 基本思想：当前像素值的估计由图像中与其具有相似领域结构的像素加权平均得到。该算法能够在去噪的同时，最大程度保留图像细节特征。 2.1. 算法实现 理论上，该算法应搜索整个图像范围，但实际考虑到效率问题，会设定两个固定大小窗口。 搜索窗口（搜索范围） 邻域窗口（y为中心） 邻域窗口在搜索窗口中滑动，根据邻域间相似性确定各像素y的权值w(x,y)。 含噪声图像为v，去噪后的图像为u，其计算方式为 权值计算方式为 V(y) 以y为中心的邻域. ||V(x) - V(y)||^2 两邻域间的距离 Z(x) 归一化系数 h 平滑参数，控制高斯函数的衰减成都，h越大，高斯函数变换越平缓，去噪水平越高，但图像越模糊。h取值应以图像中噪声水平作为依据 其中 3. Bilateral Filtering(双边滤波) 在滤波算法中，目标点的像素值通常由其周围邻域中的像素值所决定。 3.1. 2D高斯滤波 2D高斯滤波算法是对其一定范围的邻域内像素值赋以不同高斯权重值，进行加权平均。权重因子基于两像素点之间的空间距离得到，即离目标像素点距离越近，对最终结果贡献越大。 ξ 邻域像素点坐标 x 目标像素点坐标 f 图像 c(ξ, x) 基于两像素点空间距离的权重 k_{d}(x) 单位化 但高斯滤波存在缺陷，只考虑了像素之间的空间位置关系，而没有考虑考虑像素值之间的相似程度，因此会导致边缘信息（图像中不同颜色的区域）丢失。 3.2. 基于像素值关系的权重 Bilateral中加入了另一部分权重 s(f(ξ), f(x)) 为基于两像素点的像素值相似关系的权重。 3.3. Bilateral滤波 综合考虑有 其中 距离超过一定程度的像素点对目标像素点影响很小，可忽略。限定局部子区域后的离散化公式为 3.4. 可视化 对于带有噪声的图片 蓝框中心为目标像素所在位置，则其对应的高斯权重为 双边权重为 可看出Bilateral加入的相似程度部分将图像左侧与目标像素点的像素值差值过大的点滤去，从而保持了边缘。 进一步可视化，双边滤波后的图像为 原图像为 高斯滤波后的图像为 可看出高斯滤波后的图像存在线性变化边缘，即边界的丢失。而双边滤波后的图像保持了边缘梯度。]]></content>
      <categories>
        <category>Knowledge Note</category>
      </categories>
      <tags>
        <tag>Non-local Means</tag>
        <tag>CRF</tag>
        <tag>Bilateral Filtering</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(CVPR 2018) Non-local Neural Networks]]></title>
    <url>%2F2017%2F12%2F19%2FNon-local%20Neural%20Networks%2F</url>
    <content type="text"><![CDATA[Wang X, Girshick R, Gupta A, et al. Non-local neural networks[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018: 7794-7803. 1. Overview convolutional和recurrent操作建立的block，每次只能处理一个局部区域，基于非局部均值滤波的思想，论文提出具有通用性的非局部组件，能够capture long range dependencies. 非局部组件通过计算所有点的特征加权和作为目标点的响应。非局部操作能够capture相距较远的点在时间和空间上的关系。 论文使用非局部模型（非局部-残差网络） 在Kinetics和Charades视频数据集上做classification任务，性能超过目前的冠军 在COCO数据集上做object detection/segmentation和pose estimation任务，性能有明显提高 1.1. Repeated 由于convolutional和recurrent建立的block每次只能处理一个局部区域，因此需要通过重复操作来实现long range dependencies. convolutional操作通过堆叠(stack) CNN层实现 recurrent操作通过记忆+循环实现 但repeated实现方式存在一些缺陷 效率低下 优化困难 信息很难在相距较远的点之间来回传递(multi-hop dependency modeling多跳依赖建模)。（例如，对于100*100的图像，左上角的点和右下角的点在前几层CNN中并无关联） 1.2. 非局部操作优点 直接计算任意两点之间的关系来capture long range dependencies，忽略点之间的距离 非局部操作高效，仅使用少量的层就能达到betst result, 额外增加的计算量少 保持输入大小不变 能capture空间和时间维度，比3D CNN更高效 2. Related Work 2.1. 非局部图像处理2.2. 图形模型 CRF能够被用于segmentation, 而非局部操作具有通用性，可用于分类和检测。 2.3. Self-attention 通过所有点的加权平均计算当前点的响应。self-attention可看做是非局部均值的一种方式。 2.4. Interaction Networks 使用pairwise interaction。其变体Relation Network使用到所有pair of positions. 2.5. Video Classification Architecture 最通常的做法是结合CNN和RNN，但是前馈模型使用3D CNN，将预训练权重的2D卷积核inflating成3D卷积核。此外，optical flow和trajectory能提高性能。 3. Non-local Neural Networks 3.1. Formulation通用形式为 i 输出位置点 j 所有位置点 x 输入 y 输出 f pairwise function, 计算点i和点j之间关系，输出为标量 g 计算representation of 点j C 归一化因子 3.2. 区别 非局部操作涉及到所有位置点，而convolutional操作只处理局部区域，recurrent操作通常只涉及到当前时间状态与最近时刻状态(j=i或j=i) 非局部操作基于不同点之间的关系计算响应，而fc的权重通过学习得到。 非局部操作能够适用不同大小的输入，并且能够任意加入到网络中，而fc的输入大小需要事先固定，且通常只放到网络的最后一层。 3.3. Instantiations g函数使用线性embedding 通过2D CNN（空间）或3D CNN（时空）实现。 Pairwise function f函数有多种选择 Gaussian Embedded Gaussian 可写成softmax形式 Dot Product N为x位置点的总数，与Embedded Gaussian的区别在于没有softmax操作。 Concatenation [,]表示concat操作，wf为权重向量，将concated向量投影为标量。 3.4. Non-local Block x为residual连接. Block可以任意插入到预训练模型中，并保持模型的初始行为，即将W_{z}权重初始化为0. 当block使用在high level的特征图上，计算量很少。例如，T=4， H=W=14或7. 3.5. 减少计算量 Θ和φ网络channel数量设置为输入的一半 在g和φ网络后进行下采样，如连接max pooling 层 4. Video Classification Model 4.1. 2D ConvNet (C2D) 输入维度为32帧224224，其本质为1kk的卷积核，直接使用ImageNet预训练权重初始化。残差block中的卷积层相当于逐帧处理操作（卷积核和步长在帧的维度上为1）。 4.2. Inflated 3D ConvNet (I3D) 2D kk卷积核inflate为3D tkk卷积核。将kk训练权重扩展到t plane，并乘以缩放因子1/t. 两种inflate方式 inflate残差block中的33卷积核为33*3 inflate残差block中第一个11卷积核为31*1 I3D模型优于CNN+LSTM. 4.3. Non-local network将non-local网络插入到C2D和I3D网络中。 4.4. 实现细节 Train阶段 ImageNet预训练权重初始化 输入为32帧clips，从原始视频中random crop连续64帧，并丢弃剩余其他帧 输入尺寸为224*224，通过random crop得到 迭代400k次，学习率初始为0.01，每150k迭代减少10% momentum 0.9，权重衰减0.0001，dropout 0.5 fine tune模型是不固定BN层，能够防止过拟合 仅在W_{z}层后插入BN层，scale参数初始化为0，可保持整个non local block为全等映射，即保持预训练模型的初始行为 Inference阶段 均匀地从一整段视频中抽取10个clip，计算softmax得分，最终预测结果为10个softmax得分的平均。 5. Experiments on Video Classification Kinetics数据集246k训练视频，20k验证视频，包含400个人类动作分类。 5.1. Instantiations比较不同类型的非局部操作，得出结论非局部操作类型的影响并不大，性能的提升主要是由于非局部操作，而非attention机制。因此实验中默认使用Embedded Gaussian. 5.2. Stage to Add Non local Blockres5的空间大小为7*7，因此可能是因为没有足够准确的空间信息，导致准确度略微下降。 5.3. 使用更多的非局部block一般而言，更多的非局部block能提升性能准确度，multiple非局部block能够实现long range multi-hop communication (多跳交流). 从5 block R50 73.8%与base R101 73.1%，可以看出准确率的提升并不是因为层数加深，而是因为非局部block. 此外，实验也将加入的非局部block替换为标准残差block，发现准确度并没有提升。 5.4. 时空上的非局部操作 5.5. 非局部2D ConvNet vs 3D ConvNet 5.6. 非局部3DConvNet使用I3D_{311}模型 5.7. 更长的序列将每个输入clips增加到128帧，所有模型的准确度都有提升。 5.8. 与目前最好的结果比较非局部网络方法在没有使用optical flow和其他技巧的情况下，取得较好结果。 Charades数据集8k训练视频，1.8k验证视频，2k测试视频，包含157个人类动作分类。 6. Experiments on COCO 6.1. Detection与Segmentation 在不同规模的模型中加入非局部block均能提高准确度，说明非局部依赖并没有通过增加层数堆叠充分capture. 此外，单层非局部block只增加了不到5%的计算量，使用更多的非局部block结果反而有所下降。 6.2. Keypoint Detection]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Architecture</category>
        <category>Component</category>
      </categories>
      <tags>
        <tag>Component</tag>
        <tag>Classification</tag>
        <tag>Attention</tag>
        <tag>Non-local</tag>
        <tag>Video Classification</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(NIPS 2015) Spatial transformer networks]]></title>
    <url>%2F2017%2F12%2F19%2FSpatial%20transformer%20networks%2F</url>
    <content type="text"><![CDATA[Jaderberg M, Simonyan K, Zisserman A. Spatial transformer networks[C]//Advances in neural information processing systems. 2015: 2017-2025. 1. Overview 虽然CNN的效果很好，但是仍然缺乏对数据的空间不变能力，从而限制了计算和参数的效率。因此，论文提出Spatial Transformer Network (STN)。 1.1. STN 在网络中对数据显式地进行空间操作（平移、旋转、缩放、裁剪、扭曲）。由于该操作可微，因此模型能够end to end训练。 根据输入数据，动态生成空间操作参数Θ。 网络参数直接通过loss回传进行学习。可直接添加到神经网络模型中，整个训练不需额外的监督信息加入。 空间操作后的数据是与后续特定任务高度相关的。另一方面，变换后的低分辨率数据比原始数据的计算效率更高。 通过对数据进行操作实现不变性，而不是对特征提取器（卷积核）。 1.2. 适用的任务 classification co-localization spatial attention 2. Spatial Transformers STN包含3部分 (Figure 2) localization network. grid generator. sampler. 2.1. Localization Network 输入U(h, w, c) 输出空间变换参数Θ 网络可以是任何形式，如FCN、CNN等。仿射变换Θ的参数为6，投影变换参数为8，以及thin plate spline (TPS). 模型对最后一层的weight矩阵初始化为0，bias初始化为[[1, 0, 0], [0, 1, 0]]（仿射变换），即全等变换。 2.2. Parameterised Sampling Grid 首先根据采样网格大小（超参数）生成标准网格(t; x,y∈(-1, 1); (h, w, 2)). 利用空间变换参数Θ对其进行变换操作，生成采样网格(s; x,y∈(-1, 1); (h, w, 2)). 2.3. Differentiable Image Sampling 通用的采样公式可写为 k为通用采样kernel; x, m, y, n为坐标点。Φ为kernel的参数。 对于整数采样kernel，公式简化为 取x+0.5下界整数，δ函数为Kronecker delta函数 对于双线性采样kernel，公式简化为 该公式可导 2.4. Spatial Transformer Networks 由于Θ显式地编码了变换，因此也可将Θ传入后续的网络，而非变换后的特征图（或图片）。 可用STN对特征图进行上采样或下采样。但是，用固定的、小空间支持的采样kernel（双线性kernel）进行下采样会造成影响。 STN可级联或并行在网络中。 3. Experiments 3.1. Distorted MNIST 数据集distorted方式分为 R 旋转，±90°之间。 RTS 旋转+缩放+平移 P 投影 E 弹性形变（破坏性，不可逆） 所有模型都具有相同数量参数，分别使用3类变换操作：仿射变换(Aff)、投影变换(Proj)、薄板样条变换(TPS)。实验发现TPS最有效。 3.2. MNIST Addition 输入两张数字图片(h,w,2)，输出数字的和。 3.3. Street View House Numbers 每张图片有1～5个数字。因此，模型采用级联STN，并使用5个独立的softmax分类器，每个分类器包含一个空字符 3.4. Fine-Grained Classification CUB-200-2011数据集，模型采用并行STN结构。 3.5. Co-localization 使用半监督学习来定位图像中的物体。基于正确定位对象A与正确定位对象B之间的距离，比A与随机定位crop小的假设，构造hinge loss T表示crop，e为编码函数，α为margin，实验设置为1。数据集的构建操作为：将2828的数字图片放在8484背景中，并将从训练集中采样得到的16个随机6*6 crop放入背景中。当预测定位与ground-truth的交集大于0.5时，定义为预测正确。 3.6. Higher Dimensionnal Transformer 模型使用3D仿射变换和3D双线性插值操作。 另一种处理方法是：将3D空间投影到2D空间，例如]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Architecture</category>
        <category>Component</category>
      </categories>
      <tags>
        <tag>Localization</tag>
        <tag>Fine-Grained Classification</tag>
        <tag>Component</tag>
        <tag>Classification</tag>
        <tag>Attention</tag>
        <tag>STN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Transformation]]></title>
    <url>%2F2017%2F12%2F19%2FTransformation%2F</url>
    <content type="text"><![CDATA[仿射变换与投影变换数值方法——薄板样条插值（Thin-Plate Spline）薄板样条函数(Thin plate splines)的讨论与分析关于Thin Plate Spline （薄板样条函数） 1. 仿射变换 6个参数 2. 投影变换 8个参数. 通常采用z=1时的平面。 3. 薄板样条函数 3.1. 插值 已知函数y=f(x)在N+1个点x1, x2, x3, …上的函数值y1, y2, …，但是未知函数f(x). 可通过插值函数p(x)来逼近f(x). 常用的插值函数有 多项式函数 对于N次p(x)有N+1个参数，由于N+1个参数满足N+1个约束条件，因此可求出p(x). 但N阶多项式必有N-1个极值点，得到的插值函数摆动较大，类似过拟合现象。 样条函数 即分段函数，表示在相邻点x_{k}和x_{k+1}之间用低阶多项式S_{k}(x)进行插值。分段线性插值和三次样条插值都属于样条插值。 3.2. TPS (Thin plate splines) 寻找一个通过所有控制点的弯曲最小的光滑曲面 弯曲最小由能量函数定义 对于插值问题：自变量x是2维空间中的点，函数值y也是2维空间中的点，给定N个自变量x和对应函数值y，求插值函数 使得 即求两个插值函数φ。根据能量函数可得TPS插值函数形式 其中c是标量，向量a(2, 1)，向量w(N, 1)，函数向量 φ有N+3个参数，而目前只有N个约束（即y=φ(x)，N对数据）。因此，添加三个约束 有 已知N对数据，即已知X和Y，也就是第1个矩阵和第3个矩阵，求第二个矩阵（薄板样条插值函数的参数） 令 有 即可求得插值函数的参数。更进一步，可将x方向和y方向的插值函数φ通过一个矩阵运算计算 该逆矩阵称为弯曲能量矩阵 3.3. TPS应用在STN网络上 对于STN而言，已知 2维空间中的自变量x（第一个矩阵，根据尺寸超参数生成的标准网络中的点） 薄板样条（TPS）插值函数的参数（第二个矩阵，由Localization Network生成） 求 2维空间中的函数值y（第三个矩阵，标准网络经TPS变换后，得到的采样网格。即标准网格中的点映射到输入特征图或图像网格中的点）。]]></content>
      <categories>
        <category>Knowledge Note</category>
      </categories>
      <tags>
        <tag>Transformation</tag>
        <tag>Thin-Plate Spline (TPS)</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(2017) Understanding Matrix capsules with EM Routing]]></title>
    <url>%2F2017%2F12%2F18%2FUnderstanding%20Matrix%20capsules%20with%20EM%20Routing%2F</url>
    <content type="text"><![CDATA[Hui J. Understanding Matrix capsules with EM Routing[J]. Blog.​ Nov, 2017.“Understanding Matrix capsules with EM Routing (Based on Hinton’s Capsule Networks)” 1. Overview 1.1. 向量capsule缺陷此前提出的capsule结构存在一些缺陷 利用pose向量的长度表示存在概率时，使用了squash函数将向量长度压缩至[0, 1]，这阻碍了一些有意义的目标函数的使用。 使用余弦角度测量两个pose向量之间的agreement，不能很好地区分quit good agreement和very good agreement. 而使用Gaussian cluster能使实现这点。 对于长度为n的pose向量，其变换矩阵有n*n个参数。而对于n个元素的pose矩阵，其变换矩阵只有n个参数。 1.2. 矩阵capsule结构因此，本文提出一种新的capsule结构，其中包含 a logistic unit，表示该entity存在概率。 a 4*4 pose矩阵，通过学习表示entity与viewer之间的关系。 L层某个$capsule_i$的pose矩阵乘以viewpoint-invariant变换矩阵得到的结果为$capsule_i$对L+1层各$capsule_c$的pose矩阵的vote. Viewpoint-invariant变换矩阵通过学习能够表示part-whole关系。 每个vote都对应一个权重系数$r_i$,权重系数通过EM算法迭代更新。在本文中使用的迭代次数为3. 矩阵capsule结构在smallNORB数据集上相对于目前的state-of-the-art减小了45%的test errors. 并且更能够抵抗白盒对抗攻击。 2. Introduction 2.1. Viewpoint与Pose矩阵 Viewpoint的改变会导致图像像素产生较大的变化，但对表示objet和viewer之间关系的pose矩阵而言，只会产生简单的线性影响。 随着viewpoint的改变，pose矩阵以一种协调的方式进行变化，因此不同部位votes的agreement保持恒定。 2.2. 反向Attention由L+1层的所有$capsule_c$竞争L层的某个$capsule_i$，即权重系数和为1. 而正向Attention是由L层的所有$capsule_i$竞争L+1层的某个$capsule_c$。 3. EM迭代路由算法将L层各capsule_i看为一个data point，L+1层各$capsule_c$看为一个Gaussian模型。因L层各$capsule_i$的vote路由问题转化为对给定数量data point进行Gaussian聚类问题。例如，眼睛、鼻子、嘴巴的$capsule_i$都vote（聚成一个cluster） L+1层中某个$capsule_c$，即检测到人脸。 3.1. 计算公式 cost表示某个$capsule_i$属于$caps_c$的一部分的概率。cost越低，则属于的可能性越大。 λ为超参数，b为描述$capsule_c$均值的cost，可学习。 3.2. EM路由算法 实验中设置的迭代次数为3. 4. Capsule模型 4.1. ReLU+Conv1 5*5 kernel, 32 channel (A=32), stride 2. 输入: (b, c, 32, 32) 输出: (b, 32, 14, 14 ) 4.2. PrimaryCaps 1*1 kernel, 32 channel (B=32), stride 1. 输入: (b, 32, 14, 14) 输出: (b, 32, 14, 14, 17) 4.3. ConvCaps1 3*3 kernel (K=3), 32 channel (C=32), stride 2. 输入: (b, 32, 14, 14, 17) 输出: (b, 32, 6, 6, 17) 4.4. ConvCaps2 3*3 kernel (K=3), 32 channel (D=32), stride 1. 输入: (b, 32, 6, 6, 17) 输出: (b, 32, 4, 4, 17) 4.5. Class Capsule 可看做h*w kernel, 10 channel (分类类别), stride 1. 该层使用了Coordinate Addition方法，额外加入每个感知区域的xy坐标到vote的前两个元素中。 输入: (b, 32, 4, 4, 17) 输出: (b, 10, 17) 4.6. Spread Loss 最大化目标类别的activation与其他类别activation的差值。m为margin，从0.2开始线性增长，避免dead capsule. 5. Experiments samllNORB数据集包含5种类别的玩具图片：飞机、车、卡车 人类和动物。每种类别都有18个不同的视角(0-340), 9种高度和6种光照条件。图片大小为9696.实验中将其下采样为4848并做32*32 random crop操作。 更进一步，实验使用训练集不包含的viewpoint进行测试 6. 对抗鲁棒性]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Architecture</category>
        <category>Capsules</category>
      </categories>
      <tags>
        <tag>Classification</tag>
        <tag>CapsulesNet</tag>
        <tag>EM Algorithm</tag>
        <tag>Cognitive Neuroscience</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[EM Algorithm]]></title>
    <url>%2F2017%2F12%2F18%2FEM%20Algorithm%2F</url>
    <content type="text"><![CDATA[EM算法学习(Expectation Maximization Algorithm) 1. 凸函数 上凸函数 f[(a+b)/2] ≥ [f(a)+f(b)]/2 当a≠b时 f[(a+b)/2] &gt; [f(a)+f(b)]/2成立，那么称f(x)为严格的上凸函数，等号成立的条件当且仅当a=b,下凸函数与其类似。 2. Jensen不等式 如果f是上凸函数，X是随机变量，那么 f(E[X]) ≥ E[f(X)]. 如果f是严格上凸函数，那么 E[f(X)] = f(E[X]) 当且仅当p(X=E[X])，也就是说X是常量。 3. EM算法 3.1. Problem Definition 考虑一个参数估计问题，现有共n个训练样本，需有多个参数θ去拟合数据（高斯混合模型），那么这个log似然函数是 由于Θ中多个参数的某种关系，导致上面的log似然函数无法直接或梯度下降法求最大值时的Θ值。引入隐变量z，并使用Jensen不等式得到下界 (9)式紧下界为等号成立时，即随机变量 为常数。又因为 有 所以(9)成立的条件是 Q(zi)=P(zi|yi, Θ) 即后验概率。 3.2. Algorithm Step E步骤 已知（初始化）Θ和样本数据yi，计算Q(zi)。 M步骤 利用Q(zi)简化(9)中的多项式，从而可用梯度下降求偏导更新Θ值。 E步骤 根据Θ计算Q(zi). M步骤 根据Q(zi)更新Θ. … 迭代直至收敛。]]></content>
      <categories>
        <category>Knowledge Note</category>
      </categories>
      <tags>
        <tag>EM Algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(NIPS 2017) Dynamic Routing Between Capsules]]></title>
    <url>%2F2017%2F12%2F18%2FDynamic%20Routing%20Between%20Capsules%2F</url>
    <content type="text"><![CDATA[Sabour S, Frosst N, Hinton G E. Dynamic routing between capsules[C]//Advances in neural information processing systems. 2017: 3856-3866. 1. Overview 基于认知神经科学领域中芒卡斯尔(V.B. Mountcastle)发现的大脑皮层功能柱结构(功能柱-微柱)，论文提出capsule概念。 1.1. Capsule定义 Capsule是一组神经元，其activity vector表示一个特定实体(entity)类型(object or object part)的实例化参数(instantiation parameters). Activity vector的长度表示entity存在的概率，方向表示instaniation parameters. Instantiation parameters includes pose (position, size, orientation), deformation, velocity (速度), albedo (反照率), hue, texture etc. 1.2. Capsule机制L level的activity $capsule_i$ 通过transformation matrices predict L+1 level $capsules_j$的instantiation parameters. 当多个predictions agree, L+1层中某个$capsule_j$ become active. (即高层$capsule_j$代表了低层$capsule_i$的组合，如数字2的多个低层特征由多个$capsule_i$表示，同时满足这些特征时，某个代表这些特征组合的高层$capsule_j$被激活) 在CapsNet中使用了iterative routing-by-agreement机制：对于L+1 level中某个$capsule_j$的 activity vectors ($v_j$)和L level中的一个$capsule_i$对其作出的prediction ($\hat{u_{j|i}}$)而言，两者scalar product ($\hat{u_{j|i}} · v_j$)越大, A越倾向于将其output发送给这个$capsule_j$. CapsNet在MNIST数据集上优于CNN，最高达到99.75%左右。 2. Introduction 2.1. 假设 假设，人类的多层视觉系统会在每个fixation上建立一个类似parse tree的东西。 假设，对于一个fixation，parse tree可以通过对一个固定的多层神经网络carve (类似于剪枝)得到。 每层会被划分为多个神经元组(capsule), parse tree中每个node都会对应一个active capsule (即parse tree中每个node代表一层神经网络中的一组神经元，该组神经元可称为capsule). 通过iterative routing机制，每个active capsule都会选择上一层中的某个capsule作为parent node. 对于higher level视觉系统，iterative routing机制将用于解决物体部分组合到整体的过程。 2.2. 存在性表示方法 logistic unit. length of the vector of instantiation parameters (squashing function, 论文采用). 2.3. Dynamic Routing 由于capsule输出的是向量，用dynamic routing机制能够确保capsule的输出send to合适的parent node. 对于L level的capsule A和L+1 level的capsule B. A首先计算prediction vector (A的output乘以weight matrix), 然后计算prediction vector与B ouput的scalar product, 反馈给AB之间的coupling coefficient. scalar product越大，对AB之间coupling coefficient的反馈越大，即-AB之间的coupling coefficient增加，A与其他parents之间的coupling coefficients减小(softmax function). 2.4. 分割重叠数字 由于max pooling的routing机制会忽略local pool中大部分active feature detectors. 而routing by agreement more effective不存在这样的问题，因此能够分割高度重叠的object. Max pooling throw away information about the precise position of the entity within the region. 2.5. 相比于CNN的改进 使用vector output capsule代替CNN中的scalar output feature detectors. 使用routing by agreement代替max pooling. 2.6. Coding For low level capsules, location information is place coded by which capsule is active. As we ascend the hierarchy more and more of the positional information is rate coded in the real valued components of the output vector of a capsule. 从place coding到rate coding表明higher level capsule用更高自由度represent更复杂的entities, 即capsule的维度随着层级的增加而增加。 3. Capsule计算 3.1. Squashing function(L level $capsule_i$, L+1 level $capsule_j$) 使得较短向量长度缩放为0，较长向量长度缩放为1. $v_j$. capsule_j的输出向量。$s_j$. $capsule_j$的总输入向量。 即L level中所有$capsule_is$到L+1 level中特定一个$capsule_j$的prediction vector加权和。 3.2. Total input &amp; prediction vectors $u_i$. capsule_i的输出向量。$w_{ij}$. weight matrix.$u_{j|i}$. prediction value from a $capsule_i$ to a $capsule_j$.$c_{ij}$. coupling coefficients between a $capsule_i$ and a $capsule_j$. 3.3. Coupling coefficients $b_{ij}$. the log priors probabilities that $capsule_i$ should be coupled to $capsule_j$. 由两个capsule (i和j)的location和type决定，而非当前input image决定。 通过测量$v_j$和$u_j|i$之间的agreement迭代refine $c_{ij}$. 3.4. AgreementThis agreement is treated as if it were a log likelihood and is added to the initial logit, $b_{ij}$. 3.5. Algorithm 4. Margin Loss SVM损失函数 $m^{+}=0.9$, $m^{-}=0.1$ $T_c=1$, iff a digit present suggest $λ=0.5$ 5. CapsNet结构 (Figure 1) 5.1. Conv1256 kernels, 9x9, 1 stride, ReLu. 输入维度 . (batch_size, 1, 28, 28) 输出维度. (batch_size, 256, 20, 20) 5.2. PrimaryCapsules 输入维度 . (batch_size, 256, 20, 20) 输出维度. (batch_size, 32, 6, 6, 8) 激活primary capsule过程可看作是图像render的逆过程。 PrimaryCapsules is a convolutional capsule layer, 包含32个通道，每个通道含有一个convolutional 8D capsules. 即每个primary capsule包含8个(9x9, 2 stride) conv unit. PrimaryCapsule共有(32, 6, 6)个capsule输出，每个输出是一个8维向量。 (6x6) grid中的capsule share weights，与CCN的卷积核原理相同。每个capsule输出a grid of vectors，而不是single vector output. 5.3. DigitCaps 输入维度. (batch_size, 3266, 8) 输出维度. (batch_size, 10, 16) 10个16维的capsule. CapsNet中，只在PrimaryCapsules和DigitCaps层之间routing. 由于Conv1输出是1D, 不存在orientation to agree on, 因此不在Conv1和PrimaryCapsules层之间routing. 使用Adam optimizer, routing logit $b_ij$初始化为0. 5.4. Reconstruction (Figure 2) 输入维度 . (batch_size, 10*16) 或 (batch_size, 16) 输出维度. (batch_size, 28* 28) Encourage the digit capsules to encode the instantiation parameters of the input digit. 在训练阶段，mask除了ground truth对应capsule之外的所有capsule. 而测试阶段，mask除了length最大的capsule之外的所有capsule. 目标为最小化sum of squared differences. Reconstruction loss乘以0.0005系数。 To summarize, by using the reconstruction loss as a regularizer, the Capsule Network is able to learn a global linear manifold between a whole object and the pose of the object as a matrix of weights via unsupervised learning. As such, the translation invariance is encapsulated in the matrix of weights, and not during neural activity, making the neural network translation equivariance. Therefore, we are in some sense, performing a ‘mental rotation and translation’ of the image when it gets multiplied by the global linear manifold. 6. Capsules on MNIST 6.1. 实验结果 使用routing和reconstruction regularizer能够提升性能。 CNN: 24.56M parameters. CapsNet: 11.36M parameters. 6.2. Capsule单个维度代表的意义 (Figure 4)variations包括厚度、斜度和宽度。16维中几乎总有一维代表数字的宽度，一些维度代表combinations of global variation, 其他一些代表localized part of digit. 6.3. Robustness to Affine Transformations由于natural variance in skew, rotation, style, etc in hand written digits, 使用CapsNet具有健壮性。 训练集 MNISTdigit placed randomly on a black background (平移). 测试集 affNISTMNIST digit with a random small affine transformation CapsNet stop when $train_{acc}=99.23%$, $test_{acc}=79%$.CNN stop when $train_{acc}=99.22%$, $test_{acc}=66%$. 7. Segmenting Highly Overlapping Digit 7.1. Routing可看做Attention机制 Dynamic routing可以看做是并行的attention机制，L+1 level的每个capsule都会attend一些L level的activive capsules，忽略其他的。因此，在对象重叠的情况下，也能识别多个对象。 The routing by agreement should make it possible to use a prior about shape of objects to help segmentation. It should obviate the need to make higher level segmentation decisions in the domain of pixels. 7.2. MultiMNIST dataset 两张image各方向移动4个pixel，构成36x36 image. MultiMNIST result shows that each digit capsule can pick up the style and position from the votes it is receiving from PrimaryCapsules layer. 选top 2 length的capsule分别输入decoder中，得到两张digit images. 8. Other Datase 8.1. CIFAR10 (3 channel) test_acc: 89.4%. drawback 由于CIFAR10 image的背景变化太大，很难model合理大小的网络，导致性能较差。 8.2. smallNORB test_acc: 97.3%. 8.3. SVHN test_acc: 95.7%. 9. Discussion and previous work Capsule将pixel intensities转化为vectors of instantiation parameters of recognized fragments, 然后对fragments作transformation matrices, 进而预测 the instantiation parameters of larger fragments. 9.1. CNN不足之处 Convolutional nets have difficulty in generalizing to novel viewpoints. CNN具有translation (平移)不变性，但对于affine transformation (平移、旋转、缩放和斜切)不具有。因此，replicating feature detectors on a grid that grows exponentially with the number of dimensions, 或者指数级增加标注训练集。 9.2. Capsule Capsule作了一个非常强的假设：对于image中的each location, a capsule表示至多一个entity的instance. Capsule的神经元活动，会随着viewpoint (视角)的变化而变化(同变性equivariance)，而不是消除viewpoint带来的影响(如STN网络)。能够同时处理不同object(或object part)上的不同affine transformation. 9.3. Routing iteration times]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Architecture</category>
        <category>Capsules</category>
      </categories>
      <tags>
        <tag>Classification</tag>
        <tag>CapsulesNet</tag>
        <tag>Cognitive Neuroscience</tag>
        <tag>Image Reconstruction</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Capsule Extension]]></title>
    <url>%2F2017%2F12%2F18%2FCapsule%20Extension%2F</url>
    <content type="text"><![CDATA[CapsulesNet 的解析及整理 1. 大脑皮层微柱 功能柱 人的大脑皮层厚度为3～4mm，包含28x10^9神经元和相同数量胶质细胞。纵向垂直于皮层表面的细胞组织成微柱，穿越II-VI层，形成一个个结构功能单元。 灵长类的微柱包含80～100个神经元，形成30～50mm直径，深度2～4mm的柱状结构。 在视觉区的纹状皮层，每个柱内神经元数目大约200～250个。十个左右微柱组成一个功能柱。因此，一个功能柱直径大约300～600mm。不同物种的脑容量相差很大(10^3)，但功能柱大小接近。 芒卡斯尔认为功能柱是大脑皮层的基本的结构和功能单元，他有时也把它称为一个微型组件(module). 2. Max Pooling Max pooling丢弃固定大小感知域中(nm-1)/nm的信息。随着层级的增加不断进行max pooling操作，相当于逐步增加感知区域，丢弃的范围逐步增大，丢弃的信息也不断增多，最终只有image整体中各object特征被激活，而位置信息(位置信息针对一些任务并不需要，如分类)被丢弃。 3. CNN局限性 Capsule Networks Are Shaking up AI — Here’s How to Use ThemCapsule Networks Explained 3.1. 不具有平移同变性CNN具有translation invariant (平移不变性), 无论如何平移图像中的obj，都能检测到。不变性是通过Pooling下采样实现。但CNN不具有translation equivariance (平移同变性), 无法检测到obj平移的距离方向等变化，即。 由于CNN无法识别各sub-obj之间的相对位置关系，以致下图都被识别为Face。 CNNs work by accumulating sets of features at each layer. It starts of by finding edges, then shapes, then actual objects. However, the spatial relationship information of all these features is lost. 导致下图均被识别为peroson. CNN is also easily confused when viewing an image in a different orientation. 下图被识别为coal black color. CNN与CapsuleNet在识别上图人脸的区别 CNN CapsuleNet 3.2. 易受白盒对抗攻击3.3. 需要大量数据进行泛化In order for the ConvNets to be translation invariant, it has to learn different filters for each different viewpoints, and in doing so it requires a lot of data. 3.4. CNN无法很好地表示人类视觉系统CNN利用filter从low level visual data提取high level information. 而对于人类系统而言，当触发视觉刺激时，大脑的内建机制会将low level visual data route到大脑某些部分。 此外，人类视觉系统会对obj建立coordinate frames，并选择一个参考点，旋转obj. 4. ConvNet与CapsuleNet区别 CapsuleNet (mimics the human vision system) strives for translation equivariance instead of translation invariance, allowing it to generalize to a greater degree from different view points with less training data. 5. Inverse Graphics 5.1. 图像渲染过程To go from a mesh object onto pixels on a screen, it takes the pose of the whole object, and multiplies it by a transformation matrix. This outputs the pose of the object’s part in a lower dimension (2D). )## 5.2. 图像逆过程lower dimension –&gt; whole object 5.3. 权重矩阵因此，可用权重矩阵表示两者之间的关系。这些权重是viewpoint invariant. Meaning that however much the pose of the part has changed we can get back the pose of the whole using the same matrix of weights. 利用reconstruction得到该权重矩阵 6. 矢量神经元 知乎: 如何看待Hinton的论文《Dynamic Routing Between Capsules》 7. Hinton对CNN的思考 7.1. 生物神经系统的思考 解剖学上并未发现神经系统存在反向传播及求导的结构。 神经系统具有分层结构，但层数不多。生物系统传导在ms量级，GPU在us量级，同步出现问题。 大脑皮层存在微柱结构(Cortical minicolumn)，其内部含有上百个神经元，并存在内部分层结构，比NN的一层结构更为复杂。 7.2. 认知神经科学的思考人会不自觉根据物体形状建立坐标框架(coordinate frame), 并通过对坐标框架旋转。 坐标框架的不同会影响人的认知。 坐标框架参与到物体识别过程中，识别过程手空间概念的支配。 CNN不存在坐标框架。Hinton提出猜想：物体与观察者之间的关系（如物体姿态），应该由一整套激活的神经元表示，而不是由单个神经元，或者一组粗编码（coarse-coded，一层中并没有经过精细地组织）的神经元表示。这样才能有效表达坐标框架的先验知识。 7.3. CNN的局限性 不变性物体不随变化而变化。如空间不变性。 同变性用变化矩阵进行转换后，物体表示依旧不变。是对物体内容的变换。 CNN对旋转没有不变性。可采用数据增强方式达到旋转不变性。 CNN的不变性通过Pooling实现。 平移和旋转不变性舍弃了坐标框架。 虽然CNN准确率高，但是最终目标应该是对内容的良好表示，从而达到理解内容。 8. Hinton提出的Capsule 8.1. Capsule需具备的性质 一层中具有复杂的内部结构。 能表达坐标框架 实现同变性 8.2. Capsule神经元Capsule用一组神经元代表一个实体，仅且代表一个实体。 模长代表某个实体（物体或其一部分）出现的概率。 方向/位置代表实体的一般姿态(generalized pose)，报货位置、方向、尺寸、速度、颜色等。 8.3. 视角变换矩阵CapsuleNet用视角变换矩阵处理场景中两物体间的关联，不改变它们的相对关系。 8.4. 两种同变性 位置编码 (place-coded)视觉中的内容位置发生较大变化，由不同Capsule表示其内容。 速率编码 (rate-coded)视觉中的内容位置发生较小变化，由相同capsule表示其内容，但是内容有所改变。 高层的capsule有更广的域(domain)，所以低层的place-coded信息到高层会变成rate-coded.]]></content>
      <categories>
        <category>Knowledge Note</category>
      </categories>
      <tags>
        <tag>Cognitive Neuroscience</tag>
        <tag>Capsule</tag>
        <tag>Cerebral Cortex</tag>
        <tag>CNN Limitation</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(CVPR 2017) Image-to-image translation with conditional adversarial networks]]></title>
    <url>%2F2017%2F11%2F03%2FImage-to-Image%20Translation%20with%20Conditional%20Adversarial%20Networks%2F</url>
    <content type="text"><![CDATA[Isola P, Zhu J Y, Zhou T, et al. Image-to-image translation with conditional adversarial networks[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2017: 1125-1134. 本篇论文提出conditionnal GAN (supervised)结构 学习input image到output image之间的映射。 学习特定的loss function用于训练映射，即单独使用L1 loss (或L2 loss)会产生blur现象，而再此基础上进一步使用adversarial loss能够学习到适合特定数据集的loss function, 从而sharpen生成的图像 (判别器D能够判断blurry image为fake)。因此，针对不同任务 (Figure 1)，该方法具有通用性。 1. Contribution 针对不同任务，cGAN具有通用性。 Achieve good result, 并分析cGAN结构中的一些重要部分 2. Relative Work 2.1. Loss类型 Structured loss 每个像素点独立考虑. per-pixel classification loss, regression. Unstructured loss penalize the joint configuration of the output, 如conditional random fields. cGAN的unstructured loss是学习到的。 2.2. cGAN 前人也apply GANs in conditional seting, 但是针对特定应用的，而本论文的cGAN提出的是通用框架。 本论文的cGAN使用到了U-Net和PatchGAN. 3. Objective GAN z-&gt;G-&gt;y y-&gt;D-&gt;true or fake cGAN (Figure 2) {x, z}-&gt;G-&gt;y {x, y}-&gt;D-&gt;true or fake 使用L2会产生更严重的blur. 最终的目标函数 没有噪声z的网络会产生一个特定的输出，无法match any distribution, 因此cGAN加入噪声z，但在本篇论文实验中发现，G能够学习到如何ignore 噪声，从而在模型的test阶段也使用dropout产生noise. 4. Network architectures 4.1. Skip connection of G 在auto-encoder结构中，input的所有信息会在所有layers传输。为了避免这种方法，在AE的基础上添加skip connection, 即U-Net (Figure 3). 4.2. Markovian D (PatchGAN) L1 loss和L2 loss能够capture low frequencies, 因此需要约束D能够capture high frequency structure，即PatchGAN (N X N patches). D effectively models the image as a Markov random field. 4.3. Optimization and inference 在test阶段，使用dropout, BN使用 the statistics of the test batch, rather than aggregated statistics of the training batch. instance normalization在图像生成任务上很有效。(batch size为1，使用the statistics of the test batch) 5. Experiments L1产生blur. cGAN sharp imaged，但是存在artifacts. (Table 1) cGAN优于GAN，加上L1 loss后，cGAN也相对较优。 Colorfulness当不确定edge的位置时，L1会产生blur和averrage ( L1 will be minimized by choosing the median of of the conditional probability density function over possible colors.) 从而导致narrower distribution than the ground truth (Figure 7). Analysis of the G (Figure5) Analysis of the D (Figure 6, Table 2) Pixel GAN output 1x1 of D. Image GAN output 256x256(full image size) D. Patch GAN output 70x70(在本实验中) D. Fully-convolutional translation. Patch GAN由于不包含FC层，D和G都可应用与任何大小的图片。(Figure 8)中的G在train阶段使用256x256图片，在test阶段使用512x512图片。 Perceptual validation Semantic segmentationGAN一般用于图像生成，本论文尝试将cGAN用于做segmentation任务，但最终效果并不好。(Figure 10, Table 5)从实验结果可以看出， reconstruction losses like L1 are mostly sufficient. Semantic labels↔photoCityscapes dataset Architectural labels→photoCMP Facades dataset Map↔aerial photoGoogle Maps BW→color photos Edges→photonary Sketch→photo Day→night Failure case]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Image Processing</category>
      </categories>
      <tags>
        <tag>Image Enhancement</tag>
        <tag>Image Generation</tag>
        <tag>GAN</tag>
        <tag>Conditional GAN</tag>
        <tag>Map to Aerial</tag>
        <tag>Reality Enhancement</tag>
        <tag>Style Transfer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(NIPS 2017) Unsupervised Image-to-Image Translation Networks]]></title>
    <url>%2F2017%2F11%2F03%2FUnsupervised%20Image-to-Image%20Translation%20Networks%2F</url>
    <content type="text"><![CDATA[Liu M Y, Breuel T, Kautz J. Unsupervised image-to-image translation networks[C]//Advances in Neural Information Processing Systems. 2017: 700-708. 根据couple theory，基于joint分布可以很容易得到marginal分布，但利用marginal分布得出joint分布十分困难。 因此，本篇论文提出UNIT (UNsupervised Image-to-image Translation) 框架 (Figure 1)，利用shared-latent space假设协助从两个domain的marginal分布 ($p_{X1}(x1)$, $p_{X2}(x2)$)中学习它们之间的joint分布。 UNIT结构涉及到Coupled GAN、VAE、Cycle consistent、weight-sharing constraint. 此外，UNIT结构也能够应用到domain adaption任务上。 1. Assumptions Unsupervised Marginal $p_{X1}(x1), p_{X2}(x2)$ Supervised Joint $p_{X1,X2}(x1, x2)$ 假设存在shared-latent space，space中的shared latent code z能够同时用于恢复两个domain的图片。 确保满足$F_{1-&gt;2}=G2(E1(x1))$和$F_{2-&gt;1}=G1(E2(x2))$的一个必要条件是cycle consistent，即shared-latent space假设暗含了cycle consistent假设。 在shared-latent space的基础上，进一步假设shared intermediate representation $h$. $G1=G_{L1}*G_{L2}$ $G2=G_{L2}*G_{H}$ $G_{H}: z-&gt;h$, high level $G_{L}: h-&gt;x$, low level z可看作是场景的high-level representation (car in front, trees in back). h可看作是z的特定实现 (car occpy the following pixels). $G_{H,L}$可看作是actual image formation (tree lush green in sunny domain, dark green in rainy domain). 2. Framework 6 subnetworks (Table 1) 3. VAE Reparameterization 4. Weight-sharing 基于shared-latent space假设，enforce weight-sharing约束到两个VAE上：last few layers of E, first few layers of G. 但weight-sharing约束并不能确保两个domain中对应的图片能得到相同的latent code. 因为对于unsupervised方式而言，两个domian中不存在对应的pair能够训练网络输出同样的latent code. 然而，能够通过对抗训练将两个domain中的pair映射到同样的latent code上。 5. Stream Translation stream $X1-&gt;z1-&gt;X2$ $X2-&gt;z2-&gt;X1$ Reconstruction stream $X1-&gt;z1-&gt;X1$ $X2-&gt;z2-&gt;X2$ 6. GAN 只将对抗训练应用到translation stream上。 7. Cycle-consistent Cycle-reconstruction stream $X1-&gt;z1-&gt;X2’-&gt;z2-&gt;X1’$ $X2-&gt;z2-&gt;X1’-&gt;z1-&gt;X2’$ To further regularize the ill-posed unsupervised image-to-image translation problem. 8. Loss 交替update 更新D1, D2 (adversarial loss) 更新G1, G2, E1, E2 (VAE loss + CC loss) 9. VAE loss 用Laplacian分布model pG1,pG2. 最小化Negative log-likelihood等价于最小化image和reconstructed image绝对距离。 10. GAN loss 11. CC loss 12. Experiments Shallow D导致较差性能 (Figure 2b) Weight-sharing层数影响小 (Figure 2c) Negative log likelihood权重越大，acc越高 (Figure 2c) Ablation study (Figure 2d) Remove weight-sharing约束和reconstruction stream，结构变成CycleGAN. Remove cycle-consistent约束. Street image (Figure 3)Sunny to rainy, data to night, summery to snowy. Synthetic to real (Figure 3)Synthetic images SYNTHIA dataset.Real images Cityscape dataset. Dog breed conversion (Figure 4)ImageNet, 利用模板匹配算法extract head regions. Cat species conversion (Figure 5)ImageNet. Face attribute (Figure 6)CelebA dataset. With an attribute constituted the 1st domain, while those without the attribute constituted the 2nd domain. 13. Domain Adaption 用一个domian中的labeled数据训练分类器，并用将该分类器应用到另一个domain数据集上，该数据的labeled在训练中没有使用到。 利用UNIT进行多任务学习: Translate between source domain and target domian. 利用source domain的D提取source domain数据的特征。 共享D1, D2 high-level层的权重。 最小化D1和D2 highest layer提取feature之间的L1 loss. 实验发现spatial context information useful (RGB+normalized xy coordinates). 14. Network Architecture]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Image Processing</category>
      </categories>
      <tags>
        <tag>Image Enhancement</tag>
        <tag>Face</tag>
        <tag>Image Generation</tag>
        <tag>GAN</tag>
        <tag>Map to Aerial</tag>
        <tag>Reality Enhancement</tag>
        <tag>Style Transfer</tag>
        <tag>UNIT</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(ICCV 2017) Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks]]></title>
    <url>%2F2017%2F11%2F03%2FUnpaired%20Image-to-Image%20Translation%20using%20Cycle-Consistent%20Adversarial%20Networks%2F</url>
    <content type="text"><![CDATA[Zhu J Y, Park T, Isola P, et al. Unpaired image-to-image translation using cycle-consistent adversarial networks[C]//Proceedings of the IEEE international conference on computer vision. 2017: 2223-2232. 论文提出Cycle GAN结构，基于unpaired data (Figure 2)，学习domain X到domain Y的映射关系 (Figure 1)。 Cycle GAN能够应用到不同任务上：style transfer, object transfiguration, attribute transfer, and photo enhancement等。 Cycle GAN包含adversarial loss, cycle consistency loss. 对于某些特定任务，Cycle GAN额外包含一个identity loss. 论文假设两个不同domain之间存在underlying relationships，Cycle GAN (Figure 3)从一个image collections X中学习到一些特征，并将这些特征转换到另一个image collections Y上。 仅使用adversarial loss，无法保证生成的图像是有意义的，例如，G可能生成rubbish fool D。此外，生成图像不一定是desired。另一方面，标准的GAN过程可能会导致mode collapse问题：所有输入图像都会被映射到同一个输出图像。因此，模型引入了cycle consistent loss. 对于painting-&gt;photo的任务，为了保留输入- painting的颜色 (Figure 9)，模型引入了identiy loss. Circle GAN可看作是两个auto-encoder：GF和FG。 1. Related work Image-to-Image Translation (CGAN) I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Generative adversarial nets. In NIPS, 2014 P. Isola, J.-Y. Zhu, T. Zhou, and A. A. Efros. Image-to-image translation with conditional adversarial networks. arXiv preprint arXiv:1611.07004, 2016. Unpaired Image-to-Image Translation (VAE+GAN) M.Y. Liu, T. Breuel, and J. Kautz. Unsupervised image-to-image translation networks. arXiv preprint arXiv:1703.00848, 2017. Neural Style Transfer学习两张特定图片之间的映射，Cycle GAN学习的是两个domain之间的映射。 2. Loss Adsersarial Loss Loss Identity Loss 3. Implementation D使用70*70 PatchGAN：更少参数，能判别任意大小图像。 将adversarial loss从negative log likelihood改为least square loss. History Buffer of generated images. batch size 1 of scratch. 4. Experiments CoGANM.-. Liu and O. Tuzel. Coupled generative adversarial networks. In NIPS, pages 469–477, 2016. Pixel loss + GANSimGAN (self-regularision loss). Feature loss + GANSimGAN (vgg16 feature loss, instead of RGB loss). BiGANV. Dumoulin, I. Belghazi, B. Poole, A. Lamb, M. Arjovsky, O. Mastropietro, and A. Courville. Adversarially learned inference. arXiv preprint arXiv 2016. Pix2pixCGAN. AMT FCN score (Cycle GAN用的是unsupervised, pix2pix用的是supervised) Pixel classification Ablation 5. Dataset Labels-photo: Cityscapes dataset (Figure 5) Map-aerial photo: Google Maps (Figure 6) Labels-photo: CMP Facade database (Figure 8) Edges-shoes: UT Zappos50K dataset (Figure 8) Style transfer: Flickr, WikiArt (Figure 10) Object transfiguration&amp;season transfer: ImageNet, Flickr (Figure 13) Photo generation from paintings: Monet’s painting, Flickr (Figure 12) Photo enhancement:Flickr (Figure 14) Comparison with Gatys (Figure 15, 16) Failure cases (Figure 17)]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Image Processing</category>
      </categories>
      <tags>
        <tag>Image Enhancement</tag>
        <tag>Image Generation</tag>
        <tag>GAN</tag>
        <tag>Map to Aerial</tag>
        <tag>Style Transfer</tag>
        <tag>CycleGAN</tag>
        <tag>Label to Photo</tag>
        <tag>Edge to Photo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(2013) Auto-Encoding Variational Bayes]]></title>
    <url>%2F2017%2F11%2F03%2FAuto-Encoding%20Variational%20Bayes%2F</url>
    <content type="text"><![CDATA[Kingma D P, Welling M. Auto-encoding variational bayes[J]. arXiv preprint arXiv:1312.6114, 2013. (Figure 1) 由于通过(可观察的)变量X推断(不可观察) latent variables比较困难 (即后验概率分布p(z|x))，论文提出VAE (变分自编码器，variational auto-encoder)结构以及AEVB (自编码变分贝叶斯)算法，即通过SGVB (随机梯度变分贝叶斯，Stochastic Gradient Variational Bayes)估计使得构造的q(z|x)分布近似难以计算的p(z|x)分布。 1. Contribution 通过reparameterization方法确保梯度能够回传。 通过下界估计(变分贝叶斯推导得出)近似后验概率分布。 2. 知识点延伸 相对熵D(P||Q)=交叉熵H(P, Q) - 熵H(Q). 熵：真实分布P的平均编码长度(大于等于0)。 交叉熵：非真实分布Q的平均编码长度(大于等于[熵])。 相对熵：[交叉熵]与[熵]的差，即多出的编码bit数(大于等于0)。 D(Q||P)优化结果：P(绿色)尽可能包含Q. D(P||Q)优化结果：P(绿色)尽可能包含Q. 变分法：推导得出泛函存在极值的必要条件：欧拉-拉格朗日方程。 平均场定理：利用概率模型Q(x1, x2, …, xn)=Q(x1)Q(x2)…Q(xn)近似所要求的概率模型P(x1, x2, …, xn)=P(x1)P(x2|x1)P(x3|x2, x1)…P(xn|xn-1, …, x1). 变分贝叶斯：结合平均场定理和变分法，求出近似P分布的Q分布。通过最小化目标函数KL(Q||P)，推导出最大化下界估计。利用平均场定理进一步推出需要满足 即 涉及到计算期望: 链接 VAE 链接 3. SGVB 基于变分贝叶斯推导得出的下界估计，利用平均场定理推出最终需要满足的条件(涉及到期望计算)，但分析期望相关的解仍然存在困难。 因此，论文通过reparameterization方法 (而非平均场定理) 简化下界估计，公式右边两项分别通过decoder和encoder计算得出。 同时，reparameterization能够使得下界估计可导 (即梯度能够回传)。 4. The Problem Scenario Intractability. 边缘似然p(x)=∫p(z)p(x|z)dz难以计算，导致真实后验概率分布p(z|x)=p(x|z)p(z)/p(x)难以计算(无法使用EM算法)，以及mean-field VB难以计算。另一方面，p(x|z)可通过神经网络非线性解决。 Minibatch训练时，使用Monte Carlo EM非常慢。 论文旨在解决以下3点困难: 近似参数θ. 近似posterior p(z|x). 近似marginal p(x). 即引入encoder q(z|x)近似p(z|x). encoder (构造概率分布φ): q(z|x). decoder (真实概率分布θ): p(x|z). 论文提出一种方法联合训练encoder参数φ和decoder参数θ. 5. The Variational Bound 根据VB可得到公式 (1) L即为变分下界，可写成公式 (2,3) (公式3) 右边两项分别为q分布与真实分布p的差异、根据z重构x的误差。 6. SGVB estimator和AEVB algorithm 假设一个近似后验概率分布q(z|x)，当不假定条件x时，同样能应用于q(z)。 使用包含 (auxiliary) noise variable ε的可导变换g表示random variable z (公式4) 因此可以构造关于包含z~q(z|x)的函数f的Monte Carlo期望估计 (公式5)： (公式2) 可改写为 (公式6) 考虑到分布近似误差和重构误差，(公式2) 也可改写为 (公式7) 考虑到minibatches训练，可进一步改写为 (公式8) 7. The Reparameterization Trick 假设z服从高斯分布 可写成合理形式 即 8. Example 假设z的先验分布服从标准正态分布 9. Experiments 增加latent variable维数不会导致过拟合。 y: loss, x: iteration. 10. Solution of -D(q||p), Gaussian Case 11. MLP’s as probabilistic encoders and decoders MLP (Multi-layer Perceptron)：神经网络。 encoder使用MLP with Gaussian output. decoder使用MLPs with Gaussian or Bernoulli outputs, depending on the type of data. Bernoulli Gaussian]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Architecture</category>
        <category>Network</category>
      </categories>
      <tags>
        <tag>Network</tag>
        <tag>VAE</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(AAAI 2017) Deep Spatio-Temporal Residual Networks for Citywide Crowd Flows Prediction]]></title>
    <url>%2F2017%2F10%2F05%2FDeep%20Spatio-Temporal%20Residual%20Networks%20for%20Citywide%20Crowd%20Flows%20Prediction%2F</url>
    <content type="text"><![CDATA[Zhang J, Zheng Y, Qi D. Deep spatio-temporal residual networks for citywide crowd flows prediction[C]//Thirty-First AAAI Conference on Artificial Intelligence. 2017. 对于公共安全和交通管理而言，预测人群流动具有重要意义，但它受到事件、天气和区域间等复杂因素的影响，因此该篇论文提出一个基于深度学习的方法 (ST-ResNet, Figure 3)综合预测一个城市中各区域人群的流入 (inflow)与流出 (outflow) (Figure 1). ST-ResNet基于时间和空间数据 (spatio-temporal data)，对temprtaol closeness, period和trend 3阶段交通拥挤程度进行建模，并结合external factors对进行预测。论文基于两个数据集Beijing和New York City (NYC)进行实验，并与6个baseline进行对比。 Mobile phone signal of pedestrians. GPS trajectories of vehicles. 1. Complex Factors Spatial dependencies. Region的inflow受到附近regions的outflow影响，甚至受更远的regions影响。同时region的inflow也会影响它自身的outflow. Temporal dependencies. Region的flows也受到近期时间段的影响。例如，8点的交通阻塞可能会影响到9点的交通情况；每个工作日的高峰期相似等。 External influence. 天气条件、事件等。 2. Contribution 使用Conv来model任意两region之间的spatial dependencies. 使用3个ResNet来model temporal closeness, period和trend的temporal dependencies. Assigning different weights to aggregate 3 outputs of ResNet, and then aggregate external factors. Outperform 6 baselines on Beijing and NYC dataset. 3. Preliminaries Region. 将城市划分为grid map (Figure 2). Flow (2 channels). 归一化到[-1,1]. 4. ST-ResNet (Figure 3) ST-ResNet Include temporal closeness (recent), period (near), trend (distant) and external 4个主要部分。 Temporal closeness (recent), period (near), trend (distant)的ResNet结构相同，其中包含2个Conv和L个Residual (Figure 4). 由于地铁或高速公路会导致两个很远的region具有很强的关联性，因此使用具有层级性的stack Conv来实现，并利用residual的形式来提高stack Conv的收敛性。Relu之前增加了ReLU层。 External通过两层FC. Regard as embedding layer + mapping layer (same shape as Xt). Fusion包含3个可学习的权重参数，乘以对应的3个ResNet输出后相加，其结果再与FC输出相加，经过tanh激活函数。 MSE loss, Adam. 5. Algorithm p: one day. q: one-week. 6. DataSet (Table 1) TaxiBJ. Taxicab GPS and moteorology data. 选择最近4周作为testing data. BikeNYC. NYC Bike system. 选择最近10天作为testing data. (Figure 5) 假期和天气会影响北京办公区的流量。 (Figure 6) recent时间段的相关性更大，办公区周末的流量较低，办公区流量呈现下降趋势，居住区呈现上升趋势。 7. Baseline HA. 历史inflow,outflow均值。 ARIMA. Auto-Regressive Integrated Moving Average. 用于预测时间序列的值。 SARIMA. Seasonal ARIMA. VAR. Vector Auto-Regressive(VAR), spatio-temporal model. ST-ANN. extracts spatial (nearby 8 regions’ values) and temporal (8 previous time intervals) features, then fed into an artificial neural network. DeepST. State-of-the-art. DNN for spatio-temporal data. 4 variants: DeepST-C, DeepST-CP, DeepST-CPT, and DeepST-CPTM. Focus on temporal dependencies and external factors. 8. Experiments Theano&amp;Keras. Conv2: 2 filters. Evaluation Metric: Root MSE. 9. Related Work Conv: capture spatial dependencies. RNN: capture temporal denpendencies. ConvLSTM: capture spatial and temporal denpendencies. But can not model very long-range temporal denpendencies(period and trend). 10. Future Consider other types of flows: taxi, truck, bus, phone signal, metro card swiping.]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Prediction</category>
      </categories>
      <tags>
        <tag>Prediction</tag>
        <tag>Citywide Crowd Flows</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(2016) Automated Inference on Criminality using Face Images]]></title>
    <url>%2F2017%2F10%2F03%2FAutomated%20Inference%20on%20Criminality%20using%20Face%20Images%2F</url>
    <content type="text"><![CDATA[Wu X, Zhang X. Automated inference on criminality using face images[J]. arXiv preprint arXiv:1611.04135, 2016: 4038-4052. 该篇论文利用supervised machine learning(logistic regression, KNN, SVM, CNN) 对criminal (C) 和non-criminal (N) 面部图像进行分类(准确度最高达到89.51%)，并进行一些实验分析C与N群体之间的区别： N群体内部的面部相似度更大，C群体内部的面部差异更大。 C和N是两个concentric(同心), distinctive的manifold(流形). The variation of C greater than N. 基于面部特征的人为判断会带有偏见、先决条件等，而CV算法并不会存在这些问题。 1. Data Preparation Dataset包含1856张照片 (1126N+730C, Figure 1). 照片标准: Chinese, male, between ages of 18 and 55, no facial hair, no facial scars, or other markings. N including waiters, construction workers, taxi and truck drivers, real estate agents, doctors, lawyers and professors; half have university degrees. C including the ministry of public security of China, the departments of public security for the provinces of Guangdong, Jiangsu, Liaoning, etc. And the City police department in China. C中 235人是violent crimes (murder, rape, assault, kidnap and robbery), 剩余536人是non-violent crimes (murder, rape, assault, kidnap and robbery). Only the region of the face and upper neck is extracted. 80 × 80 images. 将每张图像的直方图与整个数据集的平均直方图相匹配，从而使得灰度图归一化到同样的强度分布。 2. Methods 面部关键点特征能够避免signal level和variant of source cameras的影响。论文使用以下四种关键点: Facial landmark point. Facial feature vector, generated by modular PCA. Facial feature vector based on Local Binary Pattern (LBP) histograms. The concatenation of above three feature vectors. (Feature-driven classifiers (LR, SVM, KNN) 3 + Data-driven classifiers (CNN)) 10-fold cross validation = 130 cases 3. Results 不同的source camera拍摄的照片可能会带有不同camera的signatures, 虽然已通过上述的landmark point解决，但在此进一步引入高斯噪声 (mean=0) 来overpower camera signatures. 实验结果与期望的一致: 性能不会出现很大的变化 (Figure 6,7;Table 2, 3). 4. Discriminating Feature 使用Feature Generating Machine (FGM)进行分析与犯罪最相关的面部部位，得出这些特征位于眼角、嘴唇和额头部位 (Figure 8). ρ: 上嘴唇的曲度 d: 内眼角之间的距离 θ: 鼻尖到嘴唇两角的角度 使用Hellinger距离分别计算C和N两者之间的上述3个部位的距离，分别为0.3208, 0.2971, 0.3855. 因此，C和N是存在一定差异的. 按照论文分析结果 (Figure 8, Table 4)脑补了一个极端的罪犯例子 三个特征的直方图 5. Face Clustering on Manifold 通过平均脸并不能很好地得出C和N群体的区别 (Figure 10)，因此需要在更高维度 (manifold流形和聚类)上进行分析。 公式2分别为cross-class average manifold和in-class average manifold. 计算得到manifold后，使用Isomap进行降维可视化。]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Face</category>
      </categories>
      <tags>
        <tag>Classification</tag>
        <tag>Face</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(NIPS 2012) Imagenet classification with deep convolutional neural networks]]></title>
    <url>%2F2017%2F09%2F29%2FImageNet%20classification%20with%20deep%20convolutional%20neural%20networks%2F</url>
    <content type="text"><![CDATA[Krizhevsky A, Sutskever I, Hinton G E. Imagenet classification with deep convolutional neural networks[C]//Advances in neural information processing systems. 2012: 1097-1105. 该篇论文提出一种CNN结构 (AlexNet)，在ImageNet数据集ILSVRC-2010 (Table 1) 和ILSVRC-2012上进行实验验证其性能：top-1 and top-5 error (Figure 4, Left). 论文中AlexNet在双GPU上进行训练，使用 ReLU替代tanh activation function，提高收敛速度。 Local Response Normalization和Overlapping Pooling提高性能。 以及通过 Data Augmentation Dropout Layer 来Reduce overfitting. 1. Dataset ImageNet数据集包含超过15 million labeled 高分辨率图像，图像可分为22,000 categories. Starting in 2010, as part of the Pascal Visual Object Challenge, an annual competition called the ImageNet Large-Scale Visual Recognition Challenge (ILSVRC) has been held. ILSVRC比赛使用ImageNet数据集中的一部分，大约包含1000个图像类别，每个类别大约1000张图片。数据集分为training、validation、testing. 评测指标分为top-1和top-5 error。之所以使用top-5 error指标是因为，有些图像可以同时分为好几个类别 (数据集的labele可能存在一定误差)。Top-5表示只要预测的类别在该图像前5个类别labele中，就算预测正确。 对数据集图像进行256$\times$256尺寸的下采样处理。由于图像尺寸大小不一，论文首先按照比例rescale图像，将图像shorter sied rescale到256大小，central crop 256$\times$256 patch. 此外，还对图像像素值进行去中心化操作(减均值) 2. ReLU 用ReLU (non-saturating nonlinearities) 替代tanh(saturating nonlinearities) 能提高收敛速度(Figure 1) 3. Local Response Normalization 对各空间点的卷积值在channel维度上作归一化(现在一般使用GoogleNet Version 2提出的Batch Normalization). Response normalization reduces our top-1 and top-5 error rates by 1.4% and 1.2%, respectively. 4. Overlapping Pooling 即stride小于kernel size. This scheme(stride=2, kernel size=3 vs stride=2, kernel size=2) reduces the top-1 and top-5 error rates by 0.4% and 0.3%. 5. 多GPU训练 GTX 580 3GB memory. 由于单块GPU显存太小，因此使用两块GPU训练。如今GTX Titan已到12G. 6. AlexNet 8 layers=5 Conv+3 FC+Softmax (Figure 2). has 60 million parameters and 650,000 neurons. 7. Data Augmentation (方法1) 先对256$\times$256图像进行horizontal reflection，再random crop 224$\times$244 patches，论文将数据集扩大2048倍。 (方法2) Altering the intensities of the RGB channels. 对RGB矩阵进行PCA得到特征值和特征向量(图1)，对特征值乘以一个系数α，α服从mean=0, std=0.1高斯分布。 8. Dropout Layer Combining the predictions of many different models is a very successful way to reduce test errors, but it appears to be too expensive for big neural networks that already take several days to train. 因此，利用dropout的随机性来改变模型training阶段的内部结构。 论文使用drop rate 0.5, 即50%几率将training阶段的输出设置为0，不参与本次forward和backward过程。 Dropout roughly doubles the number of iterations required to converge. 在testing阶段，将输出乘0.5 (如今的dropout方法，大多数不乘drop rate). 9. 基于vector进行相似性检索 基于倒数第二层FC输出的4096维Vector，计算欧氏距离进行相似图片检索 (Figure 4, Right). 在pixel leve上基于L2进行图片相似检索显然不可行，但是在high-level可用L2进行检索。 此外，4096维vector的欧氏距离计算量太大，使用auto-encoder对vector进一步压缩后再进行相似度检索，能得到更好的效果。]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Architecture</category>
        <category>Network</category>
      </categories>
      <tags>
        <tag>Network</tag>
        <tag>Classification</tag>
        <tag>AlexNet</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(CVPR 2017 Best Paper) Learning from simulated and unsupervised images through adversarial training]]></title>
    <url>%2F2017%2F09%2F19%2FLearning%20from%20simulated%20and%20unsupervised%20images%20through%20adversarial%20training%2F</url>
    <content type="text"><![CDATA[Shrivastava A, Pfister T, Tuzel O, et al. Learning from simulated and unsupervised images through adversarial training[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2017: 2107-2116. 该篇论文提出一种Unsupervised方法学习SimGAN model。该model能够使用大量unlabeled real images来增强labeled synthetic images真实性 (Figure 1). 显而易见，在real images和synthetic images之间存在一个gap，而实际上，Deep Learning做的事情就是学习两者之间的映射关系。 1. 贡献点 使用unlabeled real data来refine synthetic images. 结合adversarial loss和self-regularization loss训练Refiner Network (R). 使用一些key modification来稳定train以及防止R产生artifacts. 分别使用synthetic和refined images训练CNN进行gaze estimation任务，并在MPIIGaze Dataset上测试，进行比较。 2. 训练过程 SimGAN模型包含Refiner Network(R)和Discriminator Network(D) (Figure 2). (Algorithm 1) 对于每个training step，首先训练R $K_g$次，接着训练D $K_d$次。 训练网络的过程: Forward Input$\to$Calc Loss$\to$Backward Loss$\to$Optimize Parameters. 3. Loss Function D包含两部分loss (Formula 2)： Refined images输入D判别为False的loss（输入与Ground-truth的cross entropy loss）。 Real images输入D判别为True的loss. R包含两部分loss (Formula 1,4)： Refined images输入D判断为True的loss（与D中判别其为False形成对抗Adversarial） Synthetic images与refined images之间的L1 loss，乘以权重系数λ(hyper-parameter). 4. Self-regularazition loss Preserve the annoation information of the synthetic images. 5. Local Adversarial Loss Output a probability map (Figure 3) instead of a vertor. Prevent R from over-emphasizing certain image features to fool the current discriminator network, leading to drifting and producing artifacts. 6. History Buffer of Refined Images (Figure 4) Prevent D from only focusing on the latest refined images. 将refined images输入D之前，首先随机选择一半refined images放入Buffer中，再从Buffer中随机选择同样数目的refined images放回。 使用History Buffer的实验结果优于不使用 (Figure 10) 7. Gaze Estimation Task 分别使用synthetic images和refined images训练简单的CNN(输出vector(x,y,z))，并在MPIIGaze Dataset上进行测试。 从实验结果 (Table 2) 能够看出，经过R增强的refined images的distribution更接近real images的distribution. 8. Details R is a ResNet. λ=0.001, kd=1, kg=50. 1.2M synthetic images, 214K real images. More in here 9. Github Tensorflow&amp;Keras：wayaai/SimGAN Pytorch：AlexHex7/SimGAN_pytorch]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Image Processing</category>
      </categories>
      <tags>
        <tag>Image Generation</tag>
        <tag>GAN</tag>
        <tag>Reality Enhancement</tag>
        <tag>SimGAN</tag>
        <tag>Eye</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(CVPR 2017 Best Paper) Densely connected convolutional networks]]></title>
    <url>%2F2017%2F09%2F19%2FDensely%20connected%20convolutional%20networks%2F</url>
    <content type="text"><![CDATA[Huang G, Liu Z, Van Der Maaten L, et al. Densely connected convolutional networks[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2017: 4700-4708. 该篇论文提出一种网络结构DenseNet (Figure 2)，能够在提高性能的同时，大量降低模型的参数及占用内存 (Figure 3)。 DenseNet Block (Figure 1)结构中包含大量skip connection(shortcut)： 解决梯度消失问题 提高feature reusing和propagation. 降低Conv层的filter(kernel)数量，使网络变得更narrow.(由于存在shortcut，在通过网络的时候，不会出现信息丢失的情况。) DenseNet细节包含以下几个主要部分： Transition Layer Growth Rate Bottleneck Layer Compression 1. Transition Layer (图1) 连接在两个Dense Block之间，包含：BN- (1x1) Conv- (2x2) Avg Pooling 2. Growth Rate denoted by $k$. 即$H$层 (Figure 1)中 (3x3) Conv层的filter数量为$k$. 3. Bottleneck Layer 为了降低特征图数量，在H层的头部加上Bottleneck Layer，包含：BN-ReLU-(1x1)Conv. 该Conv层的filter数量小于输入特征图数量，从而达到降低特征图数量的目的 4. Compression 为了进一步提高模型的紧凑性，可以在Transition Layer中降低特征图数量。即降低其中的 (1x1) Conv层filter数量。 假设输入特征图数量为$m$，存在一个hyper-parameter $θ$ (0&lt;$θ$&lt;1) 使得输出特征图数量降低至$θ_m$，则(1x1) Conv层有$θ_m$ filters.]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Architecture</category>
        <category>Network</category>
      </categories>
      <tags>
        <tag>DenseNet</tag>
        <tag>Network</tag>
        <tag>Classification</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(CVPR 2017) Deep feature flow for video recognition]]></title>
    <url>%2F2017%2F09%2F19%2FDeep%20feature%20flow%20for%20video%20recognition%2F</url>
    <content type="text"><![CDATA[Zhu X, Xiong Y, Dai J, et al. Deep feature flow for video recognition[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2017: 2349-2358. 该篇论文首先说明了Video Recognition的Problem： 直接将图像识别网络应用到视频frame上会产生非常大的计算量。例如，假设图像识别网络处理一张图片需要0.1s，而30 fps的视频而言，处理1s的视频内容需要3s，这显然是不具有可行性的。 因此，论文提出了一个用于Video Recognition的Framework Deep Feature Flow (DFF) (Figure 2)： 仅仅将稀疏key frame输入Feature Network. 其他frame的特征图通过key frame的特征图进行propagation得到。 比较Figure 1中current frame feature maps和propagated feature maps： 将current frame输入Feature Network得到的特征图 对key frame特征图进行propagation得到的特征图 可以看出两类特征图基本类似。此外，可以明显看出Conv层中的#183 filter activate on车， #289 filter activate on 行人和狗。 通过Detection task实验(图1)可看出，DFF相对于Per-frame Network： 提高10倍速度。 Accuracy仅仅从73.9%降低到69.5%. DFF能大幅度提升速度是因为：Flow estimation and feature propagation are much faster than the computationof convolutional features. DFF结构包含3部分网络： Feature Network Flow Network Task Network 以及Inference和Training两个阶段 1. Feature Network 采用ResNet-50和ResNet-101 pre-trained on ImageNet Classification. 2. Task Network DeepLab for segmentation R-FCN for object detection 3. Inference Scale Function Better approximate the features the spatial warping may be inaccurate due to errors in flow estimation, object occlusion. 基于optical flow对key frame feature map进行bilinear interpolation 得到propagated feature map of current frame Key frame schedule 4. Training (图3) 选取labeled frame作为current frame（一些Dataset只有少数帧有ground-truth），随机选取附近的帧作为key frame： Current frame == key frame. 计算current frame loss(右半部分). Current frame != key frame. 计算key frame loss（左半部分）. Inference阶段固定key frame， 顺序选取current frame. 5. Dataset ImageNet VID for object detection Cityscapes for semantic segmentation 6. Task Network与Feature Network分割 论文做了一个实验，验证将Feature Network中最后多少层放入Task Network中性能最好。从Table 5中可以看出在两个网络完全分离的情况下，性能最好。 DFF Framework的通用性，论文默认采用预留Feature Network中最后1层到Task Network中。 Leaves some tunable parameters after the feature propagation, which could be more general. 7. Experiments Results 8. Future 论文提到未来将会在flow estimation和key frame scheduling两方面进行优化。 此外，论文结尾说，提出的DFF框架可能会成为一个新的研究方向。 个人认为在该框架上做大量的优化工作： 将ResNet换成DenseNet 将shortcut idea引入FlowNet，尝试提高计算速度和准确度 进行多次propogation对结果进行refined，提高准确度 修改Task Network，从而将很多其他图像处理任务（除Detection、Segmentation、Classification外）应用到视频流上。]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Segmentation</category>
      </categories>
      <tags>
        <tag>Video</tag>
        <tag>Optical Flow</tag>
        <tag>FlowNet</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(MICCAI 2015) U-net:Convolutional networks for biomedical image segmentation]]></title>
    <url>%2F2017%2F09%2F19%2FU-net%3A%20Convolutional%20networks%20for%20biomedical%20image%20segmentation%2F</url>
    <content type="text"><![CDATA[Ronneberger O, Fischer P, Brox T. U-net: Convolutional networks for biomedical image segmentation[C]//International Conference on Medical image computing and computer-assisted intervention. Springer, Cham, 2015: 234-241. 该篇论文 在FCN基础上提出U-Net结构 (Figure 1). 提出医疗影像data augmentation. 结合两者能够trained end-to-end from very few images and outperforms sliding-window CNN. Localization: a class label is supposed to be assigned to each pixel. 1. Sliding-window drawbacks Network需要对每个patch单独处理，重叠的patch产生大量冗余，因此非常慢。 Tradeoff between localization accuracy and the use of context. Large patches需要更多pooling层，导致localization accuracy下降，而small patches allow network see only little context. 2. Overlap-tile Strategy (Figure 2) 该策略支持任意大小图片的无缝分割(seamless segmentation)，蓝色区域为输入patch，黄色区域为输出patch(输入图片进行镜像处理). 3. Data Augmentation Shift and rotation invariance. Deformations and gray value invariance. Elastic deformation非常重要，能够有效模拟组织(tissue)最常见的形变方式。 4. Touching Object Challenge (Figure 3) 分离同种类型接触的细胞。 Propose the use of a weighted loss, where the separating background labels between touching cells obtain a large weight in the loss function. 预先计算ground-truth的weight map, to force the network to learn the small separation borders that we introduce between touching cells. d1,d2: the distance to the border of the nearest and second nearest cell. wc: balance the class frequencies. 5. Initialization Ideally the initial weights should be adapted such that each feature map in the network has approximately unit variance. 论文采用Gaussian 方差srqt(2/$N$), $N$为输入Node数。例如3x3 Conv层64 kernels, 则$N$ = 9 * 64 = 576. 6. Experiment Results]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Medical</category>
        <category>Segmentation</category>
      </categories>
      <tags>
        <tag>Segmentation</tag>
        <tag>Network</tag>
        <tag>UNet</tag>
        <tag>Cell</tag>
        <tag>ISBI</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(2015) Unsupervised representation learning with deep convolutional generative adversarial networks]]></title>
    <url>%2F2017%2F09%2F19%2FUnsupervised%20representation%20learning%20with%20deep%20convolutional%20generative%20adversarial%20networks%2F</url>
    <content type="text"><![CDATA[Radford A, Metz L, Chintala S. Unsupervised representation learning with deep convolutional generative adversarial networks[J]. arXiv preprint arXiv:1511.06434, 2015. 该篇论文提出Deep Convolutional GANs结构 (Figure 1)，使用一些方法来提高train稳定性，并通过实验验证 D的性能 可视化D的特征图 G的Walking in the Latent Space、遗忘性和Vector Arithmetic. 1. 提高训练稳定性的方法 Stride Conv 替代 Pooling Eliminate FC层（相对于GAN中的FC而言） BN层，除G的输出层和D的输入层外，否则导致不稳定 G使用ReLU，输出层使用Tanh。D使用LeakyReLU** 2. 验证D的性能 使用D作为Feature Extractor来classify CIFAR-10和SVHN。 3. D特征图可视化 不同特征图activate on 不同objects (Figure 5) 。 4. Walking in the Latent Space 5. G的遗忘性 G能学到不同object的表达,在second highest Conv层(倒数第二层)的特征上，利用logistic regression 预测activate窗户的filters，比较drop out窗户相关filters与否的生成结果。在drop out窗户filter的生成结果中，一些图片去掉了窗户,一些图片生成相似的其他object，如门、镜子 (Figure 6)。 6. Vector Arithmetic of Z 类似于Word2vec of Mikolov (Figure 7)。Single sample per concept were unstable. Average $Z$ of 3 sample show consistence andstable. 7. Train DCGAN on MNIST We found that removing the scale and bias parameters from batchnorm produced better results for both models. Noise introduced by batchnorm helps the generative models to better explore and generate from the underlying data distribution. 8. Code Code comes from github AaronYALai/Generative_Adversarial_Networks_PyTorch. Code of GAN Code of DCGAN]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Image Processing</category>
      </categories>
      <tags>
        <tag>Face</tag>
        <tag>Image Generation</tag>
        <tag>GAN</tag>
        <tag>Scene</tag>
        <tag>DCGAN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(CVPR 2017) Convolutional neural network architecture for geometric matching]]></title>
    <url>%2F2017%2F09%2F19%2FConvolutional%20neural%20network%20architecture%20for%20geometric%20matching%2F</url>
    <content type="text"><![CDATA[Rocco I, Arandjelovic R, Sivic J. Convolutional neural network architecture for geometric matching[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2017: 6148-6157. 该篇论文 提出CNN结构mimic传统机器学习中的geometric matching算法 (Figure 1):feature extraction, matching, simultaneous inlier detection, model parameter estimation. 使用合成数据进行训练模型，不需人工标注数据（几何变换参数） 1. 模型结构 (Figure 2)模型包含Feature extraction, Matching network和Regression network.图中两个Feature extraction CNN共享同样的参数，即用同一个网络提取A和B的特征。 2. Feature extraction Use VGG16 network cropped at the pool4 layer (before the ReLU unit), followed by per-feature L2-normalization. 3. Matching network 采用Correlation map computation with CNN feature (Figure 3). 对于特征图A中的某个空间点，计算特征图B中每个空间点与其的相关性（模拟几何变换机器学习算法）。此外，使用channel-wise normalization和ReLu操作amplify the score of the match. 论文中将该方法与常用的Concatenation和Subtraction方法进行比较(Table 2)，证明该方法效果更好。原因是 后续的Regression Network是由一系列Conv层组成，unable to detect long-range matches. 对于相同几何变换的不同图像pair而言，Concatenation和Subtraction会产生不同的输出，会增加Regression Network的难度。 此外，对correlation map进行normalization能够提升4个百分点。 4. Regression Network 考虑到参数、内存和计算量的问题，Regression Network (Figure 4)采用具有局部感知特性的Conv层，而非FC层。这种方法能够Work是因为对于AB相关性特征图上的某个空间点而言，它包含了B特征图中该点与A特征图中所有空间点的相似性得分，因此虽然使用局部性的Conv，但仍然具有全局性。 5. Hierarchy of transformations 为了得到更精确的结果，论文提出了一个hierarchy模型 (Figure 5)。该模型包含2个stage。 第一阶段estimate 6 parameters的affine transformation. 第二阶段estimate 18 parameters的thin plate spline transformation. 6. Loss Function $g_i$为uniform grid[-1, 1]，计算变换后的网格之间的距离平方。 7. Dateset Generate each example by sampling image A from a public image dataset, and generating image B by applying a random transformation $T_θ$GT to image A.]]></content>
      <categories>
        <category>Paper Note</category>
        <category>Image Processing</category>
      </categories>
      <tags>
        <tag>geometric matching</tag>
        <tag>Image Transformation</tag>
        <tag>Image Generation</tag>
      </tags>
  </entry>
</search>
